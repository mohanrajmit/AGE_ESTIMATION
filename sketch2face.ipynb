{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled19.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOD2bxzY5KrRbpDC9FumVr2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohanrajmit/AGE_ESTIMATION/blob/master/sketch2face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMjViw-IGq2G",
        "outputId": "3e9b0081-ec4b-4178-b330-3c01eb00b8f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Face-Sketch-to-Image-Generation-using-GAN'...\n",
            "remote: Enumerating objects: 864, done.\u001b[K\n",
            "remote: Counting objects: 100% (864/864), done.\u001b[K\n",
            "remote: Compressing objects: 100% (857/857), done.\u001b[K\n",
            "remote: Total 864 (delta 36), reused 810 (delta 3), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (864/864), 11.37 MiB | 10.78 MiB/s, done.\n",
            "Resolving deltas: 100% (36/36), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Malikanhar/Face-Sketch-to-Image-Generation-using-GAN.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/Face-Sketch-to-Image-Generation-using-GAN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9lO4E-dG6la",
        "outputId": "79840240-57b4-4ae4-b6ed-8bc8b5e0d12b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Face-Sketch-to-Image-Generation-using-GAN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncJN7vvIG_60",
        "outputId": "cb956ed2-d1d0-4063-91d5-6bcdde6e87f3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Compute SSIM and L2-norm.ipynb'  'Generated Images'\t  requirements.txt\n",
            " ContextualGAN.ipynb\t\t   Models\t\t  Testing.ipynb\n",
            "'Data Augmentation.ipynb'\t  'Predict Image.ipynb'\n",
            " Dataset\t\t\t   readme.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import glob\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "7gPbaLNyHAn2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_shearing(img, num, border):\n",
        "    rows = img.shape[0]\n",
        "    cols = img.shape[1]\n",
        "    if num == 0:\n",
        "        pts1 = np.float32([[5,5],[20,5],[2,20]])\n",
        "        pts2 = np.float32([[10,10],[20,5],[5,25]])\n",
        "    elif num == 1:\n",
        "        pts1 = np.float32([[5,5],[15,5],[2,20]])\n",
        "        pts2 = np.float32([[5,10],[10,10],[5,25]])\n",
        "    elif num == 2:\n",
        "        pts1 = np.float32([[5,5],[15,5],[5,20]])\n",
        "        pts2 = np.float32([[5,10],[10,10],[5,25]])\n",
        "    elif num == 3:\n",
        "        pts1 = np.float32([[5,5],[10,5],[2,20]])\n",
        "        pts2 = np.float32([[5,10],[10,10],[5,25]])\n",
        "    elif num == 4:\n",
        "        pts1 = np.float32([[5,5],[10,5],[2,20]])\n",
        "        pts2 = np.float32([[5,10],[10,10],[5,30]])\n",
        "    else:\n",
        "        pts1 = np.float32([[5,5],[10,5],[10,20]])\n",
        "        pts2 = np.float32([[5,10],[10,10],[5,30]])\n",
        "    M = cv2.getAffineTransform(pts1,pts2)\n",
        "    return cv2.warpAffine(img, M, (cols,rows), borderValue=border)"
      ],
      "metadata": {
        "id": "gy5yJ6I8HYTB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_rotation(img, degree, border):\n",
        "    rows = img.shape[0]\n",
        "    cols = img.shape[1]\n",
        "    M = cv2.getRotationMatrix2D((cols/2,rows/2),degree,1)\n",
        "    return cv2.warpAffine(img,M,(cols,rows), borderValue=border)"
      ],
      "metadata": {
        "id": "zMdX0GFfHe0E"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_image(img, skt, ang_range, shear_range, trans_range):\n",
        "    '''\n",
        "    This function transforms images to generate new images.\n",
        "    The function takes in following arguments,\n",
        "    1- Image\n",
        "    2- ang_range: Range of angles for rotation\n",
        "    3- shear_range: Range of values to apply affine transform to\n",
        "    4- trans_range: Range of values to apply translations over.\n",
        "\n",
        "    A Random uniform distribution is used to generate different parameters for transformation\n",
        "\n",
        "    '''\n",
        "    # Rotation\n",
        "\n",
        "    ang_rot = np.random.uniform(ang_range)-ang_range/2\n",
        "    rows,cols,ch = img.shape    \n",
        "    Rot_M = cv2.getRotationMatrix2D((cols/2,rows/2),ang_rot,1)\n",
        "\n",
        "    # Translation\n",
        "    tr_x = trans_range*np.random.uniform()-trans_range/2\n",
        "    tr_y = trans_range*np.random.uniform()-trans_range/2\n",
        "    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])\n",
        "\n",
        "    # Shear\n",
        "    pts1 = np.float32([[5,5],[20,5],[5,20]])\n",
        "\n",
        "    pt1 = 5+shear_range*np.random.uniform()-shear_range/2\n",
        "    pt2 = 20+shear_range*np.random.uniform()-shear_range/2\n",
        "\n",
        "    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])\n",
        "\n",
        "    shear_M = cv2.getAffineTransform(pts1,pts2)\n",
        "\n",
        "    # Border\n",
        "    idx = 0\n",
        "    border_img = tuple([int(img[idx][0][0]), int(img[idx][0][1]), int(img[idx][0][2])])\n",
        "    border_skt = tuple([int(skt[0][0][0]), int(skt[0][0][1]), int(skt[0][0][2])])\n",
        "    \n",
        "    img = cv2.warpAffine(img,Rot_M,(cols,rows), borderValue=border_img)\n",
        "    img = cv2.warpAffine(img,Trans_M,(cols,rows), borderValue=border_img)\n",
        "    img = cv2.warpAffine(img,shear_M,(cols,rows), borderValue=border_img)\n",
        "    \n",
        "    skt = cv2.warpAffine(skt,Rot_M,(cols,rows), borderValue=border_skt)\n",
        "    skt = cv2.warpAffine(skt,Trans_M,(cols,rows), borderValue=border_skt)\n",
        "    skt = cv2.warpAffine(skt,shear_M,(cols,rows), borderValue=border_skt)\n",
        "\n",
        "    return img, skt"
      ],
      "metadata": {
        "id": "9bqvtrhkHpGG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sketch_dir = 'Dataset/Augmented sketch/'\n",
        "photo_dir = 'Dataset/Augmented photo/'\n",
        "\n",
        "if not os.path.exists(sketch_dir):\n",
        "    os.mkdir(sketch_dir)\n",
        "\n",
        "if not os.path.exists(photo_dir):\n",
        "    os.mkdir(photo_dir)\n",
        "\n",
        "p_filenames = glob.glob('Dataset/CUHK/Training photo/*')\n",
        "s_filenames = glob.glob('Dataset/CUHK/Training sketch/*')"
      ],
      "metadata": {
        "id": "rKaJ8ORwHvd4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sketch_dir = 'Dataset/Augmented sketch/'\n",
        "photo_dir = 'Dataset/Augmented photo/'\n",
        "\n",
        "if not os.path.exists(sketch_dir):\n",
        "    os.mkdir(sketch_dir)\n",
        "\n",
        "if not os.path.exists(photo_dir):\n",
        "    os.mkdir(photo_dir)\n",
        "\n",
        "p_filenames = glob.glob('Dataset/CUHK/Training photo/*')\n",
        "s_filenames = glob.glob('Dataset/CUHK/Training sketch/*')"
      ],
      "metadata": {
        "id": "W_sjFPCyH4xI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "for i in range(len(p_filenames)):\n",
        "    im = cv2.imread(p_filenames[i])\n",
        "    sk = cv2.imread(s_filenames[i])\n",
        "\n",
        "    for j in range(200):\n",
        "        img, skt = transform_image(im, sk, 40, 10, 10)\n",
        "\n",
        "        cv2.imwrite(photo_dir + str(counter) + '.jpg', img)\n",
        "        cv2.imwrite(sketch_dir + str(counter) + '.jpg', skt)\n",
        "\n",
        "        counter += 1"
      ],
      "metadata": {
        "id": "nrJxaMGLIBFQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://www.github.com/keras-team/keras-contrib.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdlx9ZG2IEt8",
        "outputId": "51735c91-72ec-42ac-d720-17b212debdfd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'keras-contrib'...\n",
            "warning: redirecting to https://github.com/keras-team/keras-contrib.git/\n",
            "remote: Enumerating objects: 3634, done.\u001b[K\n",
            "remote: Total 3634 (delta 0), reused 0 (delta 0), pack-reused 3634\u001b[K\n",
            "Receiving objects: 100% (3634/3634), 861.24 KiB | 7.49 MiB/s, done.\n",
            "Resolving deltas: 100% (2330/2330), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd keras-contrib/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9pEU6XCI8qG",
        "outputId": "36793c79-d054-49c6-aecd-0e4a0de8b69d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Face-Sketch-to-Image-Generation-using-GAN/keras-contrib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup.py install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X4E4YpBJFxL",
        "outputId": "297a9d5d-5fdf-41b7-99eb-ba5f49732bf6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating keras_contrib.egg-info\n",
            "writing keras_contrib.egg-info/PKG-INFO\n",
            "writing dependency_links to keras_contrib.egg-info/dependency_links.txt\n",
            "writing requirements to keras_contrib.egg-info/requires.txt\n",
            "writing top-level names to keras_contrib.egg-info/top_level.txt\n",
            "writing manifest file 'keras_contrib.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'keras_contrib.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/keras_contrib\n",
            "copying keras_contrib/__init__.py -> build/lib/keras_contrib\n",
            "creating build/lib/keras_contrib/activations\n",
            "copying keras_contrib/activations/__init__.py -> build/lib/keras_contrib/activations\n",
            "copying keras_contrib/activations/squash.py -> build/lib/keras_contrib/activations\n",
            "creating build/lib/keras_contrib/callbacks\n",
            "copying keras_contrib/callbacks/snapshot.py -> build/lib/keras_contrib/callbacks\n",
            "copying keras_contrib/callbacks/tensorboard.py -> build/lib/keras_contrib/callbacks\n",
            "copying keras_contrib/callbacks/__init__.py -> build/lib/keras_contrib/callbacks\n",
            "copying keras_contrib/callbacks/dead_relu_detector.py -> build/lib/keras_contrib/callbacks\n",
            "copying keras_contrib/callbacks/cyclical_learning_rate.py -> build/lib/keras_contrib/callbacks\n",
            "creating build/lib/keras_contrib/metrics\n",
            "copying keras_contrib/metrics/crf_accuracies.py -> build/lib/keras_contrib/metrics\n",
            "copying keras_contrib/metrics/__init__.py -> build/lib/keras_contrib/metrics\n",
            "creating build/lib/keras_contrib/utils\n",
            "copying keras_contrib/utils/save_load_utils.py -> build/lib/keras_contrib/utils\n",
            "copying keras_contrib/utils/conv_utils.py -> build/lib/keras_contrib/utils\n",
            "copying keras_contrib/utils/__init__.py -> build/lib/keras_contrib/utils\n",
            "copying keras_contrib/utils/test_utils.py -> build/lib/keras_contrib/utils\n",
            "creating build/lib/keras_contrib/optimizers\n",
            "copying keras_contrib/optimizers/padam.py -> build/lib/keras_contrib/optimizers\n",
            "copying keras_contrib/optimizers/ftml.py -> build/lib/keras_contrib/optimizers\n",
            "copying keras_contrib/optimizers/yogi.py -> build/lib/keras_contrib/optimizers\n",
            "copying keras_contrib/optimizers/lars.py -> build/lib/keras_contrib/optimizers\n",
            "copying keras_contrib/optimizers/__init__.py -> build/lib/keras_contrib/optimizers\n",
            "creating build/lib/keras_contrib/losses\n",
            "copying keras_contrib/losses/dssim.py -> build/lib/keras_contrib/losses\n",
            "copying keras_contrib/losses/__init__.py -> build/lib/keras_contrib/losses\n",
            "copying keras_contrib/losses/crf_losses.py -> build/lib/keras_contrib/losses\n",
            "copying keras_contrib/losses/jaccard.py -> build/lib/keras_contrib/losses\n",
            "creating build/lib/keras_contrib/preprocessing\n",
            "copying keras_contrib/preprocessing/__init__.py -> build/lib/keras_contrib/preprocessing\n",
            "creating build/lib/keras_contrib/constraints\n",
            "copying keras_contrib/constraints/clip.py -> build/lib/keras_contrib/constraints\n",
            "copying keras_contrib/constraints/__init__.py -> build/lib/keras_contrib/constraints\n",
            "creating build/lib/keras_contrib/backend\n",
            "copying keras_contrib/backend/numpy_backend.py -> build/lib/keras_contrib/backend\n",
            "copying keras_contrib/backend/theano_backend.py -> build/lib/keras_contrib/backend\n",
            "copying keras_contrib/backend/cntk_backend.py -> build/lib/keras_contrib/backend\n",
            "copying keras_contrib/backend/__init__.py -> build/lib/keras_contrib/backend\n",
            "copying keras_contrib/backend/tensorflow_backend.py -> build/lib/keras_contrib/backend\n",
            "creating build/lib/keras_contrib/initializers\n",
            "copying keras_contrib/initializers/convaware.py -> build/lib/keras_contrib/initializers\n",
            "copying keras_contrib/initializers/__init__.py -> build/lib/keras_contrib/initializers\n",
            "creating build/lib/keras_contrib/applications\n",
            "copying keras_contrib/applications/resnet.py -> build/lib/keras_contrib/applications\n",
            "copying keras_contrib/applications/__init__.py -> build/lib/keras_contrib/applications\n",
            "copying keras_contrib/applications/wide_resnet.py -> build/lib/keras_contrib/applications\n",
            "copying keras_contrib/applications/nasnet.py -> build/lib/keras_contrib/applications\n",
            "copying keras_contrib/applications/densenet.py -> build/lib/keras_contrib/applications\n",
            "creating build/lib/keras_contrib/layers\n",
            "copying keras_contrib/layers/__init__.py -> build/lib/keras_contrib/layers\n",
            "copying keras_contrib/layers/capsule.py -> build/lib/keras_contrib/layers\n",
            "copying keras_contrib/layers/core.py -> build/lib/keras_contrib/layers\n",
            "copying keras_contrib/layers/crf.py -> build/lib/keras_contrib/layers\n",
            "creating build/lib/keras_contrib/tests\n",
            "copying keras_contrib/tests/regularizers.py -> build/lib/keras_contrib/tests\n",
            "copying keras_contrib/tests/metrics.py -> build/lib/keras_contrib/tests\n",
            "copying keras_contrib/tests/__init__.py -> build/lib/keras_contrib/tests\n",
            "copying keras_contrib/tests/activations.py -> build/lib/keras_contrib/tests\n",
            "copying keras_contrib/tests/optimizers.py -> build/lib/keras_contrib/tests\n",
            "creating build/lib/keras_contrib/datasets\n",
            "copying keras_contrib/datasets/coco.py -> build/lib/keras_contrib/datasets\n",
            "copying keras_contrib/datasets/conll2000.py -> build/lib/keras_contrib/datasets\n",
            "copying keras_contrib/datasets/__init__.py -> build/lib/keras_contrib/datasets\n",
            "copying keras_contrib/datasets/pascal_voc.py -> build/lib/keras_contrib/datasets\n",
            "creating build/lib/keras_contrib/regularizers\n",
            "copying keras_contrib/regularizers/__init__.py -> build/lib/keras_contrib/regularizers\n",
            "creating build/lib/keras_contrib/wrappers\n",
            "copying keras_contrib/wrappers/__init__.py -> build/lib/keras_contrib/wrappers\n",
            "creating build/lib/keras_contrib/layers/advanced_activations\n",
            "copying keras_contrib/layers/advanced_activations/srelu.py -> build/lib/keras_contrib/layers/advanced_activations\n",
            "copying keras_contrib/layers/advanced_activations/swish.py -> build/lib/keras_contrib/layers/advanced_activations\n",
            "copying keras_contrib/layers/advanced_activations/__init__.py -> build/lib/keras_contrib/layers/advanced_activations\n",
            "copying keras_contrib/layers/advanced_activations/pelu.py -> build/lib/keras_contrib/layers/advanced_activations\n",
            "copying keras_contrib/layers/advanced_activations/sinerelu.py -> build/lib/keras_contrib/layers/advanced_activations\n",
            "creating build/lib/keras_contrib/layers/normalization\n",
            "copying keras_contrib/layers/normalization/__init__.py -> build/lib/keras_contrib/layers/normalization\n",
            "copying keras_contrib/layers/normalization/instancenormalization.py -> build/lib/keras_contrib/layers/normalization\n",
            "copying keras_contrib/layers/normalization/groupnormalization.py -> build/lib/keras_contrib/layers/normalization\n",
            "creating build/lib/keras_contrib/layers/convolutional\n",
            "copying keras_contrib/layers/convolutional/__init__.py -> build/lib/keras_contrib/layers/convolutional\n",
            "copying keras_contrib/layers/convolutional/cosineconvolution2d.py -> build/lib/keras_contrib/layers/convolutional\n",
            "copying keras_contrib/layers/convolutional/subpixelupscaling.py -> build/lib/keras_contrib/layers/convolutional\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/activations\n",
            "copying build/lib/keras_contrib/activations/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/activations\n",
            "copying build/lib/keras_contrib/activations/squash.py -> build/bdist.linux-x86_64/egg/keras_contrib/activations\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/callbacks\n",
            "copying build/lib/keras_contrib/callbacks/snapshot.py -> build/bdist.linux-x86_64/egg/keras_contrib/callbacks\n",
            "copying build/lib/keras_contrib/callbacks/tensorboard.py -> build/bdist.linux-x86_64/egg/keras_contrib/callbacks\n",
            "copying build/lib/keras_contrib/callbacks/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/callbacks\n",
            "copying build/lib/keras_contrib/callbacks/dead_relu_detector.py -> build/bdist.linux-x86_64/egg/keras_contrib/callbacks\n",
            "copying build/lib/keras_contrib/callbacks/cyclical_learning_rate.py -> build/bdist.linux-x86_64/egg/keras_contrib/callbacks\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/metrics\n",
            "copying build/lib/keras_contrib/metrics/crf_accuracies.py -> build/bdist.linux-x86_64/egg/keras_contrib/metrics\n",
            "copying build/lib/keras_contrib/metrics/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/metrics\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/utils\n",
            "copying build/lib/keras_contrib/utils/save_load_utils.py -> build/bdist.linux-x86_64/egg/keras_contrib/utils\n",
            "copying build/lib/keras_contrib/utils/conv_utils.py -> build/bdist.linux-x86_64/egg/keras_contrib/utils\n",
            "copying build/lib/keras_contrib/utils/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/utils\n",
            "copying build/lib/keras_contrib/utils/test_utils.py -> build/bdist.linux-x86_64/egg/keras_contrib/utils\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/optimizers\n",
            "copying build/lib/keras_contrib/optimizers/padam.py -> build/bdist.linux-x86_64/egg/keras_contrib/optimizers\n",
            "copying build/lib/keras_contrib/optimizers/ftml.py -> build/bdist.linux-x86_64/egg/keras_contrib/optimizers\n",
            "copying build/lib/keras_contrib/optimizers/yogi.py -> build/bdist.linux-x86_64/egg/keras_contrib/optimizers\n",
            "copying build/lib/keras_contrib/optimizers/lars.py -> build/bdist.linux-x86_64/egg/keras_contrib/optimizers\n",
            "copying build/lib/keras_contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/optimizers\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/losses\n",
            "copying build/lib/keras_contrib/losses/dssim.py -> build/bdist.linux-x86_64/egg/keras_contrib/losses\n",
            "copying build/lib/keras_contrib/losses/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/losses\n",
            "copying build/lib/keras_contrib/losses/crf_losses.py -> build/bdist.linux-x86_64/egg/keras_contrib/losses\n",
            "copying build/lib/keras_contrib/losses/jaccard.py -> build/bdist.linux-x86_64/egg/keras_contrib/losses\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/preprocessing\n",
            "copying build/lib/keras_contrib/preprocessing/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/preprocessing\n",
            "copying build/lib/keras_contrib/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/constraints\n",
            "copying build/lib/keras_contrib/constraints/clip.py -> build/bdist.linux-x86_64/egg/keras_contrib/constraints\n",
            "copying build/lib/keras_contrib/constraints/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/constraints\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/backend\n",
            "copying build/lib/keras_contrib/backend/numpy_backend.py -> build/bdist.linux-x86_64/egg/keras_contrib/backend\n",
            "copying build/lib/keras_contrib/backend/theano_backend.py -> build/bdist.linux-x86_64/egg/keras_contrib/backend\n",
            "copying build/lib/keras_contrib/backend/cntk_backend.py -> build/bdist.linux-x86_64/egg/keras_contrib/backend\n",
            "copying build/lib/keras_contrib/backend/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/backend\n",
            "copying build/lib/keras_contrib/backend/tensorflow_backend.py -> build/bdist.linux-x86_64/egg/keras_contrib/backend\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/initializers\n",
            "copying build/lib/keras_contrib/initializers/convaware.py -> build/bdist.linux-x86_64/egg/keras_contrib/initializers\n",
            "copying build/lib/keras_contrib/initializers/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/initializers\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/applications\n",
            "copying build/lib/keras_contrib/applications/resnet.py -> build/bdist.linux-x86_64/egg/keras_contrib/applications\n",
            "copying build/lib/keras_contrib/applications/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/applications\n",
            "copying build/lib/keras_contrib/applications/wide_resnet.py -> build/bdist.linux-x86_64/egg/keras_contrib/applications\n",
            "copying build/lib/keras_contrib/applications/nasnet.py -> build/bdist.linux-x86_64/egg/keras_contrib/applications\n",
            "copying build/lib/keras_contrib/applications/densenet.py -> build/bdist.linux-x86_64/egg/keras_contrib/applications\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/layers\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations\n",
            "copying build/lib/keras_contrib/layers/advanced_activations/srelu.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations\n",
            "copying build/lib/keras_contrib/layers/advanced_activations/swish.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations\n",
            "copying build/lib/keras_contrib/layers/advanced_activations/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations\n",
            "copying build/lib/keras_contrib/layers/advanced_activations/pelu.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations\n",
            "copying build/lib/keras_contrib/layers/advanced_activations/sinerelu.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations\n",
            "copying build/lib/keras_contrib/layers/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers\n",
            "copying build/lib/keras_contrib/layers/capsule.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers\n",
            "copying build/lib/keras_contrib/layers/core.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization\n",
            "copying build/lib/keras_contrib/layers/normalization/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization\n",
            "copying build/lib/keras_contrib/layers/normalization/instancenormalization.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization\n",
            "copying build/lib/keras_contrib/layers/normalization/groupnormalization.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional\n",
            "copying build/lib/keras_contrib/layers/convolutional/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional\n",
            "copying build/lib/keras_contrib/layers/convolutional/cosineconvolution2d.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional\n",
            "copying build/lib/keras_contrib/layers/convolutional/subpixelupscaling.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional\n",
            "copying build/lib/keras_contrib/layers/crf.py -> build/bdist.linux-x86_64/egg/keras_contrib/layers\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/tests\n",
            "copying build/lib/keras_contrib/tests/regularizers.py -> build/bdist.linux-x86_64/egg/keras_contrib/tests\n",
            "copying build/lib/keras_contrib/tests/metrics.py -> build/bdist.linux-x86_64/egg/keras_contrib/tests\n",
            "copying build/lib/keras_contrib/tests/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/tests\n",
            "copying build/lib/keras_contrib/tests/activations.py -> build/bdist.linux-x86_64/egg/keras_contrib/tests\n",
            "copying build/lib/keras_contrib/tests/optimizers.py -> build/bdist.linux-x86_64/egg/keras_contrib/tests\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/datasets\n",
            "copying build/lib/keras_contrib/datasets/coco.py -> build/bdist.linux-x86_64/egg/keras_contrib/datasets\n",
            "copying build/lib/keras_contrib/datasets/conll2000.py -> build/bdist.linux-x86_64/egg/keras_contrib/datasets\n",
            "copying build/lib/keras_contrib/datasets/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/datasets\n",
            "copying build/lib/keras_contrib/datasets/pascal_voc.py -> build/bdist.linux-x86_64/egg/keras_contrib/datasets\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/regularizers\n",
            "copying build/lib/keras_contrib/regularizers/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/regularizers\n",
            "creating build/bdist.linux-x86_64/egg/keras_contrib/wrappers\n",
            "copying build/lib/keras_contrib/wrappers/__init__.py -> build/bdist.linux-x86_64/egg/keras_contrib/wrappers\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/activations/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/activations/squash.py to squash.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/callbacks/snapshot.py to snapshot.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/callbacks/tensorboard.py to tensorboard.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/callbacks/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/callbacks/dead_relu_detector.py to dead_relu_detector.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/callbacks/cyclical_learning_rate.py to cyclical_learning_rate.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/metrics/crf_accuracies.py to crf_accuracies.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/metrics/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/utils/save_load_utils.py to save_load_utils.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/utils/conv_utils.py to conv_utils.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/utils/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/utils/test_utils.py to test_utils.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/optimizers/padam.py to padam.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/optimizers/ftml.py to ftml.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/optimizers/yogi.py to yogi.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/optimizers/lars.py to lars.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/optimizers/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/losses/dssim.py to dssim.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/losses/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/losses/crf_losses.py to crf_losses.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/losses/jaccard.py to jaccard.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/preprocessing/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/constraints/clip.py to clip.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/constraints/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/backend/numpy_backend.py to numpy_backend.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/backend/theano_backend.py to theano_backend.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/backend/cntk_backend.py to cntk_backend.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/backend/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/backend/tensorflow_backend.py to tensorflow_backend.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/initializers/convaware.py to convaware.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/initializers/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/applications/resnet.py to resnet.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/applications/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/applications/wide_resnet.py to wide_resnet.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/applications/nasnet.py to nasnet.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/applications/densenet.py to densenet.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations/srelu.py to srelu.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations/swish.py to swish.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations/pelu.py to pelu.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/advanced_activations/sinerelu.py to sinerelu.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/capsule.py to capsule.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/core.py to core.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization/instancenormalization.py to instancenormalization.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/normalization/groupnormalization.py to groupnormalization.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional/cosineconvolution2d.py to cosineconvolution2d.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/convolutional/subpixelupscaling.py to subpixelupscaling.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/layers/crf.py to crf.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/tests/regularizers.py to regularizers.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/tests/metrics.py to metrics.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/tests/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/tests/activations.py to activations.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/tests/optimizers.py to optimizers.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/datasets/coco.py to coco.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/datasets/conll2000.py to conll2000.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/datasets/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/datasets/pascal_voc.py to pascal_voc.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/regularizers/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/keras_contrib/wrappers/__init__.py to __init__.cpython-37.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying keras_contrib.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying keras_contrib.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying keras_contrib.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying keras_contrib.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying keras_contrib.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/keras_contrib-2.0.8-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing keras_contrib-2.0.8-py3.7.egg\n",
            "Copying keras_contrib-2.0.8-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
            "Adding keras-contrib 2.0.8 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/keras_contrib-2.0.8-py3.7.egg\n",
            "Processing dependencies for keras-contrib==2.0.8\n",
            "Searching for keras==2.7.0\n",
            "Best match: keras 2.7.0\n",
            "Adding keras 2.7.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for keras-contrib==2.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeJpBsYCJJdB",
        "outputId": "bb38a75d-78da-4d5e-a4f1-f59aec73fc55"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Face-Sketch-to-Image-Generation-using-GAN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate, BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import load_img\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import datetime\n",
        "import natsort\n",
        "import scipy\n",
        "import sys\n",
        "import os\n",
        "import cv2"
      ],
      "metadata": {
        "id": "J8hGVIS7JQDp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_filename(path):\n",
        "    dirFiles = os.listdir(path)\n",
        "    for i, file in enumerate(dirFiles):\n",
        "        dirFiles[i] = path + file\n",
        "    return natsort.natsorted(dirFiles ,reverse=False)\n",
        "\n",
        "# load all images in a directory into memory\n",
        "def load_images(list_path, size=(256, 256)):\n",
        "    img_list = list()\n",
        "    # enumerate filenames in directory, assume all are images\n",
        "    for filename in list_path:\n",
        "        # load and resize the image\n",
        "        pixels = load_img(filename, target_size=size)\n",
        "        # convert to numpy array\n",
        "        pixels = img_to_array(pixels)\n",
        "        pixels = (pixels - 127.5) / 127.5\n",
        "        img_list.append(pixels)\n",
        "    return np.asarray(img_list)"
      ],
      "metadata": {
        "id": "xYVShJqiJTEo"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select a batch of random samples, returns images and target\n",
        "def generate_real_samples(dataset, n_samples, patch_shape):\n",
        "    # unpack dataset\n",
        "    trainA, trainB = dataset\n",
        "\n",
        "    # choose random instances\n",
        "    ix = np.random.randint(0, trainA.shape[0], n_samples)\n",
        "    \n",
        "    # retrieve selected images\n",
        "    X1, X2 = trainA[ix], trainB[ix]\n",
        "    \n",
        "    # generate 'real' class labels (1)\n",
        "    y = np.ones((n_samples, patch_shape, patch_shape, 1))\n",
        "    \n",
        "    return [X1, X2], y\n",
        "\n",
        "# generate a batch of images, returns images and targets\n",
        "def generate_fake_samples(g_model, samples, patch_shape):\n",
        "    # generate fake instance\n",
        "    X = g_model.predict(samples)\n",
        "    \n",
        "    # create 'fake' class labels (0)\n",
        "    y = np.zeros((len(X), patch_shape, patch_shape, 1))\n",
        "    \n",
        "    return X, y"
      ],
      "metadata": {
        "id": "auJKhZHiKREn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate samples and save as a plot and save the model\n",
        "def summarize_performance(step, g_model, d_model, dataset, target_dir='', n_samples=3):\n",
        "    if target_dir and not os.path.exists(target_dir):\n",
        "        os.mkdir(target_dir)\n",
        "    # select a sample of input images\n",
        "    [X_realA, X_realB], _ = generate_real_samples(dataset, n_samples, 1)\n",
        "    # generate a batch of fake samples\n",
        "    X_fakeB, _ = generate_fake_samples(g_model, X_realA, 1)\n",
        "    # scale all pixels from [-1,1] to [0,1]\n",
        "    X_realA = (X_realA + 1) / 2.0\n",
        "    X_realB = (X_realB + 1) / 2.0\n",
        "    X_fakeB = (X_fakeB + 1) / 2.0\n",
        "    # plot real source images\n",
        "    for i in range(n_samples):\n",
        "        plt.subplot(3, n_samples, 1 + i)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(X_realA[i])\n",
        "    # plot generated target image\n",
        "    for i in range(n_samples):\n",
        "        plt.subplot(3, n_samples, 1 + n_samples + i)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(X_fakeB[i])\n",
        "    # plot real target image\n",
        "    for i in range(n_samples):\n",
        "        plt.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(X_realB[i])\n",
        "    # save plot to file\n",
        "    filename1 = 'plot_%06d.png' % (step+1)\n",
        "    plt.savefig(target_dir + filename1)\n",
        "    plt.close()\n",
        "    # save the generator model\n",
        "    g_model.save(target_dir + 'g_model.h5')\n",
        "    \n",
        "    # save the discriminator model\n",
        "    d_model.save(target_dir + 'd_model.h5')\n",
        "    \n",
        "    print('>Saved: %s and %s' % (filename1, 'g_model & d_model'))"
      ],
      "metadata": {
        "id": "hSbIVtnmKVo-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator(img_shape):\n",
        "    def conv2d(layer_in, n_filter, norm=True):\n",
        "        d = Conv2D(n_filter, kernel_size=4, strides=2, padding='same')(layer_in)\n",
        "        d = LeakyReLU(0.2)(d)\n",
        "        if norm:\n",
        "            d = InstanceNormalization()(d)\n",
        "        return d\n",
        "    \n",
        "    def deconv2d(layer_in, skip_in, n_filter, dropout=0.5):\n",
        "        d = UpSampling2D(size=2)(layer_in)\n",
        "        d = Conv2D(n_filter, kernel_size=4, strides=1, padding='same', activation='relu')(d)\n",
        "        if dropout:\n",
        "            d = Dropout(dropout)(d)\n",
        "        d = InstanceNormalization()(d)\n",
        "        d = Concatenate()([d, skip_in])\n",
        "        return d\n",
        "    \n",
        "    # Input Layer\n",
        "    in_img = Input(shape=img_shape)\n",
        "    \n",
        "    # Downsampling\n",
        "    d1 = conv2d(in_img, 64, norm=False)\n",
        "    d2 = conv2d(d1, 128)\n",
        "    d3 = conv2d(d2, 256)\n",
        "    d4 = conv2d(d3, 512)\n",
        "    d5 = conv2d(d4, 512)\n",
        "    d6 = conv2d(d5, 512)\n",
        "    d7 = conv2d(d6, 512)\n",
        "    \n",
        "    # Upsampling\n",
        "    u1 = deconv2d(d7, d6, 512)\n",
        "    u2 = deconv2d(u1, d5, 512)\n",
        "    u3 = deconv2d(u2, d4, 512)\n",
        "    u4 = deconv2d(u3, d3, 256, dropout=0)\n",
        "    u5 = deconv2d(u4, d2, 128, dropout=0)\n",
        "    u6 = deconv2d(u5, d1, 64, dropout=0)\n",
        "    u7 = UpSampling2D(size=2)(u6)\n",
        "    \n",
        "    out_img = Conv2D(3, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n",
        "    \n",
        "    return Model(in_img, out_img, name='generator')"
      ],
      "metadata": {
        "id": "3vL38LAcKZs4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminator(img_shape):\n",
        "    def d_layer(layer_in, n_filter, norm=True):\n",
        "        d = Conv2D(n_filter, kernel_size=4, strides=2, padding='same')(layer_in)\n",
        "        d = LeakyReLU(0.2)(d)\n",
        "        if norm:\n",
        "            d = InstanceNormalization()(d)\n",
        "        return d\n",
        "    \n",
        "    in_src_img = Input(shape=img_shape)\n",
        "    in_target_img = Input(shape=img_shape)\n",
        "    \n",
        "    merged = Concatenate()([in_src_img, in_target_img])\n",
        "    \n",
        "    d1 = d_layer(merged, 64, norm=False)\n",
        "    d2 = d_layer(d1, 128)\n",
        "    d3 = d_layer(d1, 256)\n",
        "    d4 = d_layer(d1, 512)\n",
        "\n",
        "    out = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
        "    \n",
        "    return Model([in_src_img, in_target_img], out, name='discriminator')"
      ],
      "metadata": {
        "id": "qETHPoJgKgQG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GAN(g_model, d_model, img_shape):\n",
        "    d_model.trainable = False\n",
        "    in_img = Input(shape=img_shape)\n",
        "    gen_out = g_model(in_img)\n",
        "    dis_out = d_model([in_img, gen_out])\n",
        "    model = Model(in_img, [dis_out, gen_out], name='GAN')\n",
        "    return model"
      ],
      "metadata": {
        "id": "dXPaeZtaKkMT"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(d_model, g_model, gan_model, data, target_dir, n_epochs=100, n_batch=16):\n",
        "    # determine the output square shape of the discriminator\n",
        "    n_patch = d_model.output_shape[1]\n",
        "    \n",
        "    blue_photo = data[0]\n",
        "    blue_sketch = data[1]\n",
        "    \n",
        "    for i in range(n_epochs):\n",
        "        print(' ========== Epoch', i+1, '========== ')\n",
        "        \n",
        "        blue_photo, blue_sketch = shuffle(blue_photo, blue_sketch)\n",
        "\n",
        "        for j in range(int(len(blue_photo)/n_batch)):\n",
        "            \n",
        "            start = int(j*n_batch)\n",
        "            end = int(min(len(blue_photo), (j*n_batch)+n_batch))\n",
        "            \n",
        "            dataset = [load_images(blue_photo[start:end]), load_images(blue_sketch[start:end])]\n",
        "\n",
        "            # select a batch of real samples\n",
        "            [X_realA, X_realB], y_real = generate_real_samples(dataset, n_batch, n_patch)\n",
        "            \n",
        "            # generate a batch of fake samples\n",
        "            X_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n",
        "            \n",
        "            # update discriminator for real samples\n",
        "            d_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
        "            \n",
        "            # update discriminator for generated samples\n",
        "            d_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
        "            \n",
        "            d_loss = 0.5 * np.add(d_loss1, d_loss2)\n",
        "            \n",
        "            # update the generator\n",
        "            g_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n",
        "            \n",
        "            # summarize performance\n",
        "            print('Batch : %d, D Loss : %.3f | G Loss : %.3f' % (j+1, d_loss, g_loss))\n",
        "        \n",
        "        # summarize model performance\n",
        "#         if (i+1) % 10 == 0:\n",
        "        summarize_performance(i, g_model, d_model, dataset, target_dir)"
      ],
      "metadata": {
        "id": "jdnBqPTMKnTE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "from keras.losses import mean_absolute_error\n",
        "\n",
        "def pixel_loss(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_true - y_pred))\n",
        "\n",
        "def contextual_loss (y_true, y_pred):\n",
        "    a = tf.image.rgb_to_grayscale(tf.slice(\n",
        "                                y_pred, \n",
        "                                [0,0,0,0], \n",
        "                                [16, 256, 256, 3]))\n",
        "    \n",
        "    b = tf.image.rgb_to_grayscale(tf.slice(\n",
        "                                y_true, \n",
        "                                [0,0,0,0], \n",
        "                                [16, 256, 256, 3]))\n",
        "    \n",
        "    y_pred = tf.divide(tf.add(tf.reshape(a, [tf.shape(a)[0], -1]), 1), 2)\n",
        "    y_true = tf.divide(tf.add(tf.reshape(b, [tf.shape(b)[0], -1]), 1), 2)\n",
        "    \n",
        "#     tf.assert_rank(y_true,2)\n",
        "#     tf.assert_rank(y_pred,2)\n",
        "    \n",
        "    p_shape = tf.shape(y_true)\n",
        "    q_shape = tf.shape(y_pred)\n",
        "#     tf.assert_equal(p_shape, q_shape)\n",
        "    \n",
        "    # normalize sum to 1\n",
        "    p_ = tf.divide(y_true, tf.tile(tf.expand_dims(tf.reduce_sum(y_true, axis=1), 1), [1,p_shape[1]]))\n",
        "    q_ = tf.divide(y_pred, tf.tile(tf.expand_dims(tf.reduce_sum(y_pred, axis=1), 1), [1,p_shape[1]]))\n",
        "    \n",
        "    return tf.reduce_sum(tf.multiply(p_, tf.math.log(tf.divide(p_, q_))), axis=1)\n",
        "\n",
        "def total_loss (y_true, y_pred):\n",
        "\n",
        "    px_loss = pixel_loss(y_true, y_pred)\n",
        "\n",
        "    ctx_loss = contextual_loss(y_true, y_pred)\n",
        "    \n",
        "    return (0.2 * px_loss) + (0.8 * ctx_loss)"
      ],
      "metadata": {
        "id": "6hjpdY3UKtWq"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset path\n",
        "b_photo_path = 'Dataset/Augmented photo/'\n",
        "b_sketch_path = 'Dataset/Augmented sketch/'\n",
        "\n",
        "blue_photo = load_filename(b_photo_path)\n",
        "blue_sketch = load_filename(b_sketch_path)"
      ],
      "metadata": {
        "id": "Yu4Wsk1oKy5A"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(cv2.cvtColor(cv2.imread(blue_photo[1102]).astype('uint8'), cv2.COLOR_BGR2RGB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "QpDpCjFeK2Ce",
        "outputId": "ea34ffd5-d39f-41c8-ce18-915113b0a00b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcdf0092b90>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(cv2.cvtColor(cv2.imread(blue_sketch[1102]).astype('uint8'), cv2.COLOR_BGR2RGB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "v0iNOHaHK4k-",
        "outputId": "120025e5-cc16-485f-ee1f-12f760ba20b3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcdf018c490>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_shape = (256, 256, 3)\n",
        "\n",
        "d_model = discriminator(img_shape)\n",
        "\n",
        "g_model = generator(img_shape)\n",
        "\n",
        "gan_model = GAN(g_model, d_model, img_shape)"
      ],
      "metadata": {
        "id": "pcQ7pbM1K7mR"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gan_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJKsNDl0K--h",
        "outputId": "ae6803cc-c11b-4be5-8f05-886d46532e1f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"GAN\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_8 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " generator (Functional)         (None, 256, 256, 3)  41825691    ['input_8[0][0]']                \n",
            "                                                                                                  \n",
            " discriminator (Functional)     (None, 64, 64, 1)    539203      ['input_8[0][0]',                \n",
            "                                                                  'generator[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 42,364,894\n",
            "Trainable params: 41,825,691\n",
            "Non-trainable params: 539,203\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = Adam(lr=2e-4, beta_1=0.5)\n",
        "\n",
        "d_model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
        "gan_model.compile(loss=['binary_crossentropy', total_loss], optimizer=opt, loss_weights=[1,100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD-fa6muLB97",
        "outputId": "3984bbf8-29ce-4234-90cb-96e914674832"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(d_model, g_model, gan_model, [blue_sketch, blue_photo], 'Models/Pixel[02]_Context[08]/', n_epochs = 100, n_batch=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3ptJslYLGhe",
        "outputId": "a441e763-e999-4230-c7a0-ef3faa505177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ========== Epoch 1 ========== \n",
            "Batch : 1, D Loss : 3.600 | G Loss : 37.221\n",
            "Batch : 2, D Loss : 3.703 | G Loss : 29.210\n",
            "Batch : 3, D Loss : 4.488 | G Loss : 21.521\n",
            "Batch : 4, D Loss : 6.662 | G Loss : 22.072\n",
            "Batch : 5, D Loss : 6.740 | G Loss : 21.110\n",
            "Batch : 6, D Loss : 6.379 | G Loss : 21.846\n",
            "Batch : 7, D Loss : 6.701 | G Loss : 23.375\n",
            "Batch : 8, D Loss : 6.568 | G Loss : 20.250\n",
            "Batch : 9, D Loss : 6.380 | G Loss : 19.314\n",
            "Batch : 10, D Loss : 6.456 | G Loss : 22.719\n",
            "Batch : 11, D Loss : 6.546 | G Loss : 18.761\n",
            "Batch : 12, D Loss : 6.429 | G Loss : 20.320\n",
            "Batch : 13, D Loss : 6.578 | G Loss : 19.349\n",
            "Batch : 14, D Loss : 6.590 | G Loss : 20.558\n",
            "Batch : 15, D Loss : 6.699 | G Loss : 20.656\n",
            "Batch : 16, D Loss : 6.615 | G Loss : 19.617\n",
            "Batch : 17, D Loss : 6.425 | G Loss : 21.116\n",
            "Batch : 18, D Loss : 6.036 | G Loss : 20.213\n",
            "Batch : 19, D Loss : 6.104 | G Loss : 18.704\n",
            "Batch : 20, D Loss : 5.492 | G Loss : 19.153\n",
            "Batch : 21, D Loss : 6.564 | G Loss : 18.084\n",
            "Batch : 22, D Loss : 6.310 | G Loss : 18.612\n",
            "Batch : 23, D Loss : 6.509 | G Loss : 20.984\n",
            "Batch : 24, D Loss : 6.332 | G Loss : 17.682\n",
            "Batch : 25, D Loss : 6.185 | G Loss : 17.546\n",
            "Batch : 26, D Loss : 4.906 | G Loss : 18.530\n",
            "Batch : 27, D Loss : 6.267 | G Loss : 19.474\n",
            "Batch : 28, D Loss : 6.532 | G Loss : 18.965\n",
            "Batch : 29, D Loss : 6.583 | G Loss : 18.209\n",
            "Batch : 30, D Loss : 6.255 | G Loss : 19.545\n",
            "Batch : 31, D Loss : 6.366 | G Loss : 18.043\n",
            "Batch : 32, D Loss : 6.253 | G Loss : 18.094\n",
            "Batch : 33, D Loss : 6.294 | G Loss : 17.144\n",
            "Batch : 34, D Loss : 5.958 | G Loss : 15.857\n",
            "Batch : 35, D Loss : 5.844 | G Loss : 16.917\n",
            "Batch : 36, D Loss : 5.008 | G Loss : 16.799\n",
            "Batch : 37, D Loss : 6.577 | G Loss : 20.190\n",
            "Batch : 38, D Loss : 6.707 | G Loss : 18.139\n",
            "Batch : 39, D Loss : 6.564 | G Loss : 19.380\n",
            "Batch : 40, D Loss : 5.770 | G Loss : 17.915\n",
            "Batch : 41, D Loss : 3.542 | G Loss : 24.610\n",
            "Batch : 42, D Loss : 3.633 | G Loss : 23.024\n",
            "Batch : 43, D Loss : 3.770 | G Loss : 22.710\n",
            "Batch : 44, D Loss : 6.437 | G Loss : 20.185\n",
            "Batch : 45, D Loss : 6.413 | G Loss : 18.810\n",
            "Batch : 46, D Loss : 6.568 | G Loss : 19.478\n",
            "Batch : 47, D Loss : 6.559 | G Loss : 19.268\n",
            "Batch : 48, D Loss : 6.293 | G Loss : 17.520\n",
            "Batch : 49, D Loss : 6.305 | G Loss : 17.641\n",
            "Batch : 50, D Loss : 5.900 | G Loss : 17.996\n",
            "Batch : 51, D Loss : 5.954 | G Loss : 17.057\n",
            "Batch : 52, D Loss : 6.324 | G Loss : 16.333\n",
            "Batch : 53, D Loss : 6.409 | G Loss : 18.448\n",
            "Batch : 54, D Loss : 6.259 | G Loss : 18.722\n",
            "Batch : 55, D Loss : 5.857 | G Loss : 16.617\n",
            "Batch : 56, D Loss : 4.412 | G Loss : 15.616\n",
            "Batch : 57, D Loss : 3.859 | G Loss : 20.978\n",
            "Batch : 58, D Loss : 6.403 | G Loss : 16.561\n",
            "Batch : 59, D Loss : 6.255 | G Loss : 16.416\n",
            "Batch : 60, D Loss : 4.644 | G Loss : 15.430\n",
            "Batch : 61, D Loss : 3.954 | G Loss : 20.655\n",
            "Batch : 62, D Loss : 4.213 | G Loss : 16.114\n",
            "Batch : 63, D Loss : 5.999 | G Loss : 16.349\n",
            "Batch : 64, D Loss : 6.525 | G Loss : 19.020\n",
            "Batch : 65, D Loss : 6.069 | G Loss : 16.905\n",
            "Batch : 66, D Loss : 5.785 | G Loss : 15.999\n",
            "Batch : 67, D Loss : 4.255 | G Loss : 15.610\n",
            "Batch : 68, D Loss : 3.782 | G Loss : 21.664\n",
            "Batch : 69, D Loss : 6.296 | G Loss : 18.335\n",
            "Batch : 70, D Loss : 6.166 | G Loss : 18.278\n",
            "Batch : 71, D Loss : 4.298 | G Loss : 15.223\n",
            "Batch : 72, D Loss : 3.895 | G Loss : 21.823\n",
            "Batch : 73, D Loss : 6.373 | G Loss : 19.281\n",
            "Batch : 74, D Loss : 6.452 | G Loss : 19.087\n",
            "Batch : 75, D Loss : 6.718 | G Loss : 21.392\n",
            "Batch : 76, D Loss : 6.439 | G Loss : 20.310\n",
            "Batch : 77, D Loss : 6.288 | G Loss : 18.668\n",
            "Batch : 78, D Loss : 6.345 | G Loss : 17.840\n",
            "Batch : 79, D Loss : 6.072 | G Loss : 17.209\n",
            "Batch : 80, D Loss : 6.514 | G Loss : 18.066\n",
            "Batch : 81, D Loss : 6.067 | G Loss : 17.606\n",
            "Batch : 82, D Loss : 6.365 | G Loss : 17.140\n",
            "Batch : 83, D Loss : 6.050 | G Loss : 18.590\n",
            "Batch : 84, D Loss : 6.545 | G Loss : 17.920\n",
            "Batch : 85, D Loss : 6.003 | G Loss : 18.389\n",
            "Batch : 86, D Loss : 5.827 | G Loss : 18.176\n",
            "Batch : 87, D Loss : 6.251 | G Loss : 17.630\n",
            "Batch : 88, D Loss : 6.417 | G Loss : 17.848\n",
            "Batch : 89, D Loss : 6.416 | G Loss : 18.971\n",
            "Batch : 90, D Loss : 6.173 | G Loss : 16.917\n",
            "Batch : 91, D Loss : 6.255 | G Loss : 18.106\n",
            "Batch : 92, D Loss : 5.715 | G Loss : 17.699\n",
            "Batch : 93, D Loss : 6.211 | G Loss : 17.890\n",
            "Batch : 94, D Loss : 6.412 | G Loss : 17.686\n",
            "Batch : 95, D Loss : 6.380 | G Loss : 17.600\n",
            "Batch : 96, D Loss : 6.293 | G Loss : 17.802\n",
            "Batch : 97, D Loss : 6.160 | G Loss : 17.803\n",
            "Batch : 98, D Loss : 6.315 | G Loss : 17.337\n",
            "Batch : 99, D Loss : 6.147 | G Loss : 17.033\n",
            "Batch : 100, D Loss : 6.345 | G Loss : 16.988\n",
            "Batch : 101, D Loss : 5.996 | G Loss : 17.857\n",
            "Batch : 102, D Loss : 6.401 | G Loss : 17.863\n",
            "Batch : 103, D Loss : 6.238 | G Loss : 17.233\n",
            "Batch : 104, D Loss : 6.086 | G Loss : 17.863\n",
            "Batch : 105, D Loss : 6.459 | G Loss : 17.172\n",
            "Batch : 106, D Loss : 6.165 | G Loss : 17.045\n",
            "Batch : 107, D Loss : 6.240 | G Loss : 16.435\n",
            "Batch : 108, D Loss : 6.133 | G Loss : 17.900\n",
            "Batch : 109, D Loss : 6.054 | G Loss : 18.036\n",
            "Batch : 110, D Loss : 6.112 | G Loss : 16.846\n",
            "Batch : 111, D Loss : 6.618 | G Loss : 17.953\n",
            "Batch : 112, D Loss : 5.900 | G Loss : 18.830\n",
            "Batch : 113, D Loss : 6.542 | G Loss : 18.856\n",
            "Batch : 114, D Loss : 6.300 | G Loss : 18.483\n",
            "Batch : 115, D Loss : 6.526 | G Loss : 18.428\n",
            "Batch : 116, D Loss : 6.534 | G Loss : 17.135\n",
            "Batch : 117, D Loss : 6.302 | G Loss : 18.577\n",
            "Batch : 118, D Loss : 6.087 | G Loss : 17.383\n",
            "Batch : 119, D Loss : 6.348 | G Loss : 18.630\n",
            "Batch : 120, D Loss : 6.039 | G Loss : 16.406\n",
            "Batch : 121, D Loss : 6.249 | G Loss : 16.383\n",
            "Batch : 122, D Loss : 6.309 | G Loss : 17.461\n",
            "Batch : 123, D Loss : 6.216 | G Loss : 16.132\n",
            "Batch : 124, D Loss : 6.576 | G Loss : 18.584\n",
            "Batch : 125, D Loss : 5.808 | G Loss : 17.396\n",
            "Batch : 126, D Loss : 6.441 | G Loss : 16.675\n",
            "Batch : 127, D Loss : 6.298 | G Loss : 17.568\n",
            "Batch : 128, D Loss : 5.703 | G Loss : 17.288\n",
            "Batch : 129, D Loss : 5.923 | G Loss : 18.751\n",
            "Batch : 130, D Loss : 6.508 | G Loss : 16.859\n",
            "Batch : 131, D Loss : 6.325 | G Loss : 16.395\n",
            "Batch : 132, D Loss : 6.288 | G Loss : 17.272\n",
            "Batch : 133, D Loss : 5.758 | G Loss : 16.845\n",
            "Batch : 134, D Loss : 6.115 | G Loss : 16.990\n",
            "Batch : 135, D Loss : 6.291 | G Loss : 16.216\n",
            "Batch : 136, D Loss : 5.790 | G Loss : 17.096\n",
            "Batch : 137, D Loss : 6.012 | G Loss : 18.015\n",
            "Batch : 138, D Loss : 6.380 | G Loss : 16.469\n",
            "Batch : 139, D Loss : 6.158 | G Loss : 17.030\n",
            "Batch : 140, D Loss : 5.614 | G Loss : 16.978\n",
            "Batch : 141, D Loss : 6.327 | G Loss : 17.060\n",
            "Batch : 142, D Loss : 6.118 | G Loss : 16.522\n",
            "Batch : 143, D Loss : 6.301 | G Loss : 15.996\n",
            "Batch : 144, D Loss : 6.366 | G Loss : 16.057\n",
            "Batch : 145, D Loss : 6.094 | G Loss : 16.666\n",
            "Batch : 146, D Loss : 6.105 | G Loss : 17.815\n",
            "Batch : 147, D Loss : 6.087 | G Loss : 16.819\n",
            "Batch : 148, D Loss : 6.301 | G Loss : 17.312\n",
            "Batch : 149, D Loss : 6.231 | G Loss : 17.267\n",
            "Batch : 150, D Loss : 6.285 | G Loss : 16.549\n",
            "Batch : 151, D Loss : 6.451 | G Loss : 17.790\n",
            "Batch : 152, D Loss : 6.249 | G Loss : 16.723\n",
            "Batch : 153, D Loss : 6.216 | G Loss : 17.014\n",
            "Batch : 154, D Loss : 6.406 | G Loss : 16.451\n",
            "Batch : 155, D Loss : 6.021 | G Loss : 17.235\n",
            "Batch : 156, D Loss : 6.047 | G Loss : 16.022\n",
            "Batch : 157, D Loss : 6.273 | G Loss : 17.740\n",
            "Batch : 158, D Loss : 6.104 | G Loss : 17.021\n",
            "Batch : 159, D Loss : 5.947 | G Loss : 16.571\n",
            "Batch : 160, D Loss : 6.356 | G Loss : 15.629\n",
            "Batch : 161, D Loss : 5.176 | G Loss : 22.607\n",
            "Batch : 162, D Loss : 6.196 | G Loss : 17.252\n",
            "Batch : 163, D Loss : 6.489 | G Loss : 20.205\n",
            "Batch : 164, D Loss : 6.581 | G Loss : 19.967\n",
            "Batch : 165, D Loss : 6.387 | G Loss : 18.498\n",
            "Batch : 166, D Loss : 6.353 | G Loss : 18.828\n",
            "Batch : 167, D Loss : 6.202 | G Loss : 18.371\n",
            "Batch : 168, D Loss : 6.395 | G Loss : 19.231\n",
            "Batch : 169, D Loss : 6.341 | G Loss : 16.479\n",
            "Batch : 170, D Loss : 6.742 | G Loss : 18.173\n",
            "Batch : 171, D Loss : 5.504 | G Loss : 17.207\n",
            "Batch : 172, D Loss : 6.331 | G Loss : 18.395\n",
            "Batch : 173, D Loss : 6.470 | G Loss : 15.816\n",
            "Batch : 174, D Loss : 6.307 | G Loss : 17.224\n",
            "Batch : 175, D Loss : 6.697 | G Loss : 16.852\n",
            "Batch : 176, D Loss : 6.663 | G Loss : 15.698\n",
            "Batch : 177, D Loss : 6.510 | G Loss : 15.639\n",
            "Batch : 178, D Loss : 6.210 | G Loss : 16.476\n",
            "Batch : 179, D Loss : 6.148 | G Loss : 17.503\n",
            "Batch : 180, D Loss : 6.292 | G Loss : 16.793\n",
            "Batch : 181, D Loss : 6.233 | G Loss : 16.040\n",
            "Batch : 182, D Loss : 6.413 | G Loss : 16.980\n",
            "Batch : 183, D Loss : 6.375 | G Loss : 15.986\n",
            "Batch : 184, D Loss : 6.164 | G Loss : 16.439\n",
            "Batch : 185, D Loss : 6.088 | G Loss : 17.255\n",
            "Batch : 186, D Loss : 6.151 | G Loss : 16.340\n",
            "Batch : 187, D Loss : 5.972 | G Loss : 16.131\n",
            "Batch : 188, D Loss : 6.494 | G Loss : 16.355\n",
            "Batch : 189, D Loss : 6.177 | G Loss : 17.046\n",
            "Batch : 190, D Loss : 6.188 | G Loss : 16.407\n",
            "Batch : 191, D Loss : 6.109 | G Loss : 16.871\n",
            "Batch : 192, D Loss : 6.234 | G Loss : 15.794\n",
            "Batch : 193, D Loss : 5.568 | G Loss : 16.211\n",
            "Batch : 194, D Loss : 6.171 | G Loss : 15.773\n",
            "Batch : 195, D Loss : 6.355 | G Loss : 14.962\n",
            "Batch : 196, D Loss : 6.109 | G Loss : 15.919\n",
            "Batch : 197, D Loss : 6.150 | G Loss : 16.047\n",
            "Batch : 198, D Loss : 6.225 | G Loss : 16.349\n",
            "Batch : 199, D Loss : 6.285 | G Loss : 17.577\n",
            "Batch : 200, D Loss : 5.904 | G Loss : 18.280\n",
            "Batch : 201, D Loss : 5.945 | G Loss : 16.527\n",
            "Batch : 202, D Loss : 5.979 | G Loss : 19.076\n",
            "Batch : 203, D Loss : 5.835 | G Loss : 16.049\n",
            "Batch : 204, D Loss : 5.935 | G Loss : 16.604\n",
            "Batch : 205, D Loss : 5.869 | G Loss : 15.649\n",
            "Batch : 206, D Loss : 5.824 | G Loss : 16.795\n",
            "Batch : 207, D Loss : 5.723 | G Loss : 17.189\n",
            "Batch : 208, D Loss : 6.661 | G Loss : 19.183\n",
            "Batch : 209, D Loss : 6.153 | G Loss : 19.280\n",
            "Batch : 210, D Loss : 5.816 | G Loss : 17.072\n",
            "Batch : 211, D Loss : 5.835 | G Loss : 18.755\n",
            "Batch : 212, D Loss : 4.733 | G Loss : 15.914\n",
            "Batch : 213, D Loss : 3.750 | G Loss : 24.813\n",
            "Batch : 214, D Loss : 3.558 | G Loss : 24.005\n",
            "Batch : 215, D Loss : 3.865 | G Loss : 21.482\n",
            "Batch : 216, D Loss : 6.008 | G Loss : 17.655\n",
            "Batch : 217, D Loss : 6.119 | G Loss : 18.255\n",
            "Batch : 218, D Loss : 4.583 | G Loss : 15.660\n",
            "Batch : 219, D Loss : 3.707 | G Loss : 20.913\n",
            "Batch : 220, D Loss : 3.771 | G Loss : 22.917\n",
            "Batch : 221, D Loss : 6.853 | G Loss : 20.606\n",
            "Batch : 222, D Loss : 6.594 | G Loss : 19.501\n",
            "Batch : 223, D Loss : 6.574 | G Loss : 21.114\n",
            "Batch : 224, D Loss : 6.600 | G Loss : 20.643\n",
            "Batch : 225, D Loss : 6.537 | G Loss : 19.628\n",
            "Batch : 226, D Loss : 6.452 | G Loss : 20.145\n",
            "Batch : 227, D Loss : 6.357 | G Loss : 19.920\n",
            "Batch : 228, D Loss : 6.438 | G Loss : 19.408\n",
            "Batch : 229, D Loss : 6.612 | G Loss : 18.997\n",
            "Batch : 230, D Loss : 6.400 | G Loss : 18.950\n",
            "Batch : 231, D Loss : 6.491 | G Loss : 19.765\n",
            "Batch : 232, D Loss : 6.434 | G Loss : 19.552\n",
            "Batch : 233, D Loss : 6.340 | G Loss : 18.431\n",
            "Batch : 234, D Loss : 6.426 | G Loss : 18.530\n",
            "Batch : 235, D Loss : 6.160 | G Loss : 18.694\n",
            "Batch : 236, D Loss : 6.003 | G Loss : 18.373\n",
            "Batch : 237, D Loss : 6.361 | G Loss : 18.377\n",
            "Batch : 238, D Loss : 6.286 | G Loss : 17.079\n",
            "Batch : 239, D Loss : 6.377 | G Loss : 20.043\n",
            "Batch : 240, D Loss : 6.163 | G Loss : 17.718\n",
            "Batch : 241, D Loss : 6.248 | G Loss : 19.057\n",
            "Batch : 242, D Loss : 6.412 | G Loss : 18.433\n",
            "Batch : 243, D Loss : 6.273 | G Loss : 17.484\n",
            "Batch : 244, D Loss : 6.370 | G Loss : 17.622\n",
            "Batch : 245, D Loss : 6.146 | G Loss : 17.837\n",
            "Batch : 246, D Loss : 6.145 | G Loss : 18.637\n",
            "Batch : 247, D Loss : 6.219 | G Loss : 18.552\n",
            "Batch : 248, D Loss : 6.273 | G Loss : 17.309\n",
            "Batch : 249, D Loss : 6.012 | G Loss : 18.319\n",
            "Batch : 250, D Loss : 6.178 | G Loss : 16.721\n",
            "Batch : 251, D Loss : 6.240 | G Loss : 17.242\n",
            "Batch : 252, D Loss : 6.199 | G Loss : 17.687\n",
            "Batch : 253, D Loss : 6.321 | G Loss : 18.439\n",
            "Batch : 254, D Loss : 6.343 | G Loss : 17.308\n",
            "Batch : 255, D Loss : 5.882 | G Loss : 18.467\n",
            "Batch : 256, D Loss : 6.397 | G Loss : 17.141\n",
            "Batch : 257, D Loss : 6.314 | G Loss : 18.531\n",
            "Batch : 258, D Loss : 6.197 | G Loss : 18.209\n",
            "Batch : 259, D Loss : 6.066 | G Loss : 18.636\n",
            "Batch : 260, D Loss : 6.256 | G Loss : 18.629\n",
            "Batch : 261, D Loss : 6.220 | G Loss : 17.431\n",
            "Batch : 262, D Loss : 6.140 | G Loss : 16.541\n",
            "Batch : 263, D Loss : 6.286 | G Loss : 17.192\n",
            "Batch : 264, D Loss : 6.251 | G Loss : 16.856\n",
            "Batch : 265, D Loss : 6.215 | G Loss : 17.901\n",
            "Batch : 266, D Loss : 6.209 | G Loss : 17.366\n",
            "Batch : 267, D Loss : 6.237 | G Loss : 17.302\n",
            "Batch : 268, D Loss : 6.229 | G Loss : 18.825\n",
            "Batch : 269, D Loss : 6.199 | G Loss : 18.878\n",
            "Batch : 270, D Loss : 6.172 | G Loss : 18.257\n",
            "Batch : 271, D Loss : 6.075 | G Loss : 17.815\n",
            "Batch : 272, D Loss : 6.156 | G Loss : 17.426\n",
            "Batch : 273, D Loss : 6.507 | G Loss : 19.565\n",
            "Batch : 274, D Loss : 6.158 | G Loss : 18.427\n",
            "Batch : 275, D Loss : 6.356 | G Loss : 16.652\n",
            "Batch : 276, D Loss : 6.288 | G Loss : 17.332\n",
            "Batch : 277, D Loss : 6.285 | G Loss : 17.312\n",
            "Batch : 278, D Loss : 6.485 | G Loss : 17.831\n",
            "Batch : 279, D Loss : 6.204 | G Loss : 18.625\n",
            "Batch : 280, D Loss : 6.083 | G Loss : 18.136\n",
            "Batch : 281, D Loss : 6.199 | G Loss : 20.066\n",
            "Batch : 282, D Loss : 6.321 | G Loss : 16.561\n",
            "Batch : 283, D Loss : 6.382 | G Loss : 16.916\n",
            "Batch : 284, D Loss : 6.430 | G Loss : 17.696\n",
            "Batch : 285, D Loss : 6.342 | G Loss : 17.800\n",
            "Batch : 286, D Loss : 6.207 | G Loss : 16.969\n",
            "Batch : 287, D Loss : 6.233 | G Loss : 17.412\n",
            "Batch : 288, D Loss : 6.376 | G Loss : 18.246\n",
            "Batch : 289, D Loss : 6.364 | G Loss : 18.348\n",
            "Batch : 290, D Loss : 6.409 | G Loss : 17.473\n",
            "Batch : 291, D Loss : 6.268 | G Loss : 18.649\n",
            "Batch : 292, D Loss : 6.199 | G Loss : 16.370\n",
            "Batch : 293, D Loss : 6.498 | G Loss : 17.839\n",
            "Batch : 294, D Loss : 6.391 | G Loss : 18.743\n",
            "Batch : 295, D Loss : 6.156 | G Loss : 16.523\n",
            "Batch : 296, D Loss : 6.364 | G Loss : 17.198\n",
            "Batch : 297, D Loss : 6.118 | G Loss : 16.785\n",
            "Batch : 298, D Loss : 6.221 | G Loss : 16.231\n",
            "Batch : 299, D Loss : 6.220 | G Loss : 16.629\n",
            "Batch : 300, D Loss : 6.393 | G Loss : 16.947\n",
            "Batch : 301, D Loss : 6.376 | G Loss : 20.617\n",
            "Batch : 302, D Loss : 6.440 | G Loss : 17.175\n",
            "Batch : 303, D Loss : 6.365 | G Loss : 16.283\n",
            "Batch : 304, D Loss : 6.102 | G Loss : 18.548\n",
            "Batch : 305, D Loss : 6.332 | G Loss : 17.844\n",
            "Batch : 306, D Loss : 6.578 | G Loss : 18.738\n",
            "Batch : 307, D Loss : 6.306 | G Loss : 17.438\n",
            "Batch : 308, D Loss : 6.429 | G Loss : 18.997\n",
            "Batch : 309, D Loss : 6.396 | G Loss : 18.242\n",
            "Batch : 310, D Loss : 6.030 | G Loss : 17.943\n",
            "Batch : 311, D Loss : 6.256 | G Loss : 17.525\n",
            "Batch : 312, D Loss : 6.421 | G Loss : 17.607\n",
            "Batch : 313, D Loss : 6.325 | G Loss : 18.535\n",
            "Batch : 314, D Loss : 6.325 | G Loss : 16.835\n",
            "Batch : 315, D Loss : 6.350 | G Loss : 18.545\n",
            "Batch : 316, D Loss : 5.956 | G Loss : 17.799\n",
            "Batch : 317, D Loss : 6.135 | G Loss : 20.492\n",
            "Batch : 318, D Loss : 6.197 | G Loss : 17.581\n",
            "Batch : 319, D Loss : 6.247 | G Loss : 19.860\n",
            "Batch : 320, D Loss : 6.006 | G Loss : 18.059\n",
            "Batch : 321, D Loss : 6.028 | G Loss : 16.990\n",
            "Batch : 322, D Loss : 6.341 | G Loss : 18.233\n",
            "Batch : 323, D Loss : 5.983 | G Loss : 17.310\n",
            "Batch : 324, D Loss : 6.443 | G Loss : 16.259\n",
            "Batch : 325, D Loss : 6.206 | G Loss : 17.324\n",
            "Batch : 326, D Loss : 6.274 | G Loss : 17.846\n",
            "Batch : 327, D Loss : 5.914 | G Loss : 19.066\n",
            "Batch : 328, D Loss : 6.325 | G Loss : 16.251\n",
            "Batch : 329, D Loss : 6.497 | G Loss : 16.342\n",
            "Batch : 330, D Loss : 6.570 | G Loss : 16.158\n",
            "Batch : 331, D Loss : 6.335 | G Loss : 17.567\n",
            "Batch : 332, D Loss : 6.058 | G Loss : 17.417\n",
            "Batch : 333, D Loss : 6.223 | G Loss : 18.257\n",
            "Batch : 334, D Loss : 6.168 | G Loss : 16.700\n",
            "Batch : 335, D Loss : 6.278 | G Loss : 16.239\n",
            "Batch : 336, D Loss : 6.062 | G Loss : 17.409\n",
            "Batch : 337, D Loss : 6.283 | G Loss : 19.042\n",
            "Batch : 338, D Loss : 6.351 | G Loss : 16.041\n",
            "Batch : 339, D Loss : 6.243 | G Loss : 17.808\n",
            "Batch : 340, D Loss : 6.222 | G Loss : 17.680\n",
            "Batch : 341, D Loss : 6.282 | G Loss : 16.599\n",
            "Batch : 342, D Loss : 6.162 | G Loss : 16.224\n",
            "Batch : 343, D Loss : 6.285 | G Loss : 16.353\n",
            "Batch : 344, D Loss : 6.451 | G Loss : 20.722\n",
            "Batch : 345, D Loss : 6.241 | G Loss : 16.559\n",
            "Batch : 346, D Loss : 6.175 | G Loss : 16.049\n",
            "Batch : 347, D Loss : 6.097 | G Loss : 16.850\n",
            "Batch : 348, D Loss : 6.019 | G Loss : 18.091\n",
            "Batch : 349, D Loss : 6.351 | G Loss : 18.382\n",
            "Batch : 350, D Loss : 6.170 | G Loss : 15.819\n",
            "Batch : 351, D Loss : 6.131 | G Loss : 16.430\n",
            "Batch : 352, D Loss : 6.403 | G Loss : 16.172\n",
            "Batch : 353, D Loss : 6.169 | G Loss : 16.364\n",
            "Batch : 354, D Loss : 6.166 | G Loss : 17.479\n",
            "Batch : 355, D Loss : 6.633 | G Loss : 17.169\n",
            "Batch : 356, D Loss : 6.655 | G Loss : 16.771\n",
            "Batch : 357, D Loss : 6.474 | G Loss : 18.178\n",
            "Batch : 358, D Loss : 6.452 | G Loss : 18.499\n",
            "Batch : 359, D Loss : 6.404 | G Loss : 16.369\n",
            "Batch : 360, D Loss : 6.143 | G Loss : 17.871\n",
            "Batch : 361, D Loss : 6.395 | G Loss : 15.871\n",
            "Batch : 362, D Loss : 6.044 | G Loss : 18.071\n",
            "Batch : 363, D Loss : 6.201 | G Loss : 16.515\n",
            "Batch : 364, D Loss : 6.136 | G Loss : 18.776\n",
            "Batch : 365, D Loss : 6.161 | G Loss : 17.033\n",
            "Batch : 366, D Loss : 6.271 | G Loss : 16.958\n",
            "Batch : 367, D Loss : 6.192 | G Loss : 16.316\n",
            "Batch : 368, D Loss : 6.392 | G Loss : 16.391\n",
            "Batch : 369, D Loss : 6.098 | G Loss : 15.424\n",
            "Batch : 370, D Loss : 6.115 | G Loss : 16.150\n",
            "Batch : 371, D Loss : 6.330 | G Loss : 15.596\n",
            "Batch : 372, D Loss : 6.053 | G Loss : 15.935\n",
            "Batch : 373, D Loss : 6.179 | G Loss : 16.672\n",
            "Batch : 374, D Loss : 6.254 | G Loss : 15.647\n",
            "Batch : 375, D Loss : 6.329 | G Loss : 17.886\n",
            "Batch : 376, D Loss : 6.436 | G Loss : 15.870\n",
            "Batch : 377, D Loss : 6.383 | G Loss : 15.378\n",
            "Batch : 378, D Loss : 6.308 | G Loss : 16.447\n",
            "Batch : 379, D Loss : 6.188 | G Loss : 16.911\n",
            "Batch : 380, D Loss : 6.031 | G Loss : 16.080\n",
            "Batch : 381, D Loss : 5.877 | G Loss : 15.778\n",
            "Batch : 382, D Loss : 6.039 | G Loss : 17.162\n",
            "Batch : 383, D Loss : 6.164 | G Loss : 16.256\n",
            "Batch : 384, D Loss : 6.109 | G Loss : 17.008\n",
            "Batch : 385, D Loss : 6.204 | G Loss : 16.710\n",
            "Batch : 386, D Loss : 6.082 | G Loss : 15.304\n",
            "Batch : 387, D Loss : 6.228 | G Loss : 16.416\n",
            "Batch : 388, D Loss : 6.200 | G Loss : 15.591\n",
            "Batch : 389, D Loss : 5.907 | G Loss : 16.262\n",
            "Batch : 390, D Loss : 5.718 | G Loss : 17.252\n",
            "Batch : 391, D Loss : 6.099 | G Loss : 17.065\n",
            "Batch : 392, D Loss : 6.023 | G Loss : 17.534\n",
            "Batch : 393, D Loss : 5.824 | G Loss : 17.223\n",
            "Batch : 394, D Loss : 5.941 | G Loss : 14.757\n",
            "Batch : 395, D Loss : 6.111 | G Loss : 16.447\n",
            "Batch : 396, D Loss : 5.901 | G Loss : 15.948\n",
            "Batch : 397, D Loss : 6.070 | G Loss : 14.860\n",
            "Batch : 398, D Loss : 6.017 | G Loss : 16.378\n",
            "Batch : 399, D Loss : 5.866 | G Loss : 15.622\n",
            "Batch : 400, D Loss : 5.662 | G Loss : 15.838\n",
            "Batch : 401, D Loss : 6.011 | G Loss : 17.409\n",
            "Batch : 402, D Loss : 6.232 | G Loss : 16.054\n",
            "Batch : 403, D Loss : 5.968 | G Loss : 15.938\n",
            "Batch : 404, D Loss : 5.550 | G Loss : 14.774\n",
            "Batch : 405, D Loss : 6.151 | G Loss : 17.134\n",
            "Batch : 406, D Loss : 6.079 | G Loss : 17.050\n",
            "Batch : 407, D Loss : 5.999 | G Loss : 16.617\n",
            "Batch : 408, D Loss : 6.036 | G Loss : 15.706\n",
            "Batch : 409, D Loss : 6.100 | G Loss : 14.869\n",
            "Batch : 410, D Loss : 5.459 | G Loss : 15.148\n",
            "Batch : 411, D Loss : 5.073 | G Loss : 16.208\n",
            "Batch : 412, D Loss : 6.059 | G Loss : 15.432\n",
            "Batch : 413, D Loss : 4.633 | G Loss : 14.899\n",
            "Batch : 414, D Loss : 3.779 | G Loss : 20.086\n",
            "Batch : 415, D Loss : 5.038 | G Loss : 15.777\n",
            "Batch : 416, D Loss : 4.883 | G Loss : 14.185\n",
            "Batch : 417, D Loss : 4.189 | G Loss : 17.443\n",
            "Batch : 418, D Loss : 6.320 | G Loss : 17.807\n",
            "Batch : 419, D Loss : 6.406 | G Loss : 17.567\n",
            "Batch : 420, D Loss : 6.159 | G Loss : 18.310\n",
            "Batch : 421, D Loss : 6.249 | G Loss : 17.100\n",
            "Batch : 422, D Loss : 6.038 | G Loss : 17.983\n",
            "Batch : 423, D Loss : 6.165 | G Loss : 16.423\n",
            "Batch : 424, D Loss : 5.934 | G Loss : 16.635\n",
            "Batch : 425, D Loss : 6.367 | G Loss : 15.688\n",
            "Batch : 426, D Loss : 6.113 | G Loss : 16.175\n",
            "Batch : 427, D Loss : 6.005 | G Loss : 16.250\n",
            "Batch : 428, D Loss : 5.971 | G Loss : 16.404\n",
            "Batch : 429, D Loss : 5.797 | G Loss : 14.428\n",
            "Batch : 430, D Loss : 5.898 | G Loss : 15.593\n",
            "Batch : 431, D Loss : 5.685 | G Loss : 17.727\n",
            "Batch : 432, D Loss : 5.651 | G Loss : 15.344\n",
            "Batch : 433, D Loss : 6.236 | G Loss : 17.500\n",
            "Batch : 434, D Loss : 6.376 | G Loss : 18.909\n",
            "Batch : 435, D Loss : 6.098 | G Loss : 16.920\n",
            "Batch : 436, D Loss : 6.497 | G Loss : 16.656\n",
            "Batch : 437, D Loss : 6.251 | G Loss : 18.762\n",
            "Batch : 438, D Loss : 6.191 | G Loss : 17.532\n",
            "Batch : 439, D Loss : 6.172 | G Loss : 18.704\n",
            "Batch : 440, D Loss : 6.388 | G Loss : 19.718\n",
            "Batch : 441, D Loss : 6.227 | G Loss : 17.673\n",
            "Batch : 442, D Loss : 6.305 | G Loss : 17.445\n",
            "Batch : 443, D Loss : 6.047 | G Loss : 17.570\n",
            "Batch : 444, D Loss : 6.319 | G Loss : 16.978\n",
            "Batch : 445, D Loss : 6.489 | G Loss : 16.337\n",
            "Batch : 446, D Loss : 6.282 | G Loss : 16.338\n",
            "Batch : 447, D Loss : 6.512 | G Loss : 16.258\n",
            "Batch : 448, D Loss : 6.424 | G Loss : 17.313\n",
            "Batch : 449, D Loss : 6.300 | G Loss : 17.649\n",
            "Batch : 450, D Loss : 6.094 | G Loss : 17.024\n",
            "Batch : 451, D Loss : 6.213 | G Loss : 16.797\n",
            "Batch : 452, D Loss : 6.407 | G Loss : 16.744\n",
            "Batch : 453, D Loss : 6.472 | G Loss : 17.129\n",
            "Batch : 454, D Loss : 6.482 | G Loss : 16.448\n",
            "Batch : 455, D Loss : 6.375 | G Loss : 15.339\n",
            "Batch : 456, D Loss : 6.242 | G Loss : 16.701\n",
            "Batch : 457, D Loss : 6.436 | G Loss : 17.582\n",
            "Batch : 458, D Loss : 6.428 | G Loss : 16.754\n",
            "Batch : 459, D Loss : 6.364 | G Loss : 16.839\n",
            "Batch : 460, D Loss : 6.171 | G Loss : 17.266\n",
            "Batch : 461, D Loss : 6.396 | G Loss : 15.944\n",
            "Batch : 462, D Loss : 6.306 | G Loss : 17.624\n",
            "Batch : 463, D Loss : 6.392 | G Loss : 16.708\n",
            "Batch : 464, D Loss : 6.362 | G Loss : 18.890\n",
            "Batch : 465, D Loss : 6.288 | G Loss : 16.783\n",
            "Batch : 466, D Loss : 6.287 | G Loss : 16.601\n",
            "Batch : 467, D Loss : 6.239 | G Loss : 17.202\n",
            "Batch : 468, D Loss : 6.299 | G Loss : 17.531\n",
            "Batch : 469, D Loss : 6.210 | G Loss : 18.556\n",
            "Batch : 470, D Loss : 6.179 | G Loss : 16.238\n",
            "Batch : 471, D Loss : 6.215 | G Loss : 15.869\n",
            "Batch : 472, D Loss : 6.193 | G Loss : 16.999\n",
            "Batch : 473, D Loss : 6.305 | G Loss : 16.866\n",
            "Batch : 474, D Loss : 6.330 | G Loss : 16.898\n",
            "Batch : 475, D Loss : 6.354 | G Loss : 16.373\n",
            "Batch : 476, D Loss : 6.413 | G Loss : 16.899\n",
            "Batch : 477, D Loss : 6.273 | G Loss : 15.765\n",
            "Batch : 478, D Loss : 6.544 | G Loss : 16.612\n",
            "Batch : 479, D Loss : 6.255 | G Loss : 16.424\n",
            "Batch : 480, D Loss : 6.463 | G Loss : 17.024\n",
            "Batch : 481, D Loss : 6.279 | G Loss : 15.894\n",
            "Batch : 482, D Loss : 5.971 | G Loss : 16.280\n",
            "Batch : 483, D Loss : 6.297 | G Loss : 17.199\n",
            "Batch : 484, D Loss : 6.655 | G Loss : 15.958\n",
            "Batch : 485, D Loss : 6.273 | G Loss : 17.275\n",
            "Batch : 486, D Loss : 6.313 | G Loss : 17.048\n",
            "Batch : 487, D Loss : 6.348 | G Loss : 17.706\n",
            "Batch : 488, D Loss : 6.350 | G Loss : 17.659\n",
            "Batch : 489, D Loss : 6.280 | G Loss : 16.820\n",
            "Batch : 490, D Loss : 6.153 | G Loss : 15.674\n",
            "Batch : 491, D Loss : 6.196 | G Loss : 17.219\n",
            "Batch : 492, D Loss : 6.296 | G Loss : 16.148\n",
            "Batch : 493, D Loss : 6.457 | G Loss : 17.214\n",
            "Batch : 494, D Loss : 6.221 | G Loss : 16.621\n",
            "Batch : 495, D Loss : 6.178 | G Loss : 17.363\n",
            "Batch : 496, D Loss : 6.306 | G Loss : 15.976\n",
            "Batch : 497, D Loss : 6.038 | G Loss : 16.492\n",
            "Batch : 498, D Loss : 6.119 | G Loss : 16.997\n",
            "Batch : 499, D Loss : 6.436 | G Loss : 16.547\n",
            "Batch : 500, D Loss : 6.128 | G Loss : 17.038\n",
            "Batch : 501, D Loss : 5.990 | G Loss : 17.822\n",
            "Batch : 502, D Loss : 6.137 | G Loss : 16.651\n",
            "Batch : 503, D Loss : 6.360 | G Loss : 16.778\n",
            "Batch : 504, D Loss : 6.436 | G Loss : 17.048\n",
            "Batch : 505, D Loss : 6.185 | G Loss : 16.740\n",
            "Batch : 506, D Loss : 6.113 | G Loss : 17.083\n",
            "Batch : 507, D Loss : 6.213 | G Loss : 17.037\n",
            "Batch : 508, D Loss : 6.159 | G Loss : 16.018\n",
            "Batch : 509, D Loss : 6.254 | G Loss : 16.096\n",
            "Batch : 510, D Loss : 6.292 | G Loss : 18.735\n",
            "Batch : 511, D Loss : 6.161 | G Loss : 15.990\n",
            "Batch : 512, D Loss : 6.281 | G Loss : 17.277\n",
            "Batch : 513, D Loss : 6.296 | G Loss : 15.782\n",
            "Batch : 514, D Loss : 6.394 | G Loss : 16.041\n",
            "Batch : 515, D Loss : 5.946 | G Loss : 16.912\n",
            "Batch : 516, D Loss : 6.014 | G Loss : 17.246\n",
            "Batch : 517, D Loss : 6.269 | G Loss : 15.488\n",
            "Batch : 518, D Loss : 6.300 | G Loss : 16.281\n",
            "Batch : 519, D Loss : 6.275 | G Loss : 16.461\n",
            "Batch : 520, D Loss : 6.322 | G Loss : 15.383\n",
            "Batch : 521, D Loss : 6.284 | G Loss : 15.969\n",
            "Batch : 522, D Loss : 6.129 | G Loss : 15.675\n",
            "Batch : 523, D Loss : 6.075 | G Loss : 17.462\n",
            "Batch : 524, D Loss : 6.186 | G Loss : 15.005\n",
            "Batch : 525, D Loss : 6.285 | G Loss : 15.314\n",
            "Batch : 526, D Loss : 6.132 | G Loss : 17.219\n",
            "Batch : 527, D Loss : 6.132 | G Loss : 16.035\n",
            "Batch : 528, D Loss : 6.295 | G Loss : 15.886\n",
            "Batch : 529, D Loss : 6.360 | G Loss : 16.573\n",
            "Batch : 530, D Loss : 6.271 | G Loss : 17.627\n",
            "Batch : 531, D Loss : 6.305 | G Loss : 15.617\n",
            "Batch : 532, D Loss : 6.092 | G Loss : 17.150\n",
            "Batch : 533, D Loss : 6.232 | G Loss : 17.238\n",
            "Batch : 534, D Loss : 6.085 | G Loss : 16.414\n",
            "Batch : 535, D Loss : 6.299 | G Loss : 17.678\n",
            "Batch : 536, D Loss : 6.245 | G Loss : 15.705\n",
            "Batch : 537, D Loss : 6.221 | G Loss : 17.777\n",
            "Batch : 538, D Loss : 6.285 | G Loss : 16.072\n",
            "Batch : 539, D Loss : 6.348 | G Loss : 15.954\n",
            "Batch : 540, D Loss : 6.248 | G Loss : 15.377\n",
            "Batch : 541, D Loss : 5.990 | G Loss : 16.566\n",
            "Batch : 542, D Loss : 5.967 | G Loss : 17.322\n",
            "Batch : 543, D Loss : 6.046 | G Loss : 17.049\n",
            "Batch : 544, D Loss : 6.201 | G Loss : 17.204\n",
            "Batch : 545, D Loss : 6.486 | G Loss : 15.396\n",
            "Batch : 546, D Loss : 6.414 | G Loss : 17.056\n",
            "Batch : 547, D Loss : 6.202 | G Loss : 17.537\n",
            "Batch : 548, D Loss : 5.789 | G Loss : 16.402\n",
            "Batch : 549, D Loss : 6.138 | G Loss : 16.028\n",
            "Batch : 550, D Loss : 6.347 | G Loss : 15.843\n",
            "Batch : 551, D Loss : 6.446 | G Loss : 16.244\n",
            "Batch : 552, D Loss : 6.075 | G Loss : 16.913\n",
            "Batch : 553, D Loss : 6.167 | G Loss : 16.796\n",
            "Batch : 554, D Loss : 6.317 | G Loss : 15.667\n",
            "Batch : 555, D Loss : 6.487 | G Loss : 17.869\n",
            "Batch : 556, D Loss : 6.205 | G Loss : 16.070\n",
            "Batch : 557, D Loss : 6.094 | G Loss : 15.881\n",
            "Batch : 558, D Loss : 6.246 | G Loss : 16.022\n",
            "Batch : 559, D Loss : 6.294 | G Loss : 16.186\n",
            "Batch : 560, D Loss : 6.319 | G Loss : 15.782\n",
            "Batch : 561, D Loss : 6.167 | G Loss : 17.345\n",
            "Batch : 562, D Loss : 6.149 | G Loss : 16.572\n",
            "Batch : 563, D Loss : 6.459 | G Loss : 15.708\n",
            "Batch : 564, D Loss : 6.458 | G Loss : 16.681\n",
            "Batch : 565, D Loss : 6.289 | G Loss : 15.614\n",
            "Batch : 566, D Loss : 6.132 | G Loss : 16.593\n",
            "Batch : 567, D Loss : 6.229 | G Loss : 16.256\n",
            "Batch : 568, D Loss : 6.379 | G Loss : 15.760\n",
            "Batch : 569, D Loss : 6.309 | G Loss : 15.734\n",
            "Batch : 570, D Loss : 6.292 | G Loss : 17.693\n",
            "Batch : 571, D Loss : 6.300 | G Loss : 16.599\n",
            "Batch : 572, D Loss : 6.167 | G Loss : 15.687\n",
            "Batch : 573, D Loss : 6.194 | G Loss : 16.358\n",
            "Batch : 574, D Loss : 6.248 | G Loss : 16.033\n",
            "Batch : 575, D Loss : 6.268 | G Loss : 16.352\n",
            "Batch : 576, D Loss : 6.205 | G Loss : 17.724\n",
            "Batch : 577, D Loss : 6.179 | G Loss : 15.707\n",
            "Batch : 578, D Loss : 6.071 | G Loss : 17.184\n",
            "Batch : 579, D Loss : 6.236 | G Loss : 17.508\n",
            "Batch : 580, D Loss : 6.454 | G Loss : 15.595\n",
            "Batch : 581, D Loss : 6.272 | G Loss : 16.193\n",
            "Batch : 582, D Loss : 6.277 | G Loss : 17.208\n",
            "Batch : 583, D Loss : 6.366 | G Loss : 15.392\n",
            "Batch : 584, D Loss : 6.215 | G Loss : 17.154\n",
            "Batch : 585, D Loss : 6.282 | G Loss : 15.219\n",
            "Batch : 586, D Loss : 6.280 | G Loss : 15.813\n",
            "Batch : 587, D Loss : 5.944 | G Loss : 16.620\n",
            "Batch : 588, D Loss : 5.546 | G Loss : 16.320\n",
            "Batch : 589, D Loss : 6.364 | G Loss : 15.905\n",
            "Batch : 590, D Loss : 6.041 | G Loss : 15.088\n",
            "Batch : 591, D Loss : 5.895 | G Loss : 15.719\n",
            "Batch : 592, D Loss : 5.172 | G Loss : 15.512\n",
            "Batch : 593, D Loss : 4.593 | G Loss : 15.306\n",
            "Batch : 594, D Loss : 6.388 | G Loss : 17.250\n",
            "Batch : 595, D Loss : 6.212 | G Loss : 16.258\n",
            "Batch : 596, D Loss : 6.274 | G Loss : 15.357\n",
            "Batch : 597, D Loss : 5.896 | G Loss : 16.568\n",
            "Batch : 598, D Loss : 5.440 | G Loss : 15.861\n",
            "Batch : 599, D Loss : 6.004 | G Loss : 16.068\n",
            "Batch : 600, D Loss : 5.846 | G Loss : 16.436\n",
            "Batch : 601, D Loss : 5.305 | G Loss : 16.642\n",
            "Batch : 602, D Loss : 6.129 | G Loss : 15.987\n",
            "Batch : 603, D Loss : 6.615 | G Loss : 16.590\n",
            "Batch : 604, D Loss : 6.038 | G Loss : 16.653\n",
            "Batch : 605, D Loss : 5.343 | G Loss : 14.994\n",
            "Batch : 606, D Loss : 5.287 | G Loss : 15.358\n",
            "Batch : 607, D Loss : 4.795 | G Loss : 14.858\n",
            "Batch : 608, D Loss : 4.400 | G Loss : 12.476\n",
            "Batch : 609, D Loss : 4.274 | G Loss : 11.856\n",
            "Batch : 610, D Loss : 4.367 | G Loss : 14.268\n",
            "Batch : 611, D Loss : 4.222 | G Loss : 14.530\n",
            "Batch : 612, D Loss : 6.336 | G Loss : 18.795\n",
            "Batch : 613, D Loss : 6.399 | G Loss : 18.895\n",
            "Batch : 614, D Loss : 6.447 | G Loss : 20.070\n",
            "Batch : 615, D Loss : 6.346 | G Loss : 19.897\n",
            "Batch : 616, D Loss : 6.270 | G Loss : 19.992\n",
            "Batch : 617, D Loss : 6.434 | G Loss : 19.452\n",
            "Batch : 618, D Loss : 6.237 | G Loss : 19.232\n",
            "Batch : 619, D Loss : 6.146 | G Loss : 17.453\n",
            "Batch : 620, D Loss : 6.367 | G Loss : 18.138\n",
            "Batch : 621, D Loss : 6.181 | G Loss : 17.884\n",
            "Batch : 622, D Loss : 6.536 | G Loss : 16.908\n",
            "Batch : 623, D Loss : 6.383 | G Loss : 17.382\n",
            "Batch : 624, D Loss : 6.317 | G Loss : 17.047\n",
            "Batch : 625, D Loss : 5.990 | G Loss : 18.381\n",
            "Batch : 626, D Loss : 5.893 | G Loss : 16.984\n",
            "Batch : 627, D Loss : 5.859 | G Loss : 18.046\n",
            "Batch : 628, D Loss : 6.154 | G Loss : 17.112\n",
            "Batch : 629, D Loss : 6.266 | G Loss : 16.666\n",
            "Batch : 630, D Loss : 6.165 | G Loss : 17.043\n",
            "Batch : 631, D Loss : 6.185 | G Loss : 15.767\n",
            "Batch : 632, D Loss : 6.132 | G Loss : 16.434\n",
            "Batch : 633, D Loss : 6.212 | G Loss : 16.470\n",
            "Batch : 634, D Loss : 6.005 | G Loss : 16.831\n",
            "Batch : 635, D Loss : 6.162 | G Loss : 17.670\n",
            "Batch : 636, D Loss : 6.123 | G Loss : 16.873\n",
            "Batch : 637, D Loss : 5.987 | G Loss : 16.681\n",
            "Batch : 638, D Loss : 6.031 | G Loss : 15.302\n",
            "Batch : 639, D Loss : 6.156 | G Loss : 16.498\n",
            "Batch : 640, D Loss : 6.165 | G Loss : 15.172\n",
            "Batch : 641, D Loss : 6.144 | G Loss : 14.592\n",
            "Batch : 642, D Loss : 6.242 | G Loss : 16.799\n",
            "Batch : 643, D Loss : 6.111 | G Loss : 17.622\n",
            "Batch : 644, D Loss : 6.117 | G Loss : 15.960\n",
            "Batch : 645, D Loss : 6.072 | G Loss : 17.312\n",
            "Batch : 646, D Loss : 5.776 | G Loss : 16.707\n",
            "Batch : 647, D Loss : 5.638 | G Loss : 16.856\n",
            "Batch : 648, D Loss : 5.797 | G Loss : 14.975\n",
            "Batch : 649, D Loss : 5.687 | G Loss : 14.649\n",
            "Batch : 650, D Loss : 5.093 | G Loss : 14.724\n",
            "Batch : 651, D Loss : 4.604 | G Loss : 14.900\n",
            "Batch : 652, D Loss : 4.531 | G Loss : 14.073\n",
            "Batch : 653, D Loss : 4.351 | G Loss : 13.704\n",
            "Batch : 654, D Loss : 4.017 | G Loss : 13.703\n",
            "Batch : 655, D Loss : 6.127 | G Loss : 16.645\n",
            "Batch : 656, D Loss : 6.060 | G Loss : 15.543\n",
            "Batch : 657, D Loss : 6.042 | G Loss : 16.504\n",
            "Batch : 658, D Loss : 5.769 | G Loss : 16.442\n",
            "Batch : 659, D Loss : 5.629 | G Loss : 16.581\n",
            "Batch : 660, D Loss : 5.642 | G Loss : 17.667\n",
            "Batch : 661, D Loss : 5.574 | G Loss : 14.953\n",
            "Batch : 662, D Loss : 5.717 | G Loss : 16.632\n",
            "Batch : 663, D Loss : 5.577 | G Loss : 16.199\n",
            "Batch : 664, D Loss : 6.353 | G Loss : 16.267\n",
            "Batch : 665, D Loss : 5.982 | G Loss : 15.211\n",
            "Batch : 666, D Loss : 5.995 | G Loss : 16.835\n",
            "Batch : 667, D Loss : 5.611 | G Loss : 16.437\n",
            "Batch : 668, D Loss : 5.481 | G Loss : 16.112\n",
            "Batch : 669, D Loss : 5.519 | G Loss : 14.799\n",
            "Batch : 670, D Loss : 5.199 | G Loss : 14.715\n",
            "Batch : 671, D Loss : 4.913 | G Loss : 14.974\n",
            "Batch : 672, D Loss : 4.869 | G Loss : 14.371\n",
            "Batch : 673, D Loss : 4.591 | G Loss : 13.799\n",
            "Batch : 674, D Loss : 3.962 | G Loss : 13.885\n",
            "Batch : 675, D Loss : 4.114 | G Loss : 13.049\n",
            "Batch : 676, D Loss : 4.312 | G Loss : 13.294\n",
            "Batch : 677, D Loss : 4.285 | G Loss : 14.281\n",
            "Batch : 678, D Loss : 4.407 | G Loss : 13.718\n",
            "Batch : 679, D Loss : 4.254 | G Loss : 13.527\n",
            "Batch : 680, D Loss : 4.201 | G Loss : 14.554\n",
            "Batch : 681, D Loss : 4.160 | G Loss : 13.003\n",
            "Batch : 682, D Loss : 3.969 | G Loss : 12.222\n",
            "Batch : 683, D Loss : 4.188 | G Loss : 11.880\n",
            "Batch : 684, D Loss : 4.073 | G Loss : 12.164\n",
            "Batch : 685, D Loss : 3.947 | G Loss : 13.839\n",
            "Batch : 686, D Loss : 4.167 | G Loss : 12.324\n",
            "Batch : 687, D Loss : 4.036 | G Loss : 13.248\n",
            "Batch : 688, D Loss : 3.995 | G Loss : 13.998\n",
            "Batch : 689, D Loss : 4.471 | G Loss : 14.894\n",
            "Batch : 690, D Loss : 4.133 | G Loss : 13.764\n",
            "Batch : 691, D Loss : 3.819 | G Loss : 15.161\n",
            "Batch : 692, D Loss : 5.087 | G Loss : 14.999\n",
            "Batch : 693, D Loss : 5.028 | G Loss : 14.169\n",
            "Batch : 694, D Loss : 4.743 | G Loss : 14.586\n",
            "Batch : 695, D Loss : 4.312 | G Loss : 15.029\n",
            "Batch : 696, D Loss : 3.928 | G Loss : 13.413\n",
            "Batch : 697, D Loss : 3.932 | G Loss : 13.985\n",
            "Batch : 698, D Loss : 4.088 | G Loss : 12.941\n",
            "Batch : 699, D Loss : 3.973 | G Loss : 12.742\n",
            "Batch : 700, D Loss : 4.120 | G Loss : 13.801\n",
            "Batch : 701, D Loss : 4.116 | G Loss : 13.244\n",
            "Batch : 702, D Loss : 3.912 | G Loss : 12.637\n",
            "Batch : 703, D Loss : 4.004 | G Loss : 11.898\n",
            "Batch : 704, D Loss : 4.081 | G Loss : 13.798\n",
            "Batch : 705, D Loss : 4.131 | G Loss : 13.621\n",
            "Batch : 706, D Loss : 3.991 | G Loss : 14.222\n",
            "Batch : 707, D Loss : 4.146 | G Loss : 13.094\n",
            "Batch : 708, D Loss : 4.046 | G Loss : 14.188\n",
            "Batch : 709, D Loss : 4.106 | G Loss : 11.774\n",
            "Batch : 710, D Loss : 3.845 | G Loss : 12.906\n",
            "Batch : 711, D Loss : 4.276 | G Loss : 13.299\n",
            "Batch : 712, D Loss : 4.155 | G Loss : 14.442\n",
            "Batch : 713, D Loss : 3.971 | G Loss : 11.943\n",
            "Batch : 714, D Loss : 4.171 | G Loss : 12.890\n",
            "Batch : 715, D Loss : 4.120 | G Loss : 11.868\n",
            "Batch : 716, D Loss : 3.918 | G Loss : 13.158\n",
            "Batch : 717, D Loss : 4.064 | G Loss : 12.263\n",
            "Batch : 718, D Loss : 3.935 | G Loss : 13.937\n",
            "Batch : 719, D Loss : 4.095 | G Loss : 11.992\n",
            "Batch : 720, D Loss : 3.963 | G Loss : 12.166\n",
            "Batch : 721, D Loss : 3.997 | G Loss : 12.719\n",
            "Batch : 722, D Loss : 4.144 | G Loss : 12.224\n",
            "Batch : 723, D Loss : 4.080 | G Loss : 12.158\n",
            "Batch : 724, D Loss : 3.854 | G Loss : 12.281\n",
            "Batch : 725, D Loss : 4.167 | G Loss : 13.244\n",
            "Batch : 726, D Loss : 3.975 | G Loss : 12.359\n",
            "Batch : 727, D Loss : 3.878 | G Loss : 13.402\n",
            "Batch : 728, D Loss : 4.816 | G Loss : 15.092\n",
            "Batch : 729, D Loss : 4.894 | G Loss : 13.013\n",
            "Batch : 730, D Loss : 4.388 | G Loss : 13.592\n",
            "Batch : 731, D Loss : 3.980 | G Loss : 13.054\n",
            "Batch : 732, D Loss : 3.762 | G Loss : 14.958\n",
            "Batch : 733, D Loss : 6.236 | G Loss : 17.053\n",
            "Batch : 734, D Loss : 6.356 | G Loss : 16.803\n",
            "Batch : 735, D Loss : 6.418 | G Loss : 15.159\n",
            "Batch : 736, D Loss : 6.116 | G Loss : 15.321\n",
            "Batch : 737, D Loss : 6.219 | G Loss : 14.789\n",
            "Batch : 738, D Loss : 6.114 | G Loss : 15.317\n",
            "Batch : 739, D Loss : 5.504 | G Loss : 16.409\n",
            "Batch : 740, D Loss : 5.071 | G Loss : 14.705\n",
            "Batch : 741, D Loss : 4.730 | G Loss : 13.863\n",
            "Batch : 742, D Loss : 4.736 | G Loss : 12.815\n",
            "Batch : 743, D Loss : 4.648 | G Loss : 14.623\n",
            "Batch : 744, D Loss : 4.474 | G Loss : 13.625\n",
            "Batch : 745, D Loss : 4.445 | G Loss : 13.555\n",
            "Batch : 746, D Loss : 4.374 | G Loss : 13.005\n",
            "Batch : 747, D Loss : 4.167 | G Loss : 12.552\n",
            "Batch : 748, D Loss : 3.966 | G Loss : 11.840\n",
            "Batch : 749, D Loss : 3.990 | G Loss : 12.655\n",
            "Batch : 750, D Loss : 3.947 | G Loss : 13.715\n",
            "Batch : 751, D Loss : 4.145 | G Loss : 12.925\n",
            "Batch : 752, D Loss : 3.952 | G Loss : 12.714\n",
            "Batch : 753, D Loss : 4.032 | G Loss : 12.063\n",
            "Batch : 754, D Loss : 4.284 | G Loss : 13.924\n",
            "Batch : 755, D Loss : 3.900 | G Loss : 14.102\n",
            "Batch : 756, D Loss : 3.890 | G Loss : 12.601\n",
            "Batch : 757, D Loss : 3.999 | G Loss : 12.829\n",
            "Batch : 758, D Loss : 3.939 | G Loss : 12.450\n",
            "Batch : 759, D Loss : 4.063 | G Loss : 13.598\n",
            "Batch : 760, D Loss : 3.940 | G Loss : 12.318\n",
            "Batch : 761, D Loss : 3.974 | G Loss : 13.276\n",
            "Batch : 762, D Loss : 4.152 | G Loss : 14.538\n",
            "Batch : 763, D Loss : 4.230 | G Loss : 13.963\n",
            "Batch : 764, D Loss : 4.033 | G Loss : 12.706\n",
            "Batch : 765, D Loss : 3.969 | G Loss : 13.850\n",
            "Batch : 766, D Loss : 3.996 | G Loss : 12.526\n",
            "Batch : 767, D Loss : 4.296 | G Loss : 14.200\n",
            "Batch : 768, D Loss : 4.250 | G Loss : 13.874\n",
            "Batch : 769, D Loss : 4.143 | G Loss : 13.721\n",
            "Batch : 770, D Loss : 4.010 | G Loss : 11.875\n",
            "Batch : 771, D Loss : 3.926 | G Loss : 14.428\n",
            "Batch : 772, D Loss : 4.105 | G Loss : 13.086\n",
            "Batch : 773, D Loss : 4.141 | G Loss : 12.923\n",
            "Batch : 774, D Loss : 3.982 | G Loss : 13.692\n",
            "Batch : 775, D Loss : 4.121 | G Loss : 12.156\n",
            "Batch : 776, D Loss : 4.049 | G Loss : 13.099\n",
            "Batch : 777, D Loss : 3.928 | G Loss : 14.166\n",
            "Batch : 778, D Loss : 4.278 | G Loss : 14.516\n",
            "Batch : 779, D Loss : 4.521 | G Loss : 12.655\n",
            "Batch : 780, D Loss : 4.211 | G Loss : 13.049\n",
            "Batch : 781, D Loss : 3.909 | G Loss : 14.665\n",
            "Batch : 782, D Loss : 6.232 | G Loss : 16.359\n",
            "Batch : 783, D Loss : 6.128 | G Loss : 14.825\n",
            "Batch : 784, D Loss : 5.941 | G Loss : 13.971\n",
            "Batch : 785, D Loss : 5.834 | G Loss : 14.521\n",
            "Batch : 786, D Loss : 5.555 | G Loss : 18.134\n",
            "Batch : 787, D Loss : 5.638 | G Loss : 15.452\n",
            "Batch : 788, D Loss : 5.112 | G Loss : 14.882\n",
            "Batch : 789, D Loss : 4.283 | G Loss : 14.870\n",
            "Batch : 790, D Loss : 4.652 | G Loss : 13.889\n",
            "Batch : 791, D Loss : 4.770 | G Loss : 14.362\n",
            "Batch : 792, D Loss : 4.464 | G Loss : 14.219\n",
            "Batch : 793, D Loss : 4.405 | G Loss : 14.669\n",
            "Batch : 794, D Loss : 4.190 | G Loss : 13.497\n",
            "Batch : 795, D Loss : 4.039 | G Loss : 12.975\n",
            "Batch : 796, D Loss : 4.017 | G Loss : 13.619\n",
            "Batch : 797, D Loss : 4.184 | G Loss : 12.166\n",
            "Batch : 798, D Loss : 4.241 | G Loss : 12.461\n",
            "Batch : 799, D Loss : 3.996 | G Loss : 13.343\n",
            "Batch : 800, D Loss : 4.081 | G Loss : 14.046\n",
            "Batch : 801, D Loss : 4.176 | G Loss : 13.458\n",
            "Batch : 802, D Loss : 4.106 | G Loss : 12.993\n",
            "Batch : 803, D Loss : 3.923 | G Loss : 12.565\n",
            "Batch : 804, D Loss : 4.278 | G Loss : 12.793\n",
            "Batch : 805, D Loss : 4.183 | G Loss : 13.267\n",
            "Batch : 806, D Loss : 4.138 | G Loss : 14.108\n",
            "Batch : 807, D Loss : 3.899 | G Loss : 13.132\n",
            "Batch : 808, D Loss : 4.202 | G Loss : 13.346\n",
            "Batch : 809, D Loss : 4.179 | G Loss : 11.929\n",
            "Batch : 810, D Loss : 3.978 | G Loss : 13.144\n",
            "Batch : 811, D Loss : 4.175 | G Loss : 13.742\n",
            "Batch : 812, D Loss : 4.218 | G Loss : 12.531\n",
            "Batch : 813, D Loss : 3.999 | G Loss : 11.701\n",
            "Batch : 814, D Loss : 3.838 | G Loss : 12.245\n",
            "Batch : 815, D Loss : 4.104 | G Loss : 12.363\n",
            "Batch : 816, D Loss : 4.215 | G Loss : 13.337\n",
            "Batch : 817, D Loss : 4.127 | G Loss : 12.076\n",
            "Batch : 818, D Loss : 3.808 | G Loss : 12.416\n",
            "Batch : 819, D Loss : 4.218 | G Loss : 13.487\n",
            "Batch : 820, D Loss : 3.959 | G Loss : 12.459\n",
            "Batch : 821, D Loss : 3.819 | G Loss : 11.931\n",
            "Batch : 822, D Loss : 3.923 | G Loss : 12.497\n",
            "Batch : 823, D Loss : 3.889 | G Loss : 13.134\n",
            "Batch : 824, D Loss : 3.924 | G Loss : 12.925\n",
            "Batch : 825, D Loss : 3.928 | G Loss : 12.582\n",
            "Batch : 826, D Loss : 4.039 | G Loss : 13.246\n",
            "Batch : 827, D Loss : 3.944 | G Loss : 12.097\n",
            "Batch : 828, D Loss : 3.971 | G Loss : 11.839\n",
            "Batch : 829, D Loss : 3.981 | G Loss : 12.852\n",
            "Batch : 830, D Loss : 4.020 | G Loss : 12.740\n",
            "Batch : 831, D Loss : 4.026 | G Loss : 11.222\n",
            "Batch : 832, D Loss : 3.823 | G Loss : 13.696\n",
            "Batch : 833, D Loss : 4.054 | G Loss : 12.637\n",
            "Batch : 834, D Loss : 3.983 | G Loss : 12.672\n",
            "Batch : 835, D Loss : 3.925 | G Loss : 11.526\n",
            "Batch : 836, D Loss : 3.906 | G Loss : 12.139\n",
            "Batch : 837, D Loss : 3.904 | G Loss : 12.432\n",
            "Batch : 838, D Loss : 4.215 | G Loss : 11.828\n",
            "Batch : 839, D Loss : 4.205 | G Loss : 12.009\n",
            "Batch : 840, D Loss : 3.905 | G Loss : 12.306\n",
            "Batch : 841, D Loss : 3.775 | G Loss : 12.305\n",
            "Batch : 842, D Loss : 4.367 | G Loss : 14.621\n",
            "Batch : 843, D Loss : 4.253 | G Loss : 14.762\n",
            "Batch : 844, D Loss : 4.322 | G Loss : 13.996\n",
            "Batch : 845, D Loss : 4.270 | G Loss : 12.958\n",
            "Batch : 846, D Loss : 4.136 | G Loss : 12.980\n",
            "Batch : 847, D Loss : 3.871 | G Loss : 13.345\n",
            "Batch : 848, D Loss : 4.493 | G Loss : 16.352\n",
            "Batch : 849, D Loss : 4.469 | G Loss : 14.069\n",
            "Batch : 850, D Loss : 4.217 | G Loss : 12.934\n",
            "Batch : 851, D Loss : 4.004 | G Loss : 12.663\n",
            "Batch : 852, D Loss : 4.036 | G Loss : 13.028\n",
            "Batch : 853, D Loss : 3.885 | G Loss : 13.959\n",
            "Batch : 854, D Loss : 3.872 | G Loss : 13.144\n",
            "Batch : 855, D Loss : 4.031 | G Loss : 12.297\n",
            "Batch : 856, D Loss : 3.926 | G Loss : 12.740\n",
            "Batch : 857, D Loss : 3.956 | G Loss : 12.889\n",
            "Batch : 858, D Loss : 3.921 | G Loss : 12.907\n",
            "Batch : 859, D Loss : 3.867 | G Loss : 12.991\n",
            "Batch : 860, D Loss : 3.890 | G Loss : 12.627\n",
            "Batch : 861, D Loss : 3.982 | G Loss : 12.792\n",
            "Batch : 862, D Loss : 3.971 | G Loss : 13.191\n",
            "Batch : 863, D Loss : 3.919 | G Loss : 11.955\n",
            "Batch : 864, D Loss : 3.940 | G Loss : 11.918\n",
            "Batch : 865, D Loss : 3.919 | G Loss : 12.178\n",
            "Batch : 866, D Loss : 3.909 | G Loss : 11.722\n",
            "Batch : 867, D Loss : 3.967 | G Loss : 13.784\n",
            "Batch : 868, D Loss : 4.006 | G Loss : 12.321\n",
            "Batch : 869, D Loss : 3.998 | G Loss : 13.867\n",
            "Batch : 870, D Loss : 4.600 | G Loss : 14.045\n",
            "Batch : 871, D Loss : 4.594 | G Loss : 13.390\n",
            "Batch : 872, D Loss : 4.380 | G Loss : 12.709\n",
            "Batch : 873, D Loss : 4.334 | G Loss : 12.623\n",
            "Batch : 874, D Loss : 4.431 | G Loss : 13.613\n",
            "Batch : 875, D Loss : 4.195 | G Loss : 12.394\n",
            "Batch : 876, D Loss : 4.090 | G Loss : 13.594\n",
            "Batch : 877, D Loss : 4.055 | G Loss : 13.956\n",
            "Batch : 878, D Loss : 4.273 | G Loss : 12.888\n",
            "Batch : 879, D Loss : 3.934 | G Loss : 12.826\n",
            "Batch : 880, D Loss : 4.004 | G Loss : 12.538\n",
            "Batch : 881, D Loss : 3.994 | G Loss : 11.400\n",
            "Batch : 882, D Loss : 4.099 | G Loss : 13.778\n",
            "Batch : 883, D Loss : 4.479 | G Loss : 12.069\n",
            "Batch : 884, D Loss : 4.289 | G Loss : 13.048\n",
            "Batch : 885, D Loss : 3.970 | G Loss : 11.256\n",
            "Batch : 886, D Loss : 4.012 | G Loss : 12.225\n",
            "Batch : 887, D Loss : 3.871 | G Loss : 11.932\n",
            "Batch : 888, D Loss : 3.969 | G Loss : 13.792\n",
            "Batch : 889, D Loss : 3.903 | G Loss : 12.112\n",
            "Batch : 890, D Loss : 4.016 | G Loss : 12.542\n",
            "Batch : 891, D Loss : 4.014 | G Loss : 12.367\n",
            "Batch : 892, D Loss : 4.043 | G Loss : 12.264\n",
            "Batch : 893, D Loss : 3.890 | G Loss : 14.460\n",
            "Batch : 894, D Loss : 3.908 | G Loss : 12.063\n",
            "Batch : 895, D Loss : 3.955 | G Loss : 13.228\n",
            "Batch : 896, D Loss : 3.868 | G Loss : 12.459\n",
            "Batch : 897, D Loss : 4.002 | G Loss : 13.069\n",
            "Batch : 898, D Loss : 3.967 | G Loss : 11.755\n",
            "Batch : 899, D Loss : 3.988 | G Loss : 12.431\n",
            "Batch : 900, D Loss : 3.823 | G Loss : 12.132\n",
            "Batch : 901, D Loss : 3.988 | G Loss : 12.296\n",
            "Batch : 902, D Loss : 3.951 | G Loss : 12.412\n",
            "Batch : 903, D Loss : 3.959 | G Loss : 14.933\n",
            "Batch : 904, D Loss : 4.187 | G Loss : 12.664\n",
            "Batch : 905, D Loss : 4.118 | G Loss : 15.708\n",
            "Batch : 906, D Loss : 3.911 | G Loss : 12.142\n",
            "Batch : 907, D Loss : 3.850 | G Loss : 11.541\n",
            "Batch : 908, D Loss : 4.094 | G Loss : 13.229\n",
            "Batch : 909, D Loss : 4.045 | G Loss : 12.540\n",
            "Batch : 910, D Loss : 3.894 | G Loss : 12.509\n",
            "Batch : 911, D Loss : 3.741 | G Loss : 12.808\n",
            "Batch : 912, D Loss : 4.972 | G Loss : 13.832\n",
            "Batch : 913, D Loss : 5.102 | G Loss : 14.500\n",
            "Batch : 914, D Loss : 4.739 | G Loss : 14.370\n",
            "Batch : 915, D Loss : 4.400 | G Loss : 15.421\n",
            "Batch : 916, D Loss : 4.215 | G Loss : 13.767\n",
            "Batch : 917, D Loss : 3.877 | G Loss : 16.196\n",
            "Batch : 918, D Loss : 4.057 | G Loss : 12.453\n",
            "Batch : 919, D Loss : 3.962 | G Loss : 13.448\n",
            "Batch : 920, D Loss : 3.817 | G Loss : 14.297\n",
            "Batch : 921, D Loss : 5.192 | G Loss : 14.341\n",
            "Batch : 922, D Loss : 5.530 | G Loss : 15.907\n",
            "Batch : 923, D Loss : 5.407 | G Loss : 15.091\n",
            "Batch : 924, D Loss : 5.246 | G Loss : 15.041\n",
            "Batch : 925, D Loss : 5.125 | G Loss : 14.701\n",
            "Batch : 926, D Loss : 4.761 | G Loss : 13.087\n",
            "Batch : 927, D Loss : 4.629 | G Loss : 12.713\n",
            "Batch : 928, D Loss : 4.765 | G Loss : 14.180\n",
            "Batch : 929, D Loss : 4.942 | G Loss : 13.308\n",
            "Batch : 930, D Loss : 4.828 | G Loss : 13.951\n",
            "Batch : 931, D Loss : 4.970 | G Loss : 14.107\n",
            "Batch : 932, D Loss : 4.772 | G Loss : 13.747\n",
            "Batch : 933, D Loss : 4.489 | G Loss : 13.039\n",
            "Batch : 934, D Loss : 4.437 | G Loss : 13.285\n",
            "Batch : 935, D Loss : 4.724 | G Loss : 13.592\n",
            "Batch : 936, D Loss : 4.473 | G Loss : 11.589\n",
            "Batch : 937, D Loss : 4.506 | G Loss : 13.772\n",
            "Batch : 938, D Loss : 4.660 | G Loss : 13.802\n",
            "Batch : 939, D Loss : 4.599 | G Loss : 13.197\n",
            "Batch : 940, D Loss : 4.279 | G Loss : 12.414\n",
            "Batch : 941, D Loss : 4.088 | G Loss : 13.238\n",
            "Batch : 942, D Loss : 3.978 | G Loss : 11.093\n",
            "Batch : 943, D Loss : 4.157 | G Loss : 12.908\n",
            "Batch : 944, D Loss : 4.258 | G Loss : 11.910\n",
            "Batch : 945, D Loss : 4.030 | G Loss : 11.684\n",
            "Batch : 946, D Loss : 3.909 | G Loss : 12.317\n",
            "Batch : 947, D Loss : 3.840 | G Loss : 11.763\n",
            "Batch : 948, D Loss : 3.895 | G Loss : 12.274\n",
            "Batch : 949, D Loss : 3.909 | G Loss : 12.632\n",
            "Batch : 950, D Loss : 3.894 | G Loss : 11.499\n",
            "Batch : 951, D Loss : 3.881 | G Loss : 12.791\n",
            "Batch : 952, D Loss : 3.933 | G Loss : 11.828\n",
            "Batch : 953, D Loss : 3.830 | G Loss : 11.256\n",
            "Batch : 954, D Loss : 3.761 | G Loss : 12.921\n",
            "Batch : 955, D Loss : 3.908 | G Loss : 11.666\n",
            "Batch : 956, D Loss : 3.893 | G Loss : 14.074\n",
            "Batch : 957, D Loss : 3.863 | G Loss : 11.466\n",
            "Batch : 958, D Loss : 3.986 | G Loss : 12.162\n",
            "Batch : 959, D Loss : 3.980 | G Loss : 12.687\n",
            "Batch : 960, D Loss : 3.875 | G Loss : 12.113\n",
            "Batch : 961, D Loss : 3.800 | G Loss : 12.999\n",
            "Batch : 962, D Loss : 3.972 | G Loss : 12.403\n",
            "Batch : 963, D Loss : 3.853 | G Loss : 12.034\n",
            "Batch : 964, D Loss : 3.811 | G Loss : 12.632\n",
            "Batch : 965, D Loss : 3.902 | G Loss : 12.948\n",
            "Batch : 966, D Loss : 3.944 | G Loss : 13.396\n",
            "Batch : 967, D Loss : 3.943 | G Loss : 13.873\n",
            "Batch : 968, D Loss : 3.828 | G Loss : 13.261\n",
            "Batch : 969, D Loss : 3.935 | G Loss : 13.763\n",
            "Batch : 970, D Loss : 3.795 | G Loss : 11.673\n",
            "Batch : 971, D Loss : 3.761 | G Loss : 11.585\n",
            "Batch : 972, D Loss : 4.081 | G Loss : 12.339\n",
            "Batch : 973, D Loss : 4.068 | G Loss : 14.350\n",
            "Batch : 974, D Loss : 3.841 | G Loss : 11.239\n",
            "Batch : 975, D Loss : 3.790 | G Loss : 13.089\n",
            "Batch : 976, D Loss : 3.868 | G Loss : 13.861\n",
            "Batch : 977, D Loss : 3.795 | G Loss : 11.250\n",
            "Batch : 978, D Loss : 3.941 | G Loss : 14.908\n",
            "Batch : 979, D Loss : 3.766 | G Loss : 11.635\n",
            "Batch : 980, D Loss : 3.787 | G Loss : 11.806\n",
            "Batch : 981, D Loss : 3.878 | G Loss : 12.814\n",
            "Batch : 982, D Loss : 3.828 | G Loss : 11.958\n",
            "Batch : 983, D Loss : 3.888 | G Loss : 11.966\n",
            "Batch : 984, D Loss : 3.928 | G Loss : 11.537\n",
            "Batch : 985, D Loss : 3.876 | G Loss : 10.917\n",
            "Batch : 986, D Loss : 6.571 | G Loss : 19.918\n",
            "Batch : 987, D Loss : 6.461 | G Loss : 19.763\n",
            "Batch : 988, D Loss : 6.287 | G Loss : 19.909\n",
            "Batch : 989, D Loss : 6.362 | G Loss : 17.761\n",
            "Batch : 990, D Loss : 6.342 | G Loss : 16.739\n",
            "Batch : 991, D Loss : 6.421 | G Loss : 17.395\n",
            "Batch : 992, D Loss : 6.328 | G Loss : 17.915\n",
            "Batch : 993, D Loss : 6.193 | G Loss : 17.693\n",
            "Batch : 994, D Loss : 6.386 | G Loss : 16.485\n",
            "Batch : 995, D Loss : 6.477 | G Loss : 15.267\n",
            "Batch : 996, D Loss : 6.248 | G Loss : 16.357\n",
            "Batch : 997, D Loss : 6.254 | G Loss : 17.769\n",
            "Batch : 998, D Loss : 6.234 | G Loss : 16.067\n",
            "Batch : 999, D Loss : 6.027 | G Loss : 16.153\n",
            "Batch : 1000, D Loss : 6.030 | G Loss : 14.229\n",
            "Batch : 1001, D Loss : 6.074 | G Loss : 15.581\n",
            "Batch : 1002, D Loss : 5.560 | G Loss : 16.412\n",
            "Batch : 1003, D Loss : 5.105 | G Loss : 16.351\n",
            "Batch : 1004, D Loss : 5.610 | G Loss : 16.078\n",
            "Batch : 1005, D Loss : 4.738 | G Loss : 15.096\n",
            "Batch : 1006, D Loss : 4.289 | G Loss : 13.792\n",
            "Batch : 1007, D Loss : 4.218 | G Loss : 15.269\n",
            "Batch : 1008, D Loss : 4.383 | G Loss : 13.339\n",
            "Batch : 1009, D Loss : 4.396 | G Loss : 14.224\n",
            "Batch : 1010, D Loss : 6.522 | G Loss : 16.465\n",
            "Batch : 1011, D Loss : 6.505 | G Loss : 17.280\n",
            "Batch : 1012, D Loss : 6.393 | G Loss : 17.188\n",
            "Batch : 1013, D Loss : 6.457 | G Loss : 15.699\n",
            "Batch : 1014, D Loss : 5.939 | G Loss : 16.932\n",
            "Batch : 1015, D Loss : 6.071 | G Loss : 15.693\n",
            "Batch : 1016, D Loss : 6.261 | G Loss : 16.901\n",
            "Batch : 1017, D Loss : 6.355 | G Loss : 16.593\n",
            "Batch : 1018, D Loss : 6.458 | G Loss : 16.222\n",
            "Batch : 1019, D Loss : 6.209 | G Loss : 16.608\n",
            "Batch : 1020, D Loss : 6.147 | G Loss : 15.674\n",
            "Batch : 1021, D Loss : 6.189 | G Loss : 16.205\n",
            "Batch : 1022, D Loss : 6.234 | G Loss : 15.809\n",
            "Batch : 1023, D Loss : 6.112 | G Loss : 16.248\n",
            "Batch : 1024, D Loss : 6.282 | G Loss : 16.006\n",
            "Batch : 1025, D Loss : 6.155 | G Loss : 15.205\n",
            "Batch : 1026, D Loss : 6.319 | G Loss : 15.970\n",
            "Batch : 1027, D Loss : 6.302 | G Loss : 15.538\n",
            "Batch : 1028, D Loss : 6.149 | G Loss : 15.224\n",
            "Batch : 1029, D Loss : 6.299 | G Loss : 15.062\n",
            "Batch : 1030, D Loss : 6.187 | G Loss : 16.067\n",
            "Batch : 1031, D Loss : 6.079 | G Loss : 16.181\n",
            "Batch : 1032, D Loss : 6.152 | G Loss : 15.444\n",
            "Batch : 1033, D Loss : 6.086 | G Loss : 15.960\n",
            "Batch : 1034, D Loss : 6.246 | G Loss : 16.130\n",
            "Batch : 1035, D Loss : 6.509 | G Loss : 15.765\n",
            "Batch : 1036, D Loss : 6.458 | G Loss : 15.392\n",
            "Batch : 1037, D Loss : 6.272 | G Loss : 14.333\n",
            "Batch : 1038, D Loss : 5.728 | G Loss : 16.470\n",
            "Batch : 1039, D Loss : 5.321 | G Loss : 14.832\n",
            "Batch : 1040, D Loss : 5.562 | G Loss : 14.175\n",
            "Batch : 1041, D Loss : 5.462 | G Loss : 16.041\n",
            "Batch : 1042, D Loss : 4.809 | G Loss : 14.512\n",
            "Batch : 1043, D Loss : 4.615 | G Loss : 14.200\n",
            "Batch : 1044, D Loss : 6.015 | G Loss : 15.396\n",
            "Batch : 1045, D Loss : 6.179 | G Loss : 16.083\n",
            "Batch : 1046, D Loss : 5.919 | G Loss : 15.212\n",
            "Batch : 1047, D Loss : 4.795 | G Loss : 14.128\n",
            "Batch : 1048, D Loss : 6.016 | G Loss : 15.265\n",
            "Batch : 1049, D Loss : 5.635 | G Loss : 15.392\n",
            "Batch : 1050, D Loss : 5.395 | G Loss : 14.590\n",
            "Batch : 1051, D Loss : 5.090 | G Loss : 15.280\n",
            "Batch : 1052, D Loss : 4.900 | G Loss : 14.998\n",
            "Batch : 1053, D Loss : 4.785 | G Loss : 15.343\n",
            "Batch : 1054, D Loss : 4.843 | G Loss : 15.609\n",
            "Batch : 1055, D Loss : 4.651 | G Loss : 15.181\n",
            "Batch : 1056, D Loss : 4.279 | G Loss : 13.997\n",
            "Batch : 1057, D Loss : 4.148 | G Loss : 14.036\n",
            "Batch : 1058, D Loss : 4.165 | G Loss : 13.615\n",
            "Batch : 1059, D Loss : 4.052 | G Loss : 12.924\n",
            "Batch : 1060, D Loss : 3.967 | G Loss : 12.637\n",
            "Batch : 1061, D Loss : 4.214 | G Loss : 12.779\n",
            "Batch : 1062, D Loss : 4.211 | G Loss : 13.370\n",
            "Batch : 1063, D Loss : 4.284 | G Loss : 12.624\n",
            "Batch : 1064, D Loss : 4.102 | G Loss : 12.913\n",
            "Batch : 1065, D Loss : 3.899 | G Loss : 12.800\n",
            "Batch : 1066, D Loss : 4.165 | G Loss : 14.487\n",
            "Batch : 1067, D Loss : 4.256 | G Loss : 13.802\n",
            "Batch : 1068, D Loss : 4.163 | G Loss : 14.388\n",
            "Batch : 1069, D Loss : 4.145 | G Loss : 11.633\n",
            "Batch : 1070, D Loss : 4.024 | G Loss : 12.666\n",
            "Batch : 1071, D Loss : 4.145 | G Loss : 12.423\n",
            "Batch : 1072, D Loss : 4.104 | G Loss : 13.290\n",
            "Batch : 1073, D Loss : 4.036 | G Loss : 13.310\n",
            "Batch : 1074, D Loss : 4.347 | G Loss : 12.347\n",
            "Batch : 1075, D Loss : 4.434 | G Loss : 13.233\n",
            "Batch : 1076, D Loss : 4.274 | G Loss : 13.718\n",
            "Batch : 1077, D Loss : 4.182 | G Loss : 13.627\n",
            "Batch : 1078, D Loss : 4.202 | G Loss : 12.934\n",
            "Batch : 1079, D Loss : 4.184 | G Loss : 12.596\n",
            "Batch : 1080, D Loss : 3.982 | G Loss : 13.069\n",
            "Batch : 1081, D Loss : 4.318 | G Loss : 14.187\n",
            "Batch : 1082, D Loss : 4.116 | G Loss : 12.245\n",
            "Batch : 1083, D Loss : 3.997 | G Loss : 12.905\n",
            "Batch : 1084, D Loss : 4.122 | G Loss : 12.991\n",
            "Batch : 1085, D Loss : 4.078 | G Loss : 12.179\n",
            "Batch : 1086, D Loss : 4.089 | G Loss : 12.586\n",
            "Batch : 1087, D Loss : 4.051 | G Loss : 13.961\n",
            "Batch : 1088, D Loss : 4.145 | G Loss : 12.162\n",
            "Batch : 1089, D Loss : 4.095 | G Loss : 12.301\n",
            "Batch : 1090, D Loss : 3.913 | G Loss : 12.312\n",
            "Batch : 1091, D Loss : 3.935 | G Loss : 13.323\n",
            "Batch : 1092, D Loss : 4.264 | G Loss : 15.287\n",
            "Batch : 1093, D Loss : 4.216 | G Loss : 12.052\n",
            "Batch : 1094, D Loss : 4.080 | G Loss : 12.684\n",
            "Batch : 1095, D Loss : 4.130 | G Loss : 13.364\n",
            "Batch : 1096, D Loss : 4.122 | G Loss : 12.485\n",
            "Batch : 1097, D Loss : 3.915 | G Loss : 12.352\n",
            "Batch : 1098, D Loss : 4.241 | G Loss : 14.224\n",
            "Batch : 1099, D Loss : 4.116 | G Loss : 11.923\n",
            "Batch : 1100, D Loss : 4.074 | G Loss : 12.363\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            ">Saved: plot_000001.png and g_model & d_model\n",
            " ========== Epoch 2 ========== \n",
            "Batch : 1, D Loss : 3.967 | G Loss : 11.617\n",
            "Batch : 2, D Loss : 4.209 | G Loss : 12.113\n",
            "Batch : 3, D Loss : 4.213 | G Loss : 12.784\n",
            "Batch : 4, D Loss : 3.933 | G Loss : 12.610\n",
            "Batch : 5, D Loss : 4.084 | G Loss : 12.177\n",
            "Batch : 6, D Loss : 3.920 | G Loss : 11.502\n",
            "Batch : 7, D Loss : 4.106 | G Loss : 12.606\n",
            "Batch : 8, D Loss : 4.154 | G Loss : 14.468\n",
            "Batch : 9, D Loss : 4.137 | G Loss : 14.230\n",
            "Batch : 10, D Loss : 4.021 | G Loss : 13.020\n",
            "Batch : 11, D Loss : 3.941 | G Loss : 11.584\n",
            "Batch : 12, D Loss : 3.986 | G Loss : 13.278\n",
            "Batch : 13, D Loss : 4.051 | G Loss : 13.280\n",
            "Batch : 14, D Loss : 4.067 | G Loss : 12.130\n",
            "Batch : 15, D Loss : 3.965 | G Loss : 12.415\n",
            "Batch : 16, D Loss : 3.959 | G Loss : 13.366\n",
            "Batch : 17, D Loss : 4.023 | G Loss : 11.874\n",
            "Batch : 18, D Loss : 4.142 | G Loss : 13.486\n",
            "Batch : 19, D Loss : 4.049 | G Loss : 12.432\n",
            "Batch : 20, D Loss : 4.025 | G Loss : 11.953\n",
            "Batch : 21, D Loss : 4.040 | G Loss : 12.333\n",
            "Batch : 22, D Loss : 3.983 | G Loss : 10.983\n",
            "Batch : 23, D Loss : 4.023 | G Loss : 12.118\n",
            "Batch : 24, D Loss : 4.069 | G Loss : 12.876\n",
            "Batch : 25, D Loss : 3.917 | G Loss : 11.937\n",
            "Batch : 26, D Loss : 3.857 | G Loss : 12.260\n",
            "Batch : 27, D Loss : 4.058 | G Loss : 14.003\n",
            "Batch : 28, D Loss : 4.072 | G Loss : 15.500\n",
            "Batch : 29, D Loss : 3.958 | G Loss : 11.900\n",
            "Batch : 30, D Loss : 4.019 | G Loss : 13.658\n",
            "Batch : 31, D Loss : 4.009 | G Loss : 11.530\n",
            "Batch : 32, D Loss : 4.041 | G Loss : 13.058\n",
            "Batch : 33, D Loss : 4.005 | G Loss : 12.469\n",
            "Batch : 34, D Loss : 3.927 | G Loss : 12.965\n",
            "Batch : 35, D Loss : 3.941 | G Loss : 12.257\n",
            "Batch : 36, D Loss : 4.016 | G Loss : 12.153\n",
            "Batch : 37, D Loss : 3.920 | G Loss : 14.361\n",
            "Batch : 38, D Loss : 4.222 | G Loss : 13.138\n",
            "Batch : 39, D Loss : 4.613 | G Loss : 11.629\n",
            "Batch : 40, D Loss : 4.415 | G Loss : 11.990\n",
            "Batch : 41, D Loss : 4.323 | G Loss : 14.988\n",
            "Batch : 42, D Loss : 4.571 | G Loss : 12.107\n",
            "Batch : 43, D Loss : 4.190 | G Loss : 11.925\n",
            "Batch : 44, D Loss : 3.897 | G Loss : 14.258\n",
            "Batch : 45, D Loss : 4.058 | G Loss : 11.930\n",
            "Batch : 46, D Loss : 4.010 | G Loss : 12.706\n",
            "Batch : 47, D Loss : 3.953 | G Loss : 11.963\n",
            "Batch : 48, D Loss : 3.877 | G Loss : 11.992\n",
            "Batch : 49, D Loss : 4.043 | G Loss : 13.063\n",
            "Batch : 50, D Loss : 3.890 | G Loss : 11.861\n",
            "Batch : 51, D Loss : 3.899 | G Loss : 12.394\n",
            "Batch : 52, D Loss : 3.942 | G Loss : 13.051\n",
            "Batch : 53, D Loss : 4.063 | G Loss : 13.916\n",
            "Batch : 54, D Loss : 3.995 | G Loss : 10.986\n",
            "Batch : 55, D Loss : 3.988 | G Loss : 11.177\n",
            "Batch : 56, D Loss : 3.922 | G Loss : 11.222\n",
            "Batch : 57, D Loss : 3.884 | G Loss : 12.821\n",
            "Batch : 58, D Loss : 6.287 | G Loss : 16.902\n",
            "Batch : 59, D Loss : 6.496 | G Loss : 17.930\n",
            "Batch : 60, D Loss : 6.450 | G Loss : 16.677\n",
            "Batch : 61, D Loss : 6.175 | G Loss : 15.716\n",
            "Batch : 62, D Loss : 6.081 | G Loss : 15.438\n",
            "Batch : 63, D Loss : 5.177 | G Loss : 16.226\n",
            "Batch : 64, D Loss : 6.247 | G Loss : 16.284\n",
            "Batch : 65, D Loss : 5.748 | G Loss : 13.526\n",
            "Batch : 66, D Loss : 5.034 | G Loss : 15.547\n",
            "Batch : 67, D Loss : 4.314 | G Loss : 13.968\n",
            "Batch : 68, D Loss : 4.089 | G Loss : 14.952\n",
            "Batch : 69, D Loss : 3.994 | G Loss : 14.680\n",
            "Batch : 70, D Loss : 3.970 | G Loss : 13.039\n",
            "Batch : 71, D Loss : 3.971 | G Loss : 13.195\n",
            "Batch : 72, D Loss : 4.001 | G Loss : 12.971\n",
            "Batch : 73, D Loss : 3.997 | G Loss : 15.454\n",
            "Batch : 74, D Loss : 3.860 | G Loss : 12.769\n",
            "Batch : 75, D Loss : 3.955 | G Loss : 11.247\n",
            "Batch : 76, D Loss : 3.993 | G Loss : 12.867\n",
            "Batch : 77, D Loss : 3.885 | G Loss : 11.488\n",
            "Batch : 78, D Loss : 3.944 | G Loss : 12.898\n",
            "Batch : 79, D Loss : 3.986 | G Loss : 12.138\n",
            "Batch : 80, D Loss : 4.037 | G Loss : 12.638\n",
            "Batch : 81, D Loss : 3.887 | G Loss : 14.231\n",
            "Batch : 82, D Loss : 3.876 | G Loss : 13.058\n",
            "Batch : 83, D Loss : 3.969 | G Loss : 11.978\n",
            "Batch : 84, D Loss : 3.968 | G Loss : 12.215\n",
            "Batch : 85, D Loss : 3.841 | G Loss : 13.195\n",
            "Batch : 86, D Loss : 3.988 | G Loss : 13.247\n",
            "Batch : 87, D Loss : 3.840 | G Loss : 12.104\n",
            "Batch : 88, D Loss : 3.967 | G Loss : 12.448\n",
            "Batch : 89, D Loss : 3.936 | G Loss : 12.068\n",
            "Batch : 90, D Loss : 4.051 | G Loss : 13.379\n",
            "Batch : 91, D Loss : 3.858 | G Loss : 13.321\n",
            "Batch : 92, D Loss : 3.925 | G Loss : 12.977\n",
            "Batch : 93, D Loss : 3.862 | G Loss : 11.070\n",
            "Batch : 94, D Loss : 3.921 | G Loss : 13.140\n",
            "Batch : 95, D Loss : 3.989 | G Loss : 12.895\n",
            "Batch : 96, D Loss : 3.815 | G Loss : 11.710\n",
            "Batch : 97, D Loss : 3.997 | G Loss : 13.192\n",
            "Batch : 98, D Loss : 3.802 | G Loss : 11.116\n",
            "Batch : 99, D Loss : 3.909 | G Loss : 11.446\n",
            "Batch : 100, D Loss : 3.950 | G Loss : 12.890\n",
            "Batch : 101, D Loss : 3.920 | G Loss : 12.678\n",
            "Batch : 102, D Loss : 3.941 | G Loss : 12.154\n",
            "Batch : 103, D Loss : 3.914 | G Loss : 12.375\n",
            "Batch : 104, D Loss : 3.862 | G Loss : 12.204\n",
            "Batch : 105, D Loss : 3.830 | G Loss : 11.780\n",
            "Batch : 106, D Loss : 3.879 | G Loss : 12.754\n",
            "Batch : 107, D Loss : 3.797 | G Loss : 12.688\n",
            "Batch : 108, D Loss : 3.847 | G Loss : 11.895\n",
            "Batch : 109, D Loss : 3.830 | G Loss : 12.894\n",
            "Batch : 110, D Loss : 3.912 | G Loss : 11.651\n",
            "Batch : 111, D Loss : 3.823 | G Loss : 12.611\n",
            "Batch : 112, D Loss : 3.871 | G Loss : 11.892\n",
            "Batch : 113, D Loss : 3.799 | G Loss : 11.406\n",
            "Batch : 114, D Loss : 3.879 | G Loss : 11.749\n",
            "Batch : 115, D Loss : 3.905 | G Loss : 12.154\n",
            "Batch : 116, D Loss : 3.801 | G Loss : 11.209\n",
            "Batch : 117, D Loss : 4.397 | G Loss : 13.486\n",
            "Batch : 118, D Loss : 4.473 | G Loss : 11.396\n",
            "Batch : 119, D Loss : 4.196 | G Loss : 12.945\n",
            "Batch : 120, D Loss : 3.947 | G Loss : 11.819\n",
            "Batch : 121, D Loss : 3.893 | G Loss : 11.953\n",
            "Batch : 122, D Loss : 3.882 | G Loss : 11.459\n",
            "Batch : 123, D Loss : 3.804 | G Loss : 12.175\n",
            "Batch : 124, D Loss : 3.906 | G Loss : 12.309\n",
            "Batch : 125, D Loss : 3.915 | G Loss : 11.586\n",
            "Batch : 126, D Loss : 3.757 | G Loss : 10.722\n",
            "Batch : 127, D Loss : 4.036 | G Loss : 14.164\n",
            "Batch : 128, D Loss : 3.735 | G Loss : 12.555\n",
            "Batch : 129, D Loss : 3.791 | G Loss : 11.935\n",
            "Batch : 130, D Loss : 3.789 | G Loss : 12.064\n",
            "Batch : 131, D Loss : 3.797 | G Loss : 13.395\n",
            "Batch : 132, D Loss : 3.761 | G Loss : 11.953\n",
            "Batch : 133, D Loss : 3.883 | G Loss : 13.137\n",
            "Batch : 134, D Loss : 3.864 | G Loss : 12.213\n",
            "Batch : 135, D Loss : 3.881 | G Loss : 11.272\n",
            "Batch : 136, D Loss : 3.847 | G Loss : 11.370\n",
            "Batch : 137, D Loss : 3.868 | G Loss : 14.097\n",
            "Batch : 138, D Loss : 3.941 | G Loss : 11.904\n",
            "Batch : 139, D Loss : 3.876 | G Loss : 11.659\n",
            "Batch : 140, D Loss : 3.944 | G Loss : 12.384\n",
            "Batch : 141, D Loss : 3.854 | G Loss : 11.825\n",
            "Batch : 142, D Loss : 3.822 | G Loss : 11.465\n",
            "Batch : 143, D Loss : 3.773 | G Loss : 13.628\n",
            "Batch : 144, D Loss : 4.150 | G Loss : 12.707\n",
            "Batch : 145, D Loss : 4.082 | G Loss : 12.385\n",
            "Batch : 146, D Loss : 3.954 | G Loss : 11.539\n",
            "Batch : 147, D Loss : 3.850 | G Loss : 13.017\n",
            "Batch : 148, D Loss : 3.802 | G Loss : 13.039\n",
            "Batch : 149, D Loss : 3.786 | G Loss : 10.852\n",
            "Batch : 150, D Loss : 3.816 | G Loss : 10.982\n",
            "Batch : 151, D Loss : 3.740 | G Loss : 11.321\n",
            "Batch : 152, D Loss : 3.963 | G Loss : 11.810\n",
            "Batch : 153, D Loss : 3.848 | G Loss : 13.093\n",
            "Batch : 154, D Loss : 3.811 | G Loss : 12.807\n",
            "Batch : 155, D Loss : 3.890 | G Loss : 12.095\n",
            "Batch : 156, D Loss : 3.756 | G Loss : 12.277\n",
            "Batch : 157, D Loss : 3.761 | G Loss : 11.680\n",
            "Batch : 158, D Loss : 3.741 | G Loss : 11.454\n",
            "Batch : 159, D Loss : 3.788 | G Loss : 12.291\n",
            "Batch : 160, D Loss : 3.849 | G Loss : 12.712\n",
            "Batch : 161, D Loss : 3.858 | G Loss : 12.813\n",
            "Batch : 162, D Loss : 3.832 | G Loss : 11.218\n",
            "Batch : 163, D Loss : 3.841 | G Loss : 11.125\n",
            "Batch : 164, D Loss : 3.807 | G Loss : 13.301\n",
            "Batch : 165, D Loss : 3.791 | G Loss : 11.675\n",
            "Batch : 166, D Loss : 3.813 | G Loss : 11.970\n",
            "Batch : 167, D Loss : 3.823 | G Loss : 12.582\n",
            "Batch : 168, D Loss : 3.850 | G Loss : 12.355\n",
            "Batch : 169, D Loss : 3.809 | G Loss : 14.675\n",
            "Batch : 170, D Loss : 3.811 | G Loss : 11.449\n",
            "Batch : 171, D Loss : 3.704 | G Loss : 12.281\n",
            "Batch : 172, D Loss : 3.856 | G Loss : 13.307\n",
            "Batch : 173, D Loss : 3.795 | G Loss : 11.939\n",
            "Batch : 174, D Loss : 3.732 | G Loss : 11.681\n",
            "Batch : 175, D Loss : 3.823 | G Loss : 10.741\n",
            "Batch : 176, D Loss : 3.794 | G Loss : 11.027\n",
            "Batch : 177, D Loss : 3.905 | G Loss : 12.635\n",
            "Batch : 178, D Loss : 3.752 | G Loss : 10.992\n",
            "Batch : 179, D Loss : 3.812 | G Loss : 13.474\n",
            "Batch : 180, D Loss : 3.929 | G Loss : 14.803\n",
            "Batch : 181, D Loss : 4.029 | G Loss : 12.201\n",
            "Batch : 182, D Loss : 3.992 | G Loss : 11.682\n",
            "Batch : 183, D Loss : 4.094 | G Loss : 14.019\n",
            "Batch : 184, D Loss : 3.927 | G Loss : 11.728\n",
            "Batch : 185, D Loss : 3.788 | G Loss : 11.795\n",
            "Batch : 186, D Loss : 3.775 | G Loss : 11.776\n",
            "Batch : 187, D Loss : 3.828 | G Loss : 11.264\n",
            "Batch : 188, D Loss : 3.913 | G Loss : 12.697\n",
            "Batch : 189, D Loss : 3.837 | G Loss : 12.297\n",
            "Batch : 190, D Loss : 3.814 | G Loss : 11.532\n",
            "Batch : 191, D Loss : 3.731 | G Loss : 11.734\n",
            "Batch : 192, D Loss : 3.792 | G Loss : 13.352\n",
            "Batch : 193, D Loss : 3.739 | G Loss : 11.244\n",
            "Batch : 194, D Loss : 3.711 | G Loss : 10.722\n",
            "Batch : 195, D Loss : 3.826 | G Loss : 11.983\n",
            "Batch : 196, D Loss : 3.864 | G Loss : 12.004\n",
            "Batch : 197, D Loss : 3.714 | G Loss : 12.034\n",
            "Batch : 198, D Loss : 3.862 | G Loss : 11.635\n",
            "Batch : 199, D Loss : 3.934 | G Loss : 11.236\n",
            "Batch : 200, D Loss : 3.810 | G Loss : 12.067\n",
            "Batch : 201, D Loss : 3.736 | G Loss : 12.270\n",
            "Batch : 202, D Loss : 3.860 | G Loss : 11.061\n",
            "Batch : 203, D Loss : 3.718 | G Loss : 11.804\n",
            "Batch : 204, D Loss : 3.747 | G Loss : 10.909\n",
            "Batch : 205, D Loss : 3.809 | G Loss : 13.209\n",
            "Batch : 206, D Loss : 3.917 | G Loss : 12.563\n",
            "Batch : 207, D Loss : 3.817 | G Loss : 11.650\n",
            "Batch : 208, D Loss : 3.874 | G Loss : 11.496\n",
            "Batch : 209, D Loss : 3.816 | G Loss : 10.884\n",
            "Batch : 210, D Loss : 3.749 | G Loss : 11.923\n",
            "Batch : 211, D Loss : 3.791 | G Loss : 11.432\n",
            "Batch : 212, D Loss : 3.811 | G Loss : 10.381\n",
            "Batch : 213, D Loss : 3.752 | G Loss : 11.178\n",
            "Batch : 214, D Loss : 3.755 | G Loss : 10.754\n",
            "Batch : 215, D Loss : 3.793 | G Loss : 11.923\n",
            "Batch : 216, D Loss : 3.825 | G Loss : 11.792\n",
            "Batch : 217, D Loss : 3.769 | G Loss : 11.553\n",
            "Batch : 218, D Loss : 4.033 | G Loss : 12.740\n",
            "Batch : 219, D Loss : 4.189 | G Loss : 12.551\n",
            "Batch : 220, D Loss : 3.976 | G Loss : 11.266\n",
            "Batch : 221, D Loss : 4.101 | G Loss : 12.924\n",
            "Batch : 222, D Loss : 4.073 | G Loss : 12.953\n",
            "Batch : 223, D Loss : 3.863 | G Loss : 12.555\n",
            "Batch : 224, D Loss : 3.763 | G Loss : 13.306\n",
            "Batch : 225, D Loss : 3.699 | G Loss : 11.303\n",
            "Batch : 226, D Loss : 3.830 | G Loss : 13.025\n",
            "Batch : 227, D Loss : 3.919 | G Loss : 11.509\n",
            "Batch : 228, D Loss : 3.755 | G Loss : 12.359\n",
            "Batch : 229, D Loss : 3.809 | G Loss : 10.870\n",
            "Batch : 230, D Loss : 3.791 | G Loss : 12.295\n",
            "Batch : 231, D Loss : 3.823 | G Loss : 11.648\n",
            "Batch : 232, D Loss : 3.917 | G Loss : 11.686\n",
            "Batch : 233, D Loss : 3.733 | G Loss : 11.523\n",
            "Batch : 234, D Loss : 3.867 | G Loss : 11.300\n",
            "Batch : 235, D Loss : 3.826 | G Loss : 11.205\n",
            "Batch : 236, D Loss : 3.758 | G Loss : 11.976\n",
            "Batch : 237, D Loss : 3.797 | G Loss : 11.021\n",
            "Batch : 238, D Loss : 3.810 | G Loss : 11.901\n",
            "Batch : 239, D Loss : 3.832 | G Loss : 11.775\n",
            "Batch : 240, D Loss : 3.773 | G Loss : 12.659\n",
            "Batch : 241, D Loss : 3.687 | G Loss : 12.444\n",
            "Batch : 242, D Loss : 3.834 | G Loss : 11.180\n",
            "Batch : 243, D Loss : 3.701 | G Loss : 11.046\n",
            "Batch : 244, D Loss : 3.779 | G Loss : 12.886\n",
            "Batch : 245, D Loss : 3.771 | G Loss : 11.456\n",
            "Batch : 246, D Loss : 3.688 | G Loss : 11.091\n",
            "Batch : 247, D Loss : 3.872 | G Loss : 11.579\n",
            "Batch : 248, D Loss : 3.749 | G Loss : 11.138\n",
            "Batch : 249, D Loss : 3.744 | G Loss : 10.439\n",
            "Batch : 250, D Loss : 3.794 | G Loss : 12.863\n",
            "Batch : 251, D Loss : 3.825 | G Loss : 10.263\n",
            "Batch : 252, D Loss : 3.818 | G Loss : 10.882\n",
            "Batch : 253, D Loss : 3.883 | G Loss : 11.816\n",
            "Batch : 254, D Loss : 3.787 | G Loss : 13.341\n",
            "Batch : 255, D Loss : 3.807 | G Loss : 9.869\n",
            "Batch : 256, D Loss : 3.832 | G Loss : 11.577\n",
            "Batch : 257, D Loss : 3.755 | G Loss : 11.730\n",
            "Batch : 258, D Loss : 3.762 | G Loss : 11.712\n",
            "Batch : 259, D Loss : 3.758 | G Loss : 10.614\n",
            "Batch : 260, D Loss : 3.803 | G Loss : 11.293\n",
            "Batch : 261, D Loss : 3.819 | G Loss : 12.461\n",
            "Batch : 262, D Loss : 3.719 | G Loss : 11.842\n",
            "Batch : 263, D Loss : 3.983 | G Loss : 13.309\n",
            "Batch : 264, D Loss : 4.230 | G Loss : 12.698\n",
            "Batch : 265, D Loss : 4.089 | G Loss : 12.550\n",
            "Batch : 266, D Loss : 3.858 | G Loss : 11.922\n",
            "Batch : 267, D Loss : 3.865 | G Loss : 12.429\n",
            "Batch : 268, D Loss : 3.850 | G Loss : 11.509\n",
            "Batch : 269, D Loss : 3.810 | G Loss : 12.623\n",
            "Batch : 270, D Loss : 3.847 | G Loss : 11.754\n",
            "Batch : 271, D Loss : 3.751 | G Loss : 12.626\n",
            "Batch : 272, D Loss : 3.641 | G Loss : 12.177\n",
            "Batch : 273, D Loss : 3.879 | G Loss : 12.333\n",
            "Batch : 274, D Loss : 3.794 | G Loss : 11.409\n",
            "Batch : 275, D Loss : 3.683 | G Loss : 11.541\n",
            "Batch : 276, D Loss : 3.901 | G Loss : 11.364\n",
            "Batch : 277, D Loss : 3.913 | G Loss : 12.140\n",
            "Batch : 278, D Loss : 3.870 | G Loss : 12.051\n",
            "Batch : 279, D Loss : 3.859 | G Loss : 11.905\n",
            "Batch : 280, D Loss : 3.943 | G Loss : 13.310\n",
            "Batch : 281, D Loss : 3.789 | G Loss : 12.146\n",
            "Batch : 282, D Loss : 3.674 | G Loss : 11.963\n",
            "Batch : 283, D Loss : 3.688 | G Loss : 12.647\n",
            "Batch : 284, D Loss : 3.786 | G Loss : 11.363\n",
            "Batch : 285, D Loss : 3.776 | G Loss : 13.451\n",
            "Batch : 286, D Loss : 3.740 | G Loss : 11.965\n",
            "Batch : 287, D Loss : 3.802 | G Loss : 11.998\n",
            "Batch : 288, D Loss : 3.749 | G Loss : 11.862\n",
            "Batch : 289, D Loss : 3.840 | G Loss : 11.828\n",
            "Batch : 290, D Loss : 3.721 | G Loss : 12.563\n",
            "Batch : 291, D Loss : 3.720 | G Loss : 10.032\n",
            "Batch : 292, D Loss : 3.843 | G Loss : 12.655\n",
            "Batch : 293, D Loss : 3.743 | G Loss : 10.383\n",
            "Batch : 294, D Loss : 3.739 | G Loss : 11.385\n",
            "Batch : 295, D Loss : 3.823 | G Loss : 11.526\n",
            "Batch : 296, D Loss : 3.734 | G Loss : 10.651\n",
            "Batch : 297, D Loss : 3.842 | G Loss : 11.116\n",
            "Batch : 298, D Loss : 3.805 | G Loss : 13.561\n",
            "Batch : 299, D Loss : 3.825 | G Loss : 11.900\n",
            "Batch : 300, D Loss : 3.858 | G Loss : 11.807\n",
            "Batch : 301, D Loss : 3.861 | G Loss : 11.121\n",
            "Batch : 302, D Loss : 3.716 | G Loss : 11.493\n",
            "Batch : 303, D Loss : 3.815 | G Loss : 12.882\n",
            "Batch : 304, D Loss : 3.781 | G Loss : 10.600\n",
            "Batch : 305, D Loss : 3.739 | G Loss : 11.614\n",
            "Batch : 306, D Loss : 3.804 | G Loss : 12.118\n",
            "Batch : 307, D Loss : 3.794 | G Loss : 10.738\n",
            "Batch : 308, D Loss : 3.785 | G Loss : 10.892\n",
            "Batch : 309, D Loss : 3.735 | G Loss : 11.475\n",
            "Batch : 310, D Loss : 3.771 | G Loss : 10.459\n",
            "Batch : 311, D Loss : 3.779 | G Loss : 13.483\n",
            "Batch : 312, D Loss : 3.780 | G Loss : 11.800\n",
            "Batch : 313, D Loss : 3.806 | G Loss : 12.106\n",
            "Batch : 314, D Loss : 3.774 | G Loss : 12.699\n",
            "Batch : 315, D Loss : 3.829 | G Loss : 11.568\n",
            "Batch : 316, D Loss : 3.766 | G Loss : 12.471\n",
            "Batch : 317, D Loss : 3.877 | G Loss : 11.207\n",
            "Batch : 318, D Loss : 3.777 | G Loss : 12.104\n",
            "Batch : 319, D Loss : 3.821 | G Loss : 11.492\n",
            "Batch : 320, D Loss : 3.788 | G Loss : 10.424\n",
            "Batch : 321, D Loss : 3.763 | G Loss : 11.394\n",
            "Batch : 322, D Loss : 3.800 | G Loss : 11.872\n",
            "Batch : 323, D Loss : 3.806 | G Loss : 12.022\n",
            "Batch : 324, D Loss : 3.786 | G Loss : 12.803\n",
            "Batch : 325, D Loss : 3.770 | G Loss : 11.829\n",
            "Batch : 326, D Loss : 3.967 | G Loss : 11.555\n",
            "Batch : 327, D Loss : 3.854 | G Loss : 12.583\n",
            "Batch : 328, D Loss : 3.917 | G Loss : 12.482\n",
            "Batch : 329, D Loss : 3.890 | G Loss : 12.338\n",
            "Batch : 330, D Loss : 3.739 | G Loss : 10.979\n",
            "Batch : 331, D Loss : 3.745 | G Loss : 12.274\n",
            "Batch : 332, D Loss : 3.813 | G Loss : 10.694\n",
            "Batch : 333, D Loss : 3.678 | G Loss : 12.243\n",
            "Batch : 334, D Loss : 3.802 | G Loss : 12.467\n",
            "Batch : 335, D Loss : 3.781 | G Loss : 11.459\n",
            "Batch : 336, D Loss : 3.690 | G Loss : 12.147\n",
            "Batch : 337, D Loss : 3.956 | G Loss : 11.264\n",
            "Batch : 338, D Loss : 3.891 | G Loss : 11.539\n",
            "Batch : 339, D Loss : 3.786 | G Loss : 13.132\n",
            "Batch : 340, D Loss : 3.754 | G Loss : 12.533\n",
            "Batch : 341, D Loss : 3.752 | G Loss : 12.015\n",
            "Batch : 342, D Loss : 4.017 | G Loss : 11.692\n",
            "Batch : 343, D Loss : 3.978 | G Loss : 11.413\n",
            "Batch : 344, D Loss : 3.888 | G Loss : 11.667\n",
            "Batch : 345, D Loss : 3.905 | G Loss : 12.584\n",
            "Batch : 346, D Loss : 3.750 | G Loss : 12.016\n",
            "Batch : 347, D Loss : 3.719 | G Loss : 11.767\n",
            "Batch : 348, D Loss : 3.845 | G Loss : 12.320\n",
            "Batch : 349, D Loss : 3.906 | G Loss : 11.578\n",
            "Batch : 350, D Loss : 3.800 | G Loss : 12.203\n",
            "Batch : 351, D Loss : 3.976 | G Loss : 11.143\n",
            "Batch : 352, D Loss : 3.837 | G Loss : 10.552\n",
            "Batch : 353, D Loss : 3.682 | G Loss : 10.338\n",
            "Batch : 354, D Loss : 3.761 | G Loss : 12.983\n",
            "Batch : 355, D Loss : 3.823 | G Loss : 11.360\n",
            "Batch : 356, D Loss : 3.739 | G Loss : 10.124\n",
            "Batch : 357, D Loss : 3.871 | G Loss : 12.485\n",
            "Batch : 358, D Loss : 3.848 | G Loss : 13.613\n",
            "Batch : 359, D Loss : 3.827 | G Loss : 10.656\n",
            "Batch : 360, D Loss : 3.784 | G Loss : 10.916\n",
            "Batch : 361, D Loss : 3.907 | G Loss : 10.772\n",
            "Batch : 362, D Loss : 3.796 | G Loss : 12.889\n",
            "Batch : 363, D Loss : 3.899 | G Loss : 11.452\n",
            "Batch : 364, D Loss : 3.873 | G Loss : 12.824\n",
            "Batch : 365, D Loss : 3.789 | G Loss : 10.699\n",
            "Batch : 366, D Loss : 3.822 | G Loss : 12.400\n",
            "Batch : 367, D Loss : 3.728 | G Loss : 11.086\n",
            "Batch : 368, D Loss : 3.792 | G Loss : 11.009\n",
            "Batch : 369, D Loss : 3.815 | G Loss : 10.840\n",
            "Batch : 370, D Loss : 3.993 | G Loss : 11.911\n",
            "Batch : 371, D Loss : 3.926 | G Loss : 11.357\n",
            "Batch : 372, D Loss : 3.686 | G Loss : 13.890\n",
            "Batch : 373, D Loss : 6.480 | G Loss : 16.620\n",
            "Batch : 374, D Loss : 6.307 | G Loss : 17.674\n",
            "Batch : 375, D Loss : 6.363 | G Loss : 16.221\n",
            "Batch : 376, D Loss : 6.079 | G Loss : 15.709\n",
            "Batch : 377, D Loss : 6.452 | G Loss : 16.410\n",
            "Batch : 378, D Loss : 6.256 | G Loss : 15.364\n",
            "Batch : 379, D Loss : 6.011 | G Loss : 14.566\n",
            "Batch : 380, D Loss : 6.332 | G Loss : 14.620\n",
            "Batch : 381, D Loss : 6.447 | G Loss : 17.045\n",
            "Batch : 382, D Loss : 5.912 | G Loss : 14.257\n",
            "Batch : 383, D Loss : 6.291 | G Loss : 14.700\n",
            "Batch : 384, D Loss : 6.386 | G Loss : 14.916\n",
            "Batch : 385, D Loss : 6.043 | G Loss : 14.957\n",
            "Batch : 386, D Loss : 5.999 | G Loss : 14.068\n",
            "Batch : 387, D Loss : 6.006 | G Loss : 14.239\n",
            "Batch : 388, D Loss : 6.128 | G Loss : 15.530\n",
            "Batch : 389, D Loss : 6.257 | G Loss : 15.125\n",
            "Batch : 390, D Loss : 5.651 | G Loss : 13.999\n",
            "Batch : 391, D Loss : 5.649 | G Loss : 13.966\n",
            "Batch : 392, D Loss : 5.114 | G Loss : 13.562\n",
            "Batch : 393, D Loss : 4.038 | G Loss : 14.265\n",
            "Batch : 394, D Loss : 5.566 | G Loss : 13.454\n",
            "Batch : 395, D Loss : 4.775 | G Loss : 12.791\n",
            "Batch : 396, D Loss : 3.687 | G Loss : 13.946\n",
            "Batch : 397, D Loss : 5.921 | G Loss : 15.015\n",
            "Batch : 398, D Loss : 6.152 | G Loss : 17.780\n",
            "Batch : 399, D Loss : 5.409 | G Loss : 14.610\n",
            "Batch : 400, D Loss : 5.297 | G Loss : 15.577\n",
            "Batch : 401, D Loss : 4.954 | G Loss : 14.878\n",
            "Batch : 402, D Loss : 5.129 | G Loss : 15.113\n",
            "Batch : 403, D Loss : 5.084 | G Loss : 14.648\n",
            "Batch : 404, D Loss : 4.790 | G Loss : 14.841\n",
            "Batch : 405, D Loss : 4.575 | G Loss : 13.382\n",
            "Batch : 406, D Loss : 4.500 | G Loss : 14.443\n",
            "Batch : 407, D Loss : 4.937 | G Loss : 16.187\n",
            "Batch : 408, D Loss : 5.097 | G Loss : 14.673\n",
            "Batch : 409, D Loss : 5.120 | G Loss : 14.288\n",
            "Batch : 410, D Loss : 4.859 | G Loss : 15.238\n",
            "Batch : 411, D Loss : 4.547 | G Loss : 12.863\n",
            "Batch : 412, D Loss : 4.390 | G Loss : 14.090\n",
            "Batch : 413, D Loss : 4.141 | G Loss : 11.627\n",
            "Batch : 414, D Loss : 4.200 | G Loss : 11.994\n",
            "Batch : 415, D Loss : 4.063 | G Loss : 11.771\n",
            "Batch : 416, D Loss : 3.949 | G Loss : 12.058\n",
            "Batch : 417, D Loss : 4.067 | G Loss : 12.369\n",
            "Batch : 418, D Loss : 4.086 | G Loss : 14.575\n",
            "Batch : 419, D Loss : 3.906 | G Loss : 11.958\n",
            "Batch : 420, D Loss : 4.251 | G Loss : 12.157\n",
            "Batch : 421, D Loss : 4.220 | G Loss : 12.261\n",
            "Batch : 422, D Loss : 4.079 | G Loss : 11.896\n",
            "Batch : 423, D Loss : 4.007 | G Loss : 11.223\n",
            "Batch : 424, D Loss : 4.171 | G Loss : 12.643\n",
            "Batch : 425, D Loss : 4.058 | G Loss : 11.850\n",
            "Batch : 426, D Loss : 4.487 | G Loss : 13.232\n",
            "Batch : 427, D Loss : 4.186 | G Loss : 13.482\n",
            "Batch : 428, D Loss : 4.037 | G Loss : 12.972\n",
            "Batch : 429, D Loss : 3.936 | G Loss : 10.843\n",
            "Batch : 430, D Loss : 4.162 | G Loss : 12.163\n",
            "Batch : 431, D Loss : 3.988 | G Loss : 13.008\n",
            "Batch : 432, D Loss : 3.953 | G Loss : 11.961\n",
            "Batch : 433, D Loss : 3.783 | G Loss : 10.715\n",
            "Batch : 434, D Loss : 3.995 | G Loss : 12.487\n",
            "Batch : 435, D Loss : 3.817 | G Loss : 12.335\n",
            "Batch : 436, D Loss : 3.958 | G Loss : 11.475\n",
            "Batch : 437, D Loss : 3.805 | G Loss : 10.306\n",
            "Batch : 438, D Loss : 3.870 | G Loss : 12.572\n",
            "Batch : 439, D Loss : 3.841 | G Loss : 11.901\n",
            "Batch : 440, D Loss : 4.011 | G Loss : 10.885\n",
            "Batch : 441, D Loss : 3.997 | G Loss : 12.984\n",
            "Batch : 442, D Loss : 4.026 | G Loss : 13.877\n",
            "Batch : 443, D Loss : 3.969 | G Loss : 11.790\n",
            "Batch : 444, D Loss : 3.885 | G Loss : 11.379\n",
            "Batch : 445, D Loss : 5.225 | G Loss : 13.639\n",
            "Batch : 446, D Loss : 5.364 | G Loss : 13.322\n",
            "Batch : 447, D Loss : 4.678 | G Loss : 12.181\n",
            "Batch : 448, D Loss : 4.084 | G Loss : 13.181\n",
            "Batch : 449, D Loss : 3.832 | G Loss : 14.294\n",
            "Batch : 450, D Loss : 4.043 | G Loss : 11.837\n",
            "Batch : 451, D Loss : 4.010 | G Loss : 11.695\n",
            "Batch : 452, D Loss : 3.945 | G Loss : 11.357\n",
            "Batch : 453, D Loss : 3.961 | G Loss : 15.202\n",
            "Batch : 454, D Loss : 3.903 | G Loss : 11.862\n",
            "Batch : 455, D Loss : 3.915 | G Loss : 11.998\n",
            "Batch : 456, D Loss : 3.763 | G Loss : 11.034\n",
            "Batch : 457, D Loss : 3.785 | G Loss : 11.724\n",
            "Batch : 458, D Loss : 3.820 | G Loss : 10.990\n",
            "Batch : 459, D Loss : 3.771 | G Loss : 13.168\n",
            "Batch : 460, D Loss : 3.914 | G Loss : 11.616\n",
            "Batch : 461, D Loss : 3.838 | G Loss : 12.102\n",
            "Batch : 462, D Loss : 3.885 | G Loss : 10.742\n",
            "Batch : 463, D Loss : 4.100 | G Loss : 12.481\n",
            "Batch : 464, D Loss : 4.105 | G Loss : 13.395\n",
            "Batch : 465, D Loss : 3.990 | G Loss : 12.520\n",
            "Batch : 466, D Loss : 3.995 | G Loss : 12.425\n",
            "Batch : 467, D Loss : 3.824 | G Loss : 12.172\n",
            "Batch : 468, D Loss : 3.851 | G Loss : 10.990\n",
            "Batch : 469, D Loss : 3.843 | G Loss : 12.752\n",
            "Batch : 470, D Loss : 3.906 | G Loss : 11.575\n",
            "Batch : 471, D Loss : 4.082 | G Loss : 11.116\n",
            "Batch : 472, D Loss : 3.942 | G Loss : 12.714\n",
            "Batch : 473, D Loss : 3.718 | G Loss : 10.675\n",
            "Batch : 474, D Loss : 3.740 | G Loss : 11.175\n",
            "Batch : 475, D Loss : 3.814 | G Loss : 12.132\n",
            "Batch : 476, D Loss : 3.881 | G Loss : 11.973\n",
            "Batch : 477, D Loss : 3.894 | G Loss : 11.851\n",
            "Batch : 478, D Loss : 3.805 | G Loss : 11.653\n",
            "Batch : 479, D Loss : 3.767 | G Loss : 11.522\n",
            "Batch : 480, D Loss : 3.822 | G Loss : 11.632\n",
            "Batch : 481, D Loss : 4.067 | G Loss : 12.285\n",
            "Batch : 482, D Loss : 3.898 | G Loss : 11.629\n",
            "Batch : 483, D Loss : 3.850 | G Loss : 13.086\n",
            "Batch : 484, D Loss : 3.721 | G Loss : 11.304\n",
            "Batch : 485, D Loss : 3.906 | G Loss : 11.372\n",
            "Batch : 486, D Loss : 4.072 | G Loss : 12.409\n",
            "Batch : 487, D Loss : 3.849 | G Loss : 10.658\n",
            "Batch : 488, D Loss : 3.742 | G Loss : 13.530\n",
            "Batch : 489, D Loss : 3.733 | G Loss : 10.569\n",
            "Batch : 490, D Loss : 3.825 | G Loss : 11.525\n",
            "Batch : 491, D Loss : 3.908 | G Loss : 12.099\n",
            "Batch : 492, D Loss : 3.803 | G Loss : 11.284\n",
            "Batch : 493, D Loss : 3.777 | G Loss : 11.273\n",
            "Batch : 494, D Loss : 3.798 | G Loss : 11.278\n",
            "Batch : 495, D Loss : 3.742 | G Loss : 11.246\n",
            "Batch : 496, D Loss : 3.884 | G Loss : 11.467\n",
            "Batch : 497, D Loss : 3.797 | G Loss : 11.786\n",
            "Batch : 498, D Loss : 3.818 | G Loss : 11.574\n",
            "Batch : 499, D Loss : 3.743 | G Loss : 11.281\n",
            "Batch : 500, D Loss : 3.811 | G Loss : 13.157\n",
            "Batch : 501, D Loss : 3.875 | G Loss : 12.527\n",
            "Batch : 502, D Loss : 3.824 | G Loss : 13.214\n",
            "Batch : 503, D Loss : 3.758 | G Loss : 10.436\n",
            "Batch : 504, D Loss : 3.757 | G Loss : 12.461\n",
            "Batch : 505, D Loss : 3.774 | G Loss : 13.401\n",
            "Batch : 506, D Loss : 3.795 | G Loss : 11.730\n",
            "Batch : 507, D Loss : 3.739 | G Loss : 11.372\n",
            "Batch : 508, D Loss : 3.751 | G Loss : 11.530\n",
            "Batch : 509, D Loss : 3.694 | G Loss : 10.578\n",
            "Batch : 510, D Loss : 3.812 | G Loss : 11.881\n",
            "Batch : 511, D Loss : 3.822 | G Loss : 10.806\n",
            "Batch : 512, D Loss : 3.796 | G Loss : 10.272\n",
            "Batch : 513, D Loss : 3.750 | G Loss : 10.696\n",
            "Batch : 514, D Loss : 3.821 | G Loss : 12.372\n",
            "Batch : 515, D Loss : 3.813 | G Loss : 12.433\n",
            "Batch : 516, D Loss : 3.783 | G Loss : 10.739\n",
            "Batch : 517, D Loss : 3.805 | G Loss : 10.674\n",
            "Batch : 518, D Loss : 3.711 | G Loss : 11.924\n",
            "Batch : 519, D Loss : 3.856 | G Loss : 10.209\n",
            "Batch : 520, D Loss : 3.782 | G Loss : 10.761\n",
            "Batch : 521, D Loss : 3.656 | G Loss : 11.991\n",
            "Batch : 522, D Loss : 3.795 | G Loss : 12.535\n",
            "Batch : 523, D Loss : 3.859 | G Loss : 11.331\n",
            "Batch : 524, D Loss : 3.757 | G Loss : 11.276\n",
            "Batch : 525, D Loss : 3.717 | G Loss : 11.514\n",
            "Batch : 526, D Loss : 3.789 | G Loss : 14.492\n",
            "Batch : 527, D Loss : 3.889 | G Loss : 12.504\n",
            "Batch : 528, D Loss : 3.778 | G Loss : 11.619\n",
            "Batch : 529, D Loss : 3.845 | G Loss : 11.124\n",
            "Batch : 530, D Loss : 3.770 | G Loss : 9.887\n",
            "Batch : 531, D Loss : 3.721 | G Loss : 11.651\n",
            "Batch : 532, D Loss : 3.760 | G Loss : 11.106\n",
            "Batch : 533, D Loss : 3.894 | G Loss : 12.778\n",
            "Batch : 534, D Loss : 3.752 | G Loss : 10.197\n",
            "Batch : 535, D Loss : 3.982 | G Loss : 12.906\n",
            "Batch : 536, D Loss : 3.885 | G Loss : 11.475\n",
            "Batch : 537, D Loss : 3.805 | G Loss : 10.570\n",
            "Batch : 538, D Loss : 3.857 | G Loss : 10.011\n",
            "Batch : 539, D Loss : 3.873 | G Loss : 12.080\n",
            "Batch : 540, D Loss : 3.829 | G Loss : 11.121\n",
            "Batch : 541, D Loss : 3.766 | G Loss : 11.162\n",
            "Batch : 542, D Loss : 3.758 | G Loss : 12.571\n",
            "Batch : 543, D Loss : 3.720 | G Loss : 11.124\n",
            "Batch : 544, D Loss : 3.758 | G Loss : 11.764\n",
            "Batch : 545, D Loss : 3.760 | G Loss : 10.924\n",
            "Batch : 546, D Loss : 3.795 | G Loss : 12.364\n",
            "Batch : 547, D Loss : 3.797 | G Loss : 11.170\n",
            "Batch : 548, D Loss : 3.820 | G Loss : 11.190\n",
            "Batch : 549, D Loss : 3.783 | G Loss : 10.632\n",
            "Batch : 550, D Loss : 3.776 | G Loss : 12.448\n",
            "Batch : 551, D Loss : 3.898 | G Loss : 11.453\n",
            "Batch : 552, D Loss : 3.761 | G Loss : 10.999\n",
            "Batch : 553, D Loss : 3.843 | G Loss : 11.632\n",
            "Batch : 554, D Loss : 3.788 | G Loss : 12.246\n",
            "Batch : 555, D Loss : 3.674 | G Loss : 10.171\n",
            "Batch : 556, D Loss : 4.409 | G Loss : 11.840\n",
            "Batch : 557, D Loss : 4.172 | G Loss : 11.582\n",
            "Batch : 558, D Loss : 4.073 | G Loss : 11.280\n",
            "Batch : 559, D Loss : 4.639 | G Loss : 13.174\n",
            "Batch : 560, D Loss : 4.615 | G Loss : 11.226\n",
            "Batch : 561, D Loss : 3.865 | G Loss : 11.076\n",
            "Batch : 562, D Loss : 3.740 | G Loss : 11.656\n",
            "Batch : 563, D Loss : 3.692 | G Loss : 11.843\n",
            "Batch : 564, D Loss : 3.852 | G Loss : 11.411\n",
            "Batch : 565, D Loss : 3.926 | G Loss : 10.825\n",
            "Batch : 566, D Loss : 3.783 | G Loss : 11.019\n",
            "Batch : 567, D Loss : 4.148 | G Loss : 12.112\n",
            "Batch : 568, D Loss : 4.030 | G Loss : 10.775\n",
            "Batch : 569, D Loss : 3.912 | G Loss : 11.652\n",
            "Batch : 570, D Loss : 3.887 | G Loss : 11.710\n",
            "Batch : 571, D Loss : 3.806 | G Loss : 10.803\n",
            "Batch : 572, D Loss : 3.837 | G Loss : 10.622\n",
            "Batch : 573, D Loss : 3.982 | G Loss : 12.506\n",
            "Batch : 574, D Loss : 3.722 | G Loss : 11.343\n",
            "Batch : 575, D Loss : 3.813 | G Loss : 11.895\n",
            "Batch : 576, D Loss : 3.883 | G Loss : 11.257\n",
            "Batch : 577, D Loss : 3.917 | G Loss : 12.283\n",
            "Batch : 578, D Loss : 3.879 | G Loss : 10.751\n",
            "Batch : 579, D Loss : 3.942 | G Loss : 13.133\n",
            "Batch : 580, D Loss : 3.823 | G Loss : 11.963\n",
            "Batch : 581, D Loss : 3.933 | G Loss : 13.233\n",
            "Batch : 582, D Loss : 3.805 | G Loss : 11.816\n",
            "Batch : 583, D Loss : 3.812 | G Loss : 11.106\n",
            "Batch : 584, D Loss : 3.836 | G Loss : 12.133\n",
            "Batch : 585, D Loss : 3.830 | G Loss : 10.527\n",
            "Batch : 586, D Loss : 3.761 | G Loss : 11.298\n",
            "Batch : 587, D Loss : 3.854 | G Loss : 11.913\n",
            "Batch : 588, D Loss : 3.794 | G Loss : 10.727\n",
            "Batch : 589, D Loss : 3.889 | G Loss : 11.180\n",
            "Batch : 590, D Loss : 3.812 | G Loss : 11.238\n",
            "Batch : 591, D Loss : 3.803 | G Loss : 10.255\n",
            "Batch : 592, D Loss : 3.784 | G Loss : 11.707\n",
            "Batch : 593, D Loss : 3.862 | G Loss : 10.731\n",
            "Batch : 594, D Loss : 3.827 | G Loss : 10.459\n",
            "Batch : 595, D Loss : 3.781 | G Loss : 12.493\n",
            "Batch : 596, D Loss : 3.833 | G Loss : 10.454\n",
            "Batch : 597, D Loss : 3.772 | G Loss : 10.377\n",
            "Batch : 598, D Loss : 3.749 | G Loss : 10.449\n",
            "Batch : 599, D Loss : 3.839 | G Loss : 11.984\n",
            "Batch : 600, D Loss : 3.878 | G Loss : 11.719\n",
            "Batch : 601, D Loss : 3.831 | G Loss : 10.544\n",
            "Batch : 602, D Loss : 3.719 | G Loss : 11.653\n",
            "Batch : 603, D Loss : 3.995 | G Loss : 10.258\n",
            "Batch : 604, D Loss : 3.935 | G Loss : 10.574\n",
            "Batch : 605, D Loss : 3.766 | G Loss : 10.303\n",
            "Batch : 606, D Loss : 3.729 | G Loss : 11.508\n",
            "Batch : 607, D Loss : 3.794 | G Loss : 11.226\n",
            "Batch : 608, D Loss : 4.015 | G Loss : 10.762\n",
            "Batch : 609, D Loss : 3.954 | G Loss : 10.706\n",
            "Batch : 610, D Loss : 4.110 | G Loss : 11.063\n",
            "Batch : 611, D Loss : 3.935 | G Loss : 10.547\n",
            "Batch : 612, D Loss : 3.871 | G Loss : 10.202\n",
            "Batch : 613, D Loss : 3.821 | G Loss : 11.226\n",
            "Batch : 614, D Loss : 3.809 | G Loss : 11.321\n",
            "Batch : 615, D Loss : 4.125 | G Loss : 12.636\n",
            "Batch : 616, D Loss : 3.998 | G Loss : 10.318\n",
            "Batch : 617, D Loss : 3.946 | G Loss : 11.948\n",
            "Batch : 618, D Loss : 4.003 | G Loss : 11.391\n",
            "Batch : 619, D Loss : 3.867 | G Loss : 12.060\n",
            "Batch : 620, D Loss : 3.863 | G Loss : 9.934\n",
            "Batch : 621, D Loss : 3.748 | G Loss : 12.393\n",
            "Batch : 622, D Loss : 3.712 | G Loss : 11.434\n",
            "Batch : 623, D Loss : 3.808 | G Loss : 10.562\n",
            "Batch : 624, D Loss : 3.872 | G Loss : 11.221\n",
            "Batch : 625, D Loss : 3.849 | G Loss : 12.759\n",
            "Batch : 626, D Loss : 3.643 | G Loss : 10.424\n",
            "Batch : 627, D Loss : 3.748 | G Loss : 11.180\n",
            "Batch : 628, D Loss : 3.775 | G Loss : 10.536\n",
            "Batch : 629, D Loss : 3.786 | G Loss : 11.169\n",
            "Batch : 630, D Loss : 3.805 | G Loss : 10.829\n",
            "Batch : 631, D Loss : 3.800 | G Loss : 11.582\n",
            "Batch : 632, D Loss : 3.828 | G Loss : 11.131\n",
            "Batch : 633, D Loss : 3.769 | G Loss : 11.441\n",
            "Batch : 634, D Loss : 3.760 | G Loss : 11.578\n",
            "Batch : 635, D Loss : 3.847 | G Loss : 10.070\n",
            "Batch : 636, D Loss : 3.691 | G Loss : 10.249\n",
            "Batch : 637, D Loss : 3.882 | G Loss : 10.628\n",
            "Batch : 638, D Loss : 3.869 | G Loss : 11.504\n",
            "Batch : 639, D Loss : 3.821 | G Loss : 10.101\n",
            "Batch : 640, D Loss : 3.918 | G Loss : 10.336\n",
            "Batch : 641, D Loss : 3.722 | G Loss : 10.800\n",
            "Batch : 642, D Loss : 3.681 | G Loss : 11.378\n",
            "Batch : 643, D Loss : 3.786 | G Loss : 10.863\n",
            "Batch : 644, D Loss : 3.713 | G Loss : 11.176\n",
            "Batch : 645, D Loss : 3.743 | G Loss : 10.422\n",
            "Batch : 646, D Loss : 3.809 | G Loss : 12.358\n",
            "Batch : 647, D Loss : 3.805 | G Loss : 11.212\n",
            "Batch : 648, D Loss : 3.752 | G Loss : 9.829\n",
            "Batch : 649, D Loss : 3.928 | G Loss : 10.210\n",
            "Batch : 650, D Loss : 3.878 | G Loss : 11.451\n",
            "Batch : 651, D Loss : 3.847 | G Loss : 11.980\n",
            "Batch : 652, D Loss : 3.925 | G Loss : 10.897\n",
            "Batch : 653, D Loss : 3.788 | G Loss : 11.454\n",
            "Batch : 654, D Loss : 3.836 | G Loss : 10.653\n",
            "Batch : 655, D Loss : 3.797 | G Loss : 11.789\n",
            "Batch : 656, D Loss : 3.743 | G Loss : 10.982\n",
            "Batch : 657, D Loss : 3.770 | G Loss : 11.474\n",
            "Batch : 658, D Loss : 3.956 | G Loss : 10.045\n",
            "Batch : 659, D Loss : 3.818 | G Loss : 11.188\n",
            "Batch : 660, D Loss : 4.505 | G Loss : 11.263\n",
            "Batch : 661, D Loss : 4.205 | G Loss : 11.228\n",
            "Batch : 662, D Loss : 3.911 | G Loss : 11.684\n",
            "Batch : 663, D Loss : 3.676 | G Loss : 11.474\n",
            "Batch : 664, D Loss : 4.326 | G Loss : 12.092\n",
            "Batch : 665, D Loss : 4.184 | G Loss : 11.218\n",
            "Batch : 666, D Loss : 4.048 | G Loss : 11.115\n",
            "Batch : 667, D Loss : 3.836 | G Loss : 11.739\n",
            "Batch : 668, D Loss : 3.912 | G Loss : 10.868\n",
            "Batch : 669, D Loss : 3.930 | G Loss : 13.149\n",
            "Batch : 670, D Loss : 3.663 | G Loss : 11.173\n",
            "Batch : 671, D Loss : 3.748 | G Loss : 10.793\n",
            "Batch : 672, D Loss : 3.825 | G Loss : 9.813\n",
            "Batch : 673, D Loss : 3.845 | G Loss : 13.217\n",
            "Batch : 674, D Loss : 3.768 | G Loss : 10.906\n",
            "Batch : 675, D Loss : 3.767 | G Loss : 10.604\n",
            "Batch : 676, D Loss : 3.717 | G Loss : 11.460\n",
            "Batch : 677, D Loss : 3.685 | G Loss : 11.524\n",
            "Batch : 678, D Loss : 3.826 | G Loss : 12.024\n",
            "Batch : 679, D Loss : 3.744 | G Loss : 10.242\n",
            "Batch : 680, D Loss : 3.776 | G Loss : 12.172\n",
            "Batch : 681, D Loss : 3.754 | G Loss : 11.291\n",
            "Batch : 682, D Loss : 3.864 | G Loss : 10.255\n",
            "Batch : 683, D Loss : 3.808 | G Loss : 11.045\n",
            "Batch : 684, D Loss : 3.740 | G Loss : 9.281\n",
            "Batch : 685, D Loss : 3.643 | G Loss : 11.148\n",
            "Batch : 686, D Loss : 3.820 | G Loss : 11.847\n",
            "Batch : 687, D Loss : 3.758 | G Loss : 11.655\n",
            "Batch : 688, D Loss : 3.835 | G Loss : 11.518\n",
            "Batch : 689, D Loss : 3.910 | G Loss : 12.107\n",
            "Batch : 690, D Loss : 3.757 | G Loss : 11.457\n",
            "Batch : 691, D Loss : 3.765 | G Loss : 11.554\n",
            "Batch : 692, D Loss : 3.803 | G Loss : 11.679\n",
            "Batch : 693, D Loss : 3.756 | G Loss : 10.057\n",
            "Batch : 694, D Loss : 3.851 | G Loss : 10.986\n",
            "Batch : 695, D Loss : 3.815 | G Loss : 13.093\n",
            "Batch : 696, D Loss : 3.894 | G Loss : 11.032\n",
            "Batch : 697, D Loss : 3.827 | G Loss : 13.282\n",
            "Batch : 698, D Loss : 3.712 | G Loss : 10.642\n",
            "Batch : 699, D Loss : 3.718 | G Loss : 10.873\n",
            "Batch : 700, D Loss : 3.764 | G Loss : 9.821\n",
            "Batch : 701, D Loss : 3.802 | G Loss : 12.339\n",
            "Batch : 702, D Loss : 3.839 | G Loss : 11.157\n",
            "Batch : 703, D Loss : 3.765 | G Loss : 12.019\n",
            "Batch : 704, D Loss : 3.790 | G Loss : 11.618\n",
            "Batch : 705, D Loss : 3.742 | G Loss : 10.196\n",
            "Batch : 706, D Loss : 3.857 | G Loss : 12.092\n",
            "Batch : 707, D Loss : 3.712 | G Loss : 11.519\n",
            "Batch : 708, D Loss : 3.961 | G Loss : 11.194\n",
            "Batch : 709, D Loss : 3.892 | G Loss : 11.389\n",
            "Batch : 710, D Loss : 3.833 | G Loss : 10.723\n",
            "Batch : 711, D Loss : 3.827 | G Loss : 10.989\n",
            "Batch : 712, D Loss : 3.835 | G Loss : 11.031\n",
            "Batch : 713, D Loss : 3.814 | G Loss : 10.763\n",
            "Batch : 714, D Loss : 3.782 | G Loss : 10.324\n",
            "Batch : 715, D Loss : 3.804 | G Loss : 11.418\n",
            "Batch : 716, D Loss : 3.792 | G Loss : 10.983\n",
            "Batch : 717, D Loss : 3.863 | G Loss : 10.175\n",
            "Batch : 718, D Loss : 3.767 | G Loss : 9.825\n",
            "Batch : 719, D Loss : 3.819 | G Loss : 9.564\n",
            "Batch : 720, D Loss : 3.877 | G Loss : 10.602\n",
            "Batch : 721, D Loss : 3.708 | G Loss : 10.427\n",
            "Batch : 722, D Loss : 3.639 | G Loss : 11.316\n",
            "Batch : 723, D Loss : 4.421 | G Loss : 10.527\n",
            "Batch : 724, D Loss : 4.281 | G Loss : 9.926\n",
            "Batch : 725, D Loss : 3.879 | G Loss : 11.879\n",
            "Batch : 726, D Loss : 3.846 | G Loss : 11.740\n",
            "Batch : 727, D Loss : 3.918 | G Loss : 11.632\n",
            "Batch : 728, D Loss : 3.720 | G Loss : 10.107\n",
            "Batch : 729, D Loss : 3.837 | G Loss : 11.095\n",
            "Batch : 730, D Loss : 3.725 | G Loss : 10.939\n",
            "Batch : 731, D Loss : 3.840 | G Loss : 10.669\n",
            "Batch : 732, D Loss : 3.775 | G Loss : 10.506\n",
            "Batch : 733, D Loss : 3.843 | G Loss : 11.435\n",
            "Batch : 734, D Loss : 3.881 | G Loss : 11.046\n",
            "Batch : 735, D Loss : 3.773 | G Loss : 10.728\n",
            "Batch : 736, D Loss : 3.766 | G Loss : 12.073\n",
            "Batch : 737, D Loss : 3.765 | G Loss : 11.703\n",
            "Batch : 738, D Loss : 3.849 | G Loss : 10.914\n",
            "Batch : 739, D Loss : 3.847 | G Loss : 11.474\n",
            "Batch : 740, D Loss : 3.717 | G Loss : 10.619\n",
            "Batch : 741, D Loss : 3.864 | G Loss : 11.132\n",
            "Batch : 742, D Loss : 3.751 | G Loss : 10.425\n",
            "Batch : 743, D Loss : 3.819 | G Loss : 12.361\n",
            "Batch : 744, D Loss : 3.841 | G Loss : 10.734\n",
            "Batch : 745, D Loss : 3.782 | G Loss : 10.383\n",
            "Batch : 746, D Loss : 3.666 | G Loss : 10.549\n",
            "Batch : 747, D Loss : 3.741 | G Loss : 10.787\n",
            "Batch : 748, D Loss : 3.822 | G Loss : 11.026\n",
            "Batch : 749, D Loss : 3.838 | G Loss : 10.443\n",
            "Batch : 750, D Loss : 3.754 | G Loss : 9.861\n",
            "Batch : 751, D Loss : 3.742 | G Loss : 10.966\n",
            "Batch : 752, D Loss : 3.813 | G Loss : 10.916\n",
            "Batch : 753, D Loss : 3.697 | G Loss : 10.846\n",
            "Batch : 754, D Loss : 3.843 | G Loss : 9.898\n",
            "Batch : 755, D Loss : 3.746 | G Loss : 8.959\n",
            "Batch : 756, D Loss : 3.713 | G Loss : 10.775\n",
            "Batch : 757, D Loss : 3.779 | G Loss : 12.689\n",
            "Batch : 758, D Loss : 3.802 | G Loss : 10.483\n",
            "Batch : 759, D Loss : 3.724 | G Loss : 11.029\n",
            "Batch : 760, D Loss : 3.826 | G Loss : 11.393\n",
            "Batch : 761, D Loss : 3.796 | G Loss : 10.697\n",
            "Batch : 762, D Loss : 3.818 | G Loss : 9.952\n",
            "Batch : 763, D Loss : 3.703 | G Loss : 10.534\n",
            "Batch : 764, D Loss : 3.720 | G Loss : 9.518\n",
            "Batch : 765, D Loss : 3.780 | G Loss : 10.601\n",
            "Batch : 766, D Loss : 3.734 | G Loss : 11.385\n",
            "Batch : 767, D Loss : 3.916 | G Loss : 11.345\n",
            "Batch : 768, D Loss : 3.738 | G Loss : 10.461\n",
            "Batch : 769, D Loss : 3.740 | G Loss : 11.098\n",
            "Batch : 770, D Loss : 3.817 | G Loss : 9.690\n",
            "Batch : 771, D Loss : 3.828 | G Loss : 11.789\n",
            "Batch : 772, D Loss : 3.756 | G Loss : 10.406\n",
            "Batch : 773, D Loss : 3.724 | G Loss : 10.020\n",
            "Batch : 774, D Loss : 3.753 | G Loss : 10.417\n",
            "Batch : 775, D Loss : 3.804 | G Loss : 11.236\n",
            "Batch : 776, D Loss : 3.785 | G Loss : 10.049\n",
            "Batch : 777, D Loss : 3.774 | G Loss : 11.261\n",
            "Batch : 778, D Loss : 4.861 | G Loss : 10.795\n",
            "Batch : 779, D Loss : 4.239 | G Loss : 12.506\n",
            "Batch : 780, D Loss : 3.920 | G Loss : 10.808\n",
            "Batch : 781, D Loss : 4.321 | G Loss : 10.288\n",
            "Batch : 782, D Loss : 4.121 | G Loss : 9.872\n",
            "Batch : 783, D Loss : 3.908 | G Loss : 10.148\n",
            "Batch : 784, D Loss : 3.809 | G Loss : 10.652\n",
            "Batch : 785, D Loss : 3.785 | G Loss : 10.900\n",
            "Batch : 786, D Loss : 3.784 | G Loss : 11.720\n",
            "Batch : 787, D Loss : 3.837 | G Loss : 9.758\n",
            "Batch : 788, D Loss : 3.850 | G Loss : 10.597\n",
            "Batch : 789, D Loss : 3.721 | G Loss : 11.383\n",
            "Batch : 790, D Loss : 3.939 | G Loss : 10.291\n",
            "Batch : 791, D Loss : 3.856 | G Loss : 10.418\n",
            "Batch : 792, D Loss : 3.761 | G Loss : 10.197\n",
            "Batch : 793, D Loss : 3.770 | G Loss : 9.385\n",
            "Batch : 794, D Loss : 3.768 | G Loss : 11.238\n",
            "Batch : 795, D Loss : 3.842 | G Loss : 10.812\n",
            "Batch : 796, D Loss : 3.861 | G Loss : 10.429\n",
            "Batch : 797, D Loss : 3.799 | G Loss : 10.697\n",
            "Batch : 798, D Loss : 3.741 | G Loss : 10.297\n",
            "Batch : 799, D Loss : 3.829 | G Loss : 12.387\n",
            "Batch : 800, D Loss : 3.799 | G Loss : 10.896\n",
            "Batch : 801, D Loss : 3.743 | G Loss : 10.217\n",
            "Batch : 802, D Loss : 3.739 | G Loss : 11.580\n",
            "Batch : 803, D Loss : 3.823 | G Loss : 9.714\n",
            "Batch : 804, D Loss : 3.714 | G Loss : 9.996\n",
            "Batch : 805, D Loss : 3.750 | G Loss : 10.034\n",
            "Batch : 806, D Loss : 3.724 | G Loss : 11.277\n",
            "Batch : 807, D Loss : 3.817 | G Loss : 10.003\n",
            "Batch : 808, D Loss : 3.797 | G Loss : 11.502\n",
            "Batch : 809, D Loss : 3.747 | G Loss : 9.733\n",
            "Batch : 810, D Loss : 3.814 | G Loss : 10.919\n",
            "Batch : 811, D Loss : 3.767 | G Loss : 9.570\n",
            "Batch : 812, D Loss : 3.742 | G Loss : 11.073\n",
            "Batch : 813, D Loss : 3.806 | G Loss : 12.252\n",
            "Batch : 814, D Loss : 3.768 | G Loss : 10.217\n",
            "Batch : 815, D Loss : 3.776 | G Loss : 11.169\n",
            "Batch : 816, D Loss : 3.790 | G Loss : 11.422\n",
            "Batch : 817, D Loss : 3.875 | G Loss : 11.285\n",
            "Batch : 818, D Loss : 3.836 | G Loss : 10.405\n",
            "Batch : 819, D Loss : 3.814 | G Loss : 9.559\n",
            "Batch : 820, D Loss : 3.791 | G Loss : 11.133\n",
            "Batch : 821, D Loss : 3.834 | G Loss : 11.037\n",
            "Batch : 822, D Loss : 3.774 | G Loss : 11.157\n",
            "Batch : 823, D Loss : 3.784 | G Loss : 10.215\n",
            "Batch : 824, D Loss : 3.675 | G Loss : 10.136\n",
            "Batch : 825, D Loss : 3.858 | G Loss : 10.470\n",
            "Batch : 826, D Loss : 3.756 | G Loss : 9.896\n",
            "Batch : 827, D Loss : 3.895 | G Loss : 10.369\n",
            "Batch : 828, D Loss : 3.727 | G Loss : 10.313\n",
            "Batch : 829, D Loss : 3.661 | G Loss : 9.512\n",
            "Batch : 830, D Loss : 3.689 | G Loss : 10.777\n",
            "Batch : 831, D Loss : 3.713 | G Loss : 9.919\n",
            "Batch : 832, D Loss : 3.802 | G Loss : 10.234\n",
            "Batch : 833, D Loss : 3.855 | G Loss : 12.830\n",
            "Batch : 834, D Loss : 3.800 | G Loss : 10.974\n",
            "Batch : 835, D Loss : 3.686 | G Loss : 9.908\n",
            "Batch : 836, D Loss : 3.756 | G Loss : 10.051\n",
            "Batch : 837, D Loss : 4.667 | G Loss : 11.208\n",
            "Batch : 838, D Loss : 4.492 | G Loss : 12.143\n",
            "Batch : 839, D Loss : 4.129 | G Loss : 10.586\n",
            "Batch : 840, D Loss : 4.251 | G Loss : 11.734\n",
            "Batch : 841, D Loss : 4.025 | G Loss : 13.003\n",
            "Batch : 842, D Loss : 3.924 | G Loss : 12.731\n",
            "Batch : 843, D Loss : 3.821 | G Loss : 11.115\n",
            "Batch : 844, D Loss : 3.877 | G Loss : 10.304\n",
            "Batch : 845, D Loss : 3.697 | G Loss : 10.915\n",
            "Batch : 846, D Loss : 3.839 | G Loss : 11.203\n",
            "Batch : 847, D Loss : 3.861 | G Loss : 10.011\n",
            "Batch : 848, D Loss : 3.791 | G Loss : 10.200\n",
            "Batch : 849, D Loss : 3.842 | G Loss : 11.335\n",
            "Batch : 850, D Loss : 3.825 | G Loss : 11.200\n",
            "Batch : 851, D Loss : 3.938 | G Loss : 9.732\n",
            "Batch : 852, D Loss : 3.847 | G Loss : 10.953\n",
            "Batch : 853, D Loss : 3.720 | G Loss : 10.702\n",
            "Batch : 854, D Loss : 3.678 | G Loss : 9.701\n",
            "Batch : 855, D Loss : 3.949 | G Loss : 10.748\n",
            "Batch : 856, D Loss : 3.813 | G Loss : 9.917\n",
            "Batch : 857, D Loss : 3.807 | G Loss : 12.636\n",
            "Batch : 858, D Loss : 3.787 | G Loss : 10.648\n",
            "Batch : 859, D Loss : 3.850 | G Loss : 12.061\n",
            "Batch : 860, D Loss : 3.838 | G Loss : 12.192\n",
            "Batch : 861, D Loss : 3.808 | G Loss : 10.344\n",
            "Batch : 862, D Loss : 3.749 | G Loss : 9.581\n",
            "Batch : 863, D Loss : 3.844 | G Loss : 12.966\n",
            "Batch : 864, D Loss : 3.919 | G Loss : 10.221\n",
            "Batch : 865, D Loss : 3.762 | G Loss : 11.130\n",
            "Batch : 866, D Loss : 3.771 | G Loss : 11.326\n",
            "Batch : 867, D Loss : 3.870 | G Loss : 10.036\n",
            "Batch : 868, D Loss : 3.718 | G Loss : 10.393\n",
            "Batch : 869, D Loss : 3.723 | G Loss : 9.045\n",
            "Batch : 870, D Loss : 3.797 | G Loss : 11.122\n",
            "Batch : 871, D Loss : 3.862 | G Loss : 10.291\n",
            "Batch : 872, D Loss : 3.839 | G Loss : 9.936\n",
            "Batch : 873, D Loss : 3.677 | G Loss : 9.575\n",
            "Batch : 874, D Loss : 3.706 | G Loss : 10.251\n",
            "Batch : 875, D Loss : 3.760 | G Loss : 11.556\n",
            "Batch : 876, D Loss : 3.855 | G Loss : 10.147\n",
            "Batch : 877, D Loss : 3.823 | G Loss : 10.020\n",
            "Batch : 878, D Loss : 3.891 | G Loss : 12.552\n",
            "Batch : 879, D Loss : 3.778 | G Loss : 10.646\n",
            "Batch : 880, D Loss : 3.889 | G Loss : 12.221\n",
            "Batch : 881, D Loss : 3.839 | G Loss : 9.795\n",
            "Batch : 882, D Loss : 3.829 | G Loss : 11.101\n",
            "Batch : 883, D Loss : 3.786 | G Loss : 11.313\n",
            "Batch : 884, D Loss : 3.678 | G Loss : 10.792\n",
            "Batch : 885, D Loss : 3.842 | G Loss : 10.429\n",
            "Batch : 886, D Loss : 3.957 | G Loss : 10.485\n",
            "Batch : 887, D Loss : 3.958 | G Loss : 12.426\n",
            "Batch : 888, D Loss : 3.875 | G Loss : 10.360\n",
            "Batch : 889, D Loss : 3.778 | G Loss : 9.326\n",
            "Batch : 890, D Loss : 3.838 | G Loss : 10.399\n",
            "Batch : 891, D Loss : 3.805 | G Loss : 11.840\n",
            "Batch : 892, D Loss : 3.877 | G Loss : 10.336\n",
            "Batch : 893, D Loss : 4.160 | G Loss : 10.732\n",
            "Batch : 894, D Loss : 4.130 | G Loss : 11.606\n",
            "Batch : 895, D Loss : 4.119 | G Loss : 11.004\n",
            "Batch : 896, D Loss : 4.011 | G Loss : 10.573\n",
            "Batch : 897, D Loss : 3.866 | G Loss : 10.914\n",
            "Batch : 898, D Loss : 3.804 | G Loss : 10.785\n",
            "Batch : 899, D Loss : 3.848 | G Loss : 9.779\n",
            "Batch : 900, D Loss : 3.803 | G Loss : 9.241\n",
            "Batch : 901, D Loss : 3.662 | G Loss : 10.500\n",
            "Batch : 902, D Loss : 3.936 | G Loss : 10.200\n",
            "Batch : 903, D Loss : 3.899 | G Loss : 10.911\n",
            "Batch : 904, D Loss : 3.719 | G Loss : 10.533\n",
            "Batch : 905, D Loss : 3.728 | G Loss : 10.031\n",
            "Batch : 906, D Loss : 4.506 | G Loss : 12.383\n",
            "Batch : 907, D Loss : 4.277 | G Loss : 11.084\n",
            "Batch : 908, D Loss : 4.016 | G Loss : 11.106\n",
            "Batch : 909, D Loss : 3.890 | G Loss : 10.765\n",
            "Batch : 910, D Loss : 3.931 | G Loss : 9.850\n",
            "Batch : 911, D Loss : 3.896 | G Loss : 10.332\n",
            "Batch : 912, D Loss : 3.863 | G Loss : 10.449\n",
            "Batch : 913, D Loss : 3.732 | G Loss : 10.787\n",
            "Batch : 914, D Loss : 3.799 | G Loss : 11.379\n",
            "Batch : 915, D Loss : 3.887 | G Loss : 10.669\n",
            "Batch : 916, D Loss : 3.889 | G Loss : 10.068\n",
            "Batch : 917, D Loss : 3.844 | G Loss : 9.913\n",
            "Batch : 918, D Loss : 3.779 | G Loss : 10.345\n",
            "Batch : 919, D Loss : 3.944 | G Loss : 10.954\n",
            "Batch : 920, D Loss : 3.883 | G Loss : 11.450\n",
            "Batch : 921, D Loss : 3.796 | G Loss : 8.850\n",
            "Batch : 922, D Loss : 3.699 | G Loss : 9.859\n",
            "Batch : 923, D Loss : 3.884 | G Loss : 11.110\n",
            "Batch : 924, D Loss : 3.863 | G Loss : 9.396\n",
            "Batch : 925, D Loss : 3.752 | G Loss : 9.990\n",
            "Batch : 926, D Loss : 3.813 | G Loss : 10.804\n",
            "Batch : 927, D Loss : 3.822 | G Loss : 10.397\n",
            "Batch : 928, D Loss : 3.816 | G Loss : 10.742\n",
            "Batch : 929, D Loss : 3.776 | G Loss : 10.228\n",
            "Batch : 930, D Loss : 3.732 | G Loss : 11.223\n",
            "Batch : 931, D Loss : 3.729 | G Loss : 9.469\n",
            "Batch : 932, D Loss : 3.963 | G Loss : 10.350\n",
            "Batch : 933, D Loss : 3.762 | G Loss : 12.670\n",
            "Batch : 934, D Loss : 3.762 | G Loss : 9.714\n",
            "Batch : 935, D Loss : 3.799 | G Loss : 9.378\n",
            "Batch : 936, D Loss : 3.827 | G Loss : 10.508\n",
            "Batch : 937, D Loss : 3.861 | G Loss : 10.543\n",
            "Batch : 938, D Loss : 3.719 | G Loss : 10.791\n",
            "Batch : 939, D Loss : 3.765 | G Loss : 10.831\n",
            "Batch : 940, D Loss : 3.790 | G Loss : 12.406\n",
            "Batch : 941, D Loss : 3.816 | G Loss : 10.322\n",
            "Batch : 942, D Loss : 3.839 | G Loss : 11.437\n",
            "Batch : 943, D Loss : 4.043 | G Loss : 10.499\n",
            "Batch : 944, D Loss : 3.850 | G Loss : 9.704\n",
            "Batch : 945, D Loss : 3.738 | G Loss : 8.784\n",
            "Batch : 946, D Loss : 3.733 | G Loss : 11.359\n",
            "Batch : 947, D Loss : 3.819 | G Loss : 9.291\n",
            "Batch : 948, D Loss : 3.767 | G Loss : 10.310\n",
            "Batch : 949, D Loss : 3.732 | G Loss : 10.393\n",
            "Batch : 950, D Loss : 3.810 | G Loss : 9.339\n",
            "Batch : 951, D Loss : 3.891 | G Loss : 10.792\n",
            "Batch : 952, D Loss : 3.982 | G Loss : 9.695\n",
            "Batch : 953, D Loss : 3.795 | G Loss : 10.532\n",
            "Batch : 954, D Loss : 3.736 | G Loss : 11.279\n",
            "Batch : 955, D Loss : 3.859 | G Loss : 10.439\n",
            "Batch : 956, D Loss : 3.727 | G Loss : 9.500\n",
            "Batch : 957, D Loss : 3.785 | G Loss : 9.769\n",
            "Batch : 958, D Loss : 3.904 | G Loss : 10.910\n",
            "Batch : 959, D Loss : 3.831 | G Loss : 8.936\n",
            "Batch : 960, D Loss : 3.751 | G Loss : 10.799\n",
            "Batch : 961, D Loss : 3.745 | G Loss : 10.287\n",
            "Batch : 962, D Loss : 3.882 | G Loss : 10.784\n",
            "Batch : 963, D Loss : 3.796 | G Loss : 10.321\n",
            "Batch : 964, D Loss : 3.813 | G Loss : 9.579\n",
            "Batch : 965, D Loss : 3.675 | G Loss : 9.620\n",
            "Batch : 966, D Loss : 3.842 | G Loss : 10.358\n",
            "Batch : 967, D Loss : 3.887 | G Loss : 12.241\n",
            "Batch : 968, D Loss : 3.823 | G Loss : 10.900\n",
            "Batch : 969, D Loss : 3.710 | G Loss : 10.436\n",
            "Batch : 970, D Loss : 3.830 | G Loss : 11.118\n",
            "Batch : 971, D Loss : 3.736 | G Loss : 10.596\n",
            "Batch : 972, D Loss : 3.847 | G Loss : 11.184\n",
            "Batch : 973, D Loss : 3.878 | G Loss : 9.577\n",
            "Batch : 974, D Loss : 3.864 | G Loss : 11.061\n",
            "Batch : 975, D Loss : 3.830 | G Loss : 10.936\n",
            "Batch : 976, D Loss : 3.754 | G Loss : 10.242\n",
            "Batch : 977, D Loss : 3.818 | G Loss : 9.656\n",
            "Batch : 978, D Loss : 4.032 | G Loss : 10.650\n",
            "Batch : 979, D Loss : 3.974 | G Loss : 10.283\n",
            "Batch : 980, D Loss : 3.953 | G Loss : 11.314\n",
            "Batch : 981, D Loss : 3.809 | G Loss : 10.355\n",
            "Batch : 982, D Loss : 3.918 | G Loss : 10.698\n",
            "Batch : 983, D Loss : 3.921 | G Loss : 10.151\n",
            "Batch : 984, D Loss : 3.799 | G Loss : 9.422\n",
            "Batch : 985, D Loss : 3.906 | G Loss : 9.767\n",
            "Batch : 986, D Loss : 3.728 | G Loss : 9.742\n",
            "Batch : 987, D Loss : 3.703 | G Loss : 10.543\n",
            "Batch : 988, D Loss : 3.698 | G Loss : 9.777\n",
            "Batch : 989, D Loss : 3.785 | G Loss : 9.619\n",
            "Batch : 990, D Loss : 3.868 | G Loss : 11.400\n",
            "Batch : 991, D Loss : 3.806 | G Loss : 10.637\n",
            "Batch : 992, D Loss : 3.875 | G Loss : 9.275\n",
            "Batch : 993, D Loss : 3.884 | G Loss : 11.926\n",
            "Batch : 994, D Loss : 3.986 | G Loss : 11.435\n",
            "Batch : 995, D Loss : 3.927 | G Loss : 10.528\n",
            "Batch : 996, D Loss : 3.809 | G Loss : 9.638\n",
            "Batch : 997, D Loss : 3.826 | G Loss : 10.497\n",
            "Batch : 998, D Loss : 3.718 | G Loss : 9.976\n",
            "Batch : 999, D Loss : 3.781 | G Loss : 12.540\n",
            "Batch : 1000, D Loss : 3.892 | G Loss : 10.027\n",
            "Batch : 1001, D Loss : 3.709 | G Loss : 11.188\n",
            "Batch : 1002, D Loss : 3.846 | G Loss : 9.905\n",
            "Batch : 1003, D Loss : 3.844 | G Loss : 10.350\n",
            "Batch : 1004, D Loss : 3.778 | G Loss : 10.193\n",
            "Batch : 1005, D Loss : 3.772 | G Loss : 9.211\n",
            "Batch : 1006, D Loss : 3.765 | G Loss : 10.261\n",
            "Batch : 1007, D Loss : 3.792 | G Loss : 10.239\n",
            "Batch : 1008, D Loss : 3.735 | G Loss : 10.100\n",
            "Batch : 1009, D Loss : 3.694 | G Loss : 10.362\n",
            "Batch : 1010, D Loss : 3.831 | G Loss : 9.252\n",
            "Batch : 1011, D Loss : 3.820 | G Loss : 11.765\n",
            "Batch : 1012, D Loss : 3.783 | G Loss : 9.420\n",
            "Batch : 1013, D Loss : 3.706 | G Loss : 9.680\n",
            "Batch : 1014, D Loss : 3.992 | G Loss : 10.346\n",
            "Batch : 1015, D Loss : 3.871 | G Loss : 9.786\n",
            "Batch : 1016, D Loss : 3.625 | G Loss : 11.345\n",
            "Batch : 1017, D Loss : 4.023 | G Loss : 9.284\n",
            "Batch : 1018, D Loss : 3.925 | G Loss : 10.588\n",
            "Batch : 1019, D Loss : 3.774 | G Loss : 12.192\n",
            "Batch : 1020, D Loss : 3.833 | G Loss : 9.654\n",
            "Batch : 1021, D Loss : 4.851 | G Loss : 11.552\n",
            "Batch : 1022, D Loss : 4.542 | G Loss : 11.234\n",
            "Batch : 1023, D Loss : 4.161 | G Loss : 11.949\n",
            "Batch : 1024, D Loss : 3.888 | G Loss : 10.003\n",
            "Batch : 1025, D Loss : 4.036 | G Loss : 10.200\n",
            "Batch : 1026, D Loss : 3.824 | G Loss : 9.876\n",
            "Batch : 1027, D Loss : 3.837 | G Loss : 8.874\n",
            "Batch : 1028, D Loss : 3.709 | G Loss : 9.623\n",
            "Batch : 1029, D Loss : 3.850 | G Loss : 10.807\n",
            "Batch : 1030, D Loss : 3.812 | G Loss : 10.764\n",
            "Batch : 1031, D Loss : 3.800 | G Loss : 9.911\n",
            "Batch : 1032, D Loss : 3.882 | G Loss : 10.330\n",
            "Batch : 1033, D Loss : 3.839 | G Loss : 10.497\n",
            "Batch : 1034, D Loss : 3.780 | G Loss : 10.445\n",
            "Batch : 1035, D Loss : 3.806 | G Loss : 9.504\n",
            "Batch : 1036, D Loss : 3.766 | G Loss : 10.343\n",
            "Batch : 1037, D Loss : 3.758 | G Loss : 9.812\n",
            "Batch : 1038, D Loss : 3.843 | G Loss : 9.403\n",
            "Batch : 1039, D Loss : 3.781 | G Loss : 9.845\n",
            "Batch : 1040, D Loss : 3.802 | G Loss : 10.677\n",
            "Batch : 1041, D Loss : 3.790 | G Loss : 10.102\n",
            "Batch : 1042, D Loss : 3.817 | G Loss : 10.215\n",
            "Batch : 1043, D Loss : 3.772 | G Loss : 9.799\n",
            "Batch : 1044, D Loss : 3.805 | G Loss : 9.620\n",
            "Batch : 1045, D Loss : 3.848 | G Loss : 10.926\n",
            "Batch : 1046, D Loss : 3.891 | G Loss : 11.157\n",
            "Batch : 1047, D Loss : 3.803 | G Loss : 11.932\n",
            "Batch : 1048, D Loss : 3.794 | G Loss : 9.726\n",
            "Batch : 1049, D Loss : 3.805 | G Loss : 9.462\n",
            "Batch : 1050, D Loss : 3.781 | G Loss : 9.512\n",
            "Batch : 1051, D Loss : 3.813 | G Loss : 10.136\n",
            "Batch : 1052, D Loss : 3.827 | G Loss : 9.286\n",
            "Batch : 1053, D Loss : 3.852 | G Loss : 9.953\n",
            "Batch : 1054, D Loss : 3.815 | G Loss : 10.259\n",
            "Batch : 1055, D Loss : 3.804 | G Loss : 11.588\n",
            "Batch : 1056, D Loss : 3.732 | G Loss : 9.561\n",
            "Batch : 1057, D Loss : 3.756 | G Loss : 8.992\n",
            "Batch : 1058, D Loss : 3.734 | G Loss : 11.575\n",
            "Batch : 1059, D Loss : 4.057 | G Loss : 12.308\n",
            "Batch : 1060, D Loss : 4.011 | G Loss : 11.756\n",
            "Batch : 1061, D Loss : 3.758 | G Loss : 10.573\n",
            "Batch : 1062, D Loss : 3.677 | G Loss : 10.373\n",
            "Batch : 1063, D Loss : 5.012 | G Loss : 12.063\n",
            "Batch : 1064, D Loss : 4.460 | G Loss : 10.138\n",
            "Batch : 1065, D Loss : 3.823 | G Loss : 11.377\n",
            "Batch : 1066, D Loss : 4.745 | G Loss : 13.482\n",
            "Batch : 1067, D Loss : 4.532 | G Loss : 10.758\n",
            "Batch : 1068, D Loss : 4.015 | G Loss : 11.099\n",
            "Batch : 1069, D Loss : 3.843 | G Loss : 10.663\n",
            "Batch : 1070, D Loss : 3.930 | G Loss : 9.648\n",
            "Batch : 1071, D Loss : 4.002 | G Loss : 12.277\n",
            "Batch : 1072, D Loss : 4.871 | G Loss : 11.234\n",
            "Batch : 1073, D Loss : 4.723 | G Loss : 11.836\n",
            "Batch : 1074, D Loss : 4.207 | G Loss : 10.155\n",
            "Batch : 1075, D Loss : 3.965 | G Loss : 11.967\n",
            "Batch : 1076, D Loss : 4.202 | G Loss : 10.500\n",
            "Batch : 1077, D Loss : 3.996 | G Loss : 11.226\n",
            "Batch : 1078, D Loss : 3.848 | G Loss : 9.631\n",
            "Batch : 1079, D Loss : 3.775 | G Loss : 9.974\n",
            "Batch : 1080, D Loss : 3.751 | G Loss : 11.957\n",
            "Batch : 1081, D Loss : 3.916 | G Loss : 9.972\n",
            "Batch : 1082, D Loss : 3.996 | G Loss : 11.991\n",
            "Batch : 1083, D Loss : 3.848 | G Loss : 10.579\n",
            "Batch : 1084, D Loss : 3.771 | G Loss : 10.973\n",
            "Batch : 1085, D Loss : 3.902 | G Loss : 9.872\n",
            "Batch : 1086, D Loss : 3.872 | G Loss : 10.251\n",
            "Batch : 1087, D Loss : 3.802 | G Loss : 9.253\n",
            "Batch : 1088, D Loss : 3.772 | G Loss : 11.502\n",
            "Batch : 1089, D Loss : 4.030 | G Loss : 11.909\n",
            "Batch : 1090, D Loss : 4.011 | G Loss : 9.865\n",
            "Batch : 1091, D Loss : 3.862 | G Loss : 10.200\n",
            "Batch : 1092, D Loss : 3.807 | G Loss : 11.928\n",
            "Batch : 1093, D Loss : 3.924 | G Loss : 10.129\n",
            "Batch : 1094, D Loss : 3.901 | G Loss : 10.221\n",
            "Batch : 1095, D Loss : 3.774 | G Loss : 9.364\n",
            "Batch : 1096, D Loss : 3.901 | G Loss : 10.700\n",
            "Batch : 1097, D Loss : 3.847 | G Loss : 10.883\n",
            "Batch : 1098, D Loss : 3.756 | G Loss : 10.116\n",
            "Batch : 1099, D Loss : 3.799 | G Loss : 9.488\n",
            "Batch : 1100, D Loss : 3.818 | G Loss : 9.728\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            ">Saved: plot_000002.png and g_model & d_model\n",
            " ========== Epoch 3 ========== \n",
            "Batch : 1, D Loss : 3.731 | G Loss : 11.050\n",
            "Batch : 2, D Loss : 5.167 | G Loss : 13.732\n",
            "Batch : 3, D Loss : 5.842 | G Loss : 12.542\n",
            "Batch : 4, D Loss : 5.162 | G Loss : 13.539\n",
            "Batch : 5, D Loss : 4.547 | G Loss : 13.564\n",
            "Batch : 6, D Loss : 4.274 | G Loss : 12.021\n",
            "Batch : 7, D Loss : 4.081 | G Loss : 10.652\n",
            "Batch : 8, D Loss : 4.083 | G Loss : 10.984\n",
            "Batch : 9, D Loss : 4.040 | G Loss : 12.803\n",
            "Batch : 10, D Loss : 4.080 | G Loss : 11.145\n",
            "Batch : 11, D Loss : 4.162 | G Loss : 12.173\n",
            "Batch : 12, D Loss : 4.186 | G Loss : 11.092\n",
            "Batch : 13, D Loss : 3.966 | G Loss : 9.655\n",
            "Batch : 14, D Loss : 4.012 | G Loss : 12.884\n",
            "Batch : 15, D Loss : 4.132 | G Loss : 11.311\n",
            "Batch : 16, D Loss : 4.325 | G Loss : 10.447\n",
            "Batch : 17, D Loss : 4.263 | G Loss : 11.026\n",
            "Batch : 18, D Loss : 4.234 | G Loss : 10.869\n",
            "Batch : 19, D Loss : 3.956 | G Loss : 10.994\n",
            "Batch : 20, D Loss : 3.919 | G Loss : 10.389\n",
            "Batch : 21, D Loss : 3.797 | G Loss : 11.879\n",
            "Batch : 22, D Loss : 3.887 | G Loss : 11.379\n",
            "Batch : 23, D Loss : 4.056 | G Loss : 10.608\n",
            "Batch : 24, D Loss : 3.987 | G Loss : 10.920\n",
            "Batch : 25, D Loss : 3.924 | G Loss : 10.060\n",
            "Batch : 26, D Loss : 3.821 | G Loss : 10.875\n",
            "Batch : 27, D Loss : 3.818 | G Loss : 10.732\n",
            "Batch : 28, D Loss : 3.785 | G Loss : 10.405\n",
            "Batch : 29, D Loss : 4.054 | G Loss : 10.431\n",
            "Batch : 30, D Loss : 3.852 | G Loss : 10.685\n",
            "Batch : 31, D Loss : 3.961 | G Loss : 10.701\n",
            "Batch : 32, D Loss : 4.040 | G Loss : 10.527\n",
            "Batch : 33, D Loss : 3.885 | G Loss : 10.057\n",
            "Batch : 34, D Loss : 3.920 | G Loss : 10.050\n",
            "Batch : 35, D Loss : 3.943 | G Loss : 9.907\n",
            "Batch : 36, D Loss : 3.957 | G Loss : 9.486\n",
            "Batch : 37, D Loss : 3.950 | G Loss : 11.212\n",
            "Batch : 38, D Loss : 3.933 | G Loss : 10.481\n",
            "Batch : 39, D Loss : 3.947 | G Loss : 11.199\n",
            "Batch : 40, D Loss : 3.855 | G Loss : 11.080\n",
            "Batch : 41, D Loss : 3.913 | G Loss : 9.893\n",
            "Batch : 42, D Loss : 3.947 | G Loss : 10.460\n",
            "Batch : 43, D Loss : 3.845 | G Loss : 8.867\n",
            "Batch : 44, D Loss : 3.829 | G Loss : 11.385\n",
            "Batch : 45, D Loss : 4.034 | G Loss : 9.821\n",
            "Batch : 46, D Loss : 3.962 | G Loss : 10.787\n",
            "Batch : 47, D Loss : 3.847 | G Loss : 10.886\n",
            "Batch : 48, D Loss : 3.812 | G Loss : 11.862\n",
            "Batch : 49, D Loss : 3.877 | G Loss : 11.152\n",
            "Batch : 50, D Loss : 3.961 | G Loss : 9.452\n",
            "Batch : 51, D Loss : 4.034 | G Loss : 12.006\n",
            "Batch : 52, D Loss : 3.897 | G Loss : 11.946\n",
            "Batch : 53, D Loss : 3.858 | G Loss : 10.934\n",
            "Batch : 54, D Loss : 3.793 | G Loss : 8.896\n",
            "Batch : 55, D Loss : 3.793 | G Loss : 9.975\n",
            "Batch : 56, D Loss : 4.273 | G Loss : 11.146\n",
            "Batch : 57, D Loss : 3.939 | G Loss : 10.339\n",
            "Batch : 58, D Loss : 4.187 | G Loss : 10.262\n",
            "Batch : 59, D Loss : 4.039 | G Loss : 9.785\n",
            "Batch : 60, D Loss : 3.877 | G Loss : 11.071\n",
            "Batch : 61, D Loss : 3.931 | G Loss : 10.670\n",
            "Batch : 62, D Loss : 3.788 | G Loss : 9.148\n",
            "Batch : 63, D Loss : 3.851 | G Loss : 10.819\n",
            "Batch : 64, D Loss : 3.876 | G Loss : 11.620\n",
            "Batch : 65, D Loss : 4.081 | G Loss : 10.893\n",
            "Batch : 66, D Loss : 3.846 | G Loss : 8.920\n",
            "Batch : 67, D Loss : 3.752 | G Loss : 9.437\n",
            "Batch : 68, D Loss : 3.904 | G Loss : 10.274\n",
            "Batch : 69, D Loss : 3.854 | G Loss : 9.752\n",
            "Batch : 70, D Loss : 3.918 | G Loss : 10.357\n",
            "Batch : 71, D Loss : 3.722 | G Loss : 10.661\n",
            "Batch : 72, D Loss : 4.069 | G Loss : 10.502\n",
            "Batch : 73, D Loss : 3.895 | G Loss : 9.865\n",
            "Batch : 74, D Loss : 3.902 | G Loss : 10.377\n",
            "Batch : 75, D Loss : 3.904 | G Loss : 9.590\n",
            "Batch : 76, D Loss : 3.773 | G Loss : 9.801\n",
            "Batch : 77, D Loss : 3.724 | G Loss : 9.719\n",
            "Batch : 78, D Loss : 3.753 | G Loss : 10.594\n",
            "Batch : 79, D Loss : 3.853 | G Loss : 10.143\n",
            "Batch : 80, D Loss : 3.844 | G Loss : 9.789\n",
            "Batch : 81, D Loss : 3.837 | G Loss : 11.365\n",
            "Batch : 82, D Loss : 3.974 | G Loss : 10.204\n",
            "Batch : 83, D Loss : 3.890 | G Loss : 9.392\n",
            "Batch : 84, D Loss : 3.883 | G Loss : 9.607\n",
            "Batch : 85, D Loss : 3.786 | G Loss : 10.862\n",
            "Batch : 86, D Loss : 6.428 | G Loss : 16.114\n",
            "Batch : 87, D Loss : 5.490 | G Loss : 17.243\n",
            "Batch : 88, D Loss : 5.245 | G Loss : 15.157\n",
            "Batch : 89, D Loss : 6.148 | G Loss : 16.703\n",
            "Batch : 90, D Loss : 4.274 | G Loss : 15.616\n",
            "Batch : 91, D Loss : 6.667 | G Loss : 16.161\n",
            "Batch : 92, D Loss : 6.783 | G Loss : 17.552\n",
            "Batch : 93, D Loss : 6.538 | G Loss : 15.709\n",
            "Batch : 94, D Loss : 6.007 | G Loss : 18.166\n",
            "Batch : 95, D Loss : 5.415 | G Loss : 15.431\n",
            "Batch : 96, D Loss : 5.011 | G Loss : 15.953\n",
            "Batch : 97, D Loss : 4.866 | G Loss : 12.637\n",
            "Batch : 98, D Loss : 3.958 | G Loss : 14.663\n",
            "Batch : 99, D Loss : 6.275 | G Loss : 15.714\n",
            "Batch : 100, D Loss : 6.514 | G Loss : 14.882\n",
            "Batch : 101, D Loss : 6.576 | G Loss : 16.388\n",
            "Batch : 102, D Loss : 6.097 | G Loss : 14.527\n",
            "Batch : 103, D Loss : 5.867 | G Loss : 14.871\n",
            "Batch : 104, D Loss : 5.384 | G Loss : 15.541\n",
            "Batch : 105, D Loss : 5.452 | G Loss : 14.730\n",
            "Batch : 106, D Loss : 5.547 | G Loss : 13.832\n",
            "Batch : 107, D Loss : 5.240 | G Loss : 13.006\n",
            "Batch : 108, D Loss : 4.824 | G Loss : 12.597\n",
            "Batch : 109, D Loss : 4.339 | G Loss : 13.435\n",
            "Batch : 110, D Loss : 4.367 | G Loss : 12.697\n",
            "Batch : 111, D Loss : 4.303 | G Loss : 11.867\n",
            "Batch : 112, D Loss : 3.921 | G Loss : 11.196\n",
            "Batch : 113, D Loss : 4.168 | G Loss : 10.910\n",
            "Batch : 114, D Loss : 4.119 | G Loss : 11.888\n",
            "Batch : 115, D Loss : 4.302 | G Loss : 10.156\n",
            "Batch : 116, D Loss : 4.160 | G Loss : 11.107\n",
            "Batch : 117, D Loss : 4.092 | G Loss : 12.162\n",
            "Batch : 118, D Loss : 4.036 | G Loss : 10.014\n",
            "Batch : 119, D Loss : 3.969 | G Loss : 10.941\n",
            "Batch : 120, D Loss : 3.966 | G Loss : 12.405\n",
            "Batch : 121, D Loss : 3.936 | G Loss : 10.654\n",
            "Batch : 122, D Loss : 3.988 | G Loss : 10.524\n",
            "Batch : 123, D Loss : 3.906 | G Loss : 9.330\n",
            "Batch : 124, D Loss : 3.887 | G Loss : 11.499\n",
            "Batch : 125, D Loss : 5.586 | G Loss : 12.411\n",
            "Batch : 126, D Loss : 5.585 | G Loss : 12.516\n",
            "Batch : 127, D Loss : 4.879 | G Loss : 14.061\n",
            "Batch : 128, D Loss : 4.331 | G Loss : 12.094\n",
            "Batch : 129, D Loss : 4.225 | G Loss : 11.303\n",
            "Batch : 130, D Loss : 4.106 | G Loss : 10.852\n",
            "Batch : 131, D Loss : 4.194 | G Loss : 11.587\n",
            "Batch : 132, D Loss : 3.949 | G Loss : 12.339\n",
            "Batch : 133, D Loss : 4.406 | G Loss : 12.106\n",
            "Batch : 134, D Loss : 4.367 | G Loss : 11.001\n",
            "Batch : 135, D Loss : 4.045 | G Loss : 12.133\n",
            "Batch : 136, D Loss : 4.042 | G Loss : 12.161\n",
            "Batch : 137, D Loss : 3.868 | G Loss : 13.341\n",
            "Batch : 138, D Loss : 4.392 | G Loss : 11.070\n",
            "Batch : 139, D Loss : 4.297 | G Loss : 12.886\n",
            "Batch : 140, D Loss : 4.158 | G Loss : 11.366\n",
            "Batch : 141, D Loss : 3.819 | G Loss : 11.011\n",
            "Batch : 142, D Loss : 3.981 | G Loss : 11.404\n",
            "Batch : 143, D Loss : 3.962 | G Loss : 11.866\n",
            "Batch : 144, D Loss : 4.038 | G Loss : 10.204\n",
            "Batch : 145, D Loss : 3.896 | G Loss : 12.197\n",
            "Batch : 146, D Loss : 4.389 | G Loss : 12.487\n",
            "Batch : 147, D Loss : 4.574 | G Loss : 12.692\n",
            "Batch : 148, D Loss : 4.252 | G Loss : 11.130\n",
            "Batch : 149, D Loss : 3.968 | G Loss : 10.799\n",
            "Batch : 150, D Loss : 3.895 | G Loss : 12.599\n",
            "Batch : 151, D Loss : 3.979 | G Loss : 11.316\n",
            "Batch : 152, D Loss : 4.086 | G Loss : 10.734\n",
            "Batch : 153, D Loss : 3.900 | G Loss : 11.026\n",
            "Batch : 154, D Loss : 3.854 | G Loss : 9.800\n",
            "Batch : 155, D Loss : 3.844 | G Loss : 9.693\n",
            "Batch : 156, D Loss : 3.788 | G Loss : 11.461\n",
            "Batch : 157, D Loss : 3.796 | G Loss : 10.969\n",
            "Batch : 158, D Loss : 3.978 | G Loss : 10.081\n",
            "Batch : 159, D Loss : 4.027 | G Loss : 11.027\n",
            "Batch : 160, D Loss : 4.069 | G Loss : 11.362\n",
            "Batch : 161, D Loss : 3.878 | G Loss : 12.579\n",
            "Batch : 162, D Loss : 3.874 | G Loss : 10.031\n",
            "Batch : 163, D Loss : 3.885 | G Loss : 9.592\n",
            "Batch : 164, D Loss : 3.778 | G Loss : 9.204\n",
            "Batch : 165, D Loss : 3.727 | G Loss : 9.782\n",
            "Batch : 166, D Loss : 4.060 | G Loss : 10.007\n",
            "Batch : 167, D Loss : 3.874 | G Loss : 10.489\n",
            "Batch : 168, D Loss : 3.855 | G Loss : 11.316\n",
            "Batch : 169, D Loss : 3.762 | G Loss : 10.605\n",
            "Batch : 170, D Loss : 3.984 | G Loss : 10.060\n",
            "Batch : 171, D Loss : 4.012 | G Loss : 11.124\n",
            "Batch : 172, D Loss : 3.902 | G Loss : 10.278\n",
            "Batch : 173, D Loss : 4.153 | G Loss : 10.868\n",
            "Batch : 174, D Loss : 3.974 | G Loss : 9.839\n",
            "Batch : 175, D Loss : 3.748 | G Loss : 10.043\n",
            "Batch : 176, D Loss : 3.745 | G Loss : 12.050\n",
            "Batch : 177, D Loss : 4.005 | G Loss : 9.385\n",
            "Batch : 178, D Loss : 3.943 | G Loss : 10.799\n",
            "Batch : 179, D Loss : 3.773 | G Loss : 10.488\n",
            "Batch : 180, D Loss : 3.950 | G Loss : 11.129\n",
            "Batch : 181, D Loss : 3.907 | G Loss : 10.028\n",
            "Batch : 182, D Loss : 3.915 | G Loss : 11.631\n",
            "Batch : 183, D Loss : 4.055 | G Loss : 10.720\n",
            "Batch : 184, D Loss : 3.910 | G Loss : 10.474\n",
            "Batch : 185, D Loss : 3.808 | G Loss : 10.136\n",
            "Batch : 186, D Loss : 3.930 | G Loss : 10.085\n",
            "Batch : 187, D Loss : 3.938 | G Loss : 12.473\n",
            "Batch : 188, D Loss : 4.026 | G Loss : 12.204\n",
            "Batch : 189, D Loss : 3.881 | G Loss : 10.766\n",
            "Batch : 190, D Loss : 3.938 | G Loss : 11.821\n",
            "Batch : 191, D Loss : 3.813 | G Loss : 10.325\n",
            "Batch : 192, D Loss : 3.959 | G Loss : 10.005\n",
            "Batch : 193, D Loss : 3.853 | G Loss : 9.778\n",
            "Batch : 194, D Loss : 3.885 | G Loss : 11.376\n",
            "Batch : 195, D Loss : 3.828 | G Loss : 10.205\n",
            "Batch : 196, D Loss : 3.813 | G Loss : 9.631\n",
            "Batch : 197, D Loss : 3.920 | G Loss : 12.527\n",
            "Batch : 198, D Loss : 3.833 | G Loss : 10.331\n",
            "Batch : 199, D Loss : 3.750 | G Loss : 11.181\n",
            "Batch : 200, D Loss : 3.889 | G Loss : 10.171\n",
            "Batch : 201, D Loss : 3.923 | G Loss : 9.827\n",
            "Batch : 202, D Loss : 3.888 | G Loss : 12.293\n",
            "Batch : 203, D Loss : 4.254 | G Loss : 11.919\n",
            "Batch : 204, D Loss : 4.065 | G Loss : 9.428\n",
            "Batch : 205, D Loss : 3.845 | G Loss : 11.375\n",
            "Batch : 206, D Loss : 3.790 | G Loss : 9.872\n",
            "Batch : 207, D Loss : 3.924 | G Loss : 10.169\n",
            "Batch : 208, D Loss : 3.920 | G Loss : 9.473\n",
            "Batch : 209, D Loss : 3.999 | G Loss : 10.625\n",
            "Batch : 210, D Loss : 3.860 | G Loss : 10.439\n",
            "Batch : 211, D Loss : 3.870 | G Loss : 8.985\n",
            "Batch : 212, D Loss : 3.804 | G Loss : 10.595\n",
            "Batch : 213, D Loss : 3.846 | G Loss : 10.178\n",
            "Batch : 214, D Loss : 3.752 | G Loss : 10.623\n",
            "Batch : 215, D Loss : 3.865 | G Loss : 8.680\n",
            "Batch : 216, D Loss : 3.827 | G Loss : 10.475\n",
            "Batch : 217, D Loss : 3.916 | G Loss : 10.317\n",
            "Batch : 218, D Loss : 3.872 | G Loss : 10.247\n",
            "Batch : 219, D Loss : 3.822 | G Loss : 9.563\n",
            "Batch : 220, D Loss : 3.812 | G Loss : 11.177\n",
            "Batch : 221, D Loss : 3.859 | G Loss : 9.772\n",
            "Batch : 222, D Loss : 3.855 | G Loss : 11.406\n",
            "Batch : 223, D Loss : 3.956 | G Loss : 9.570\n",
            "Batch : 224, D Loss : 3.884 | G Loss : 10.328\n",
            "Batch : 225, D Loss : 3.729 | G Loss : 9.176\n",
            "Batch : 226, D Loss : 3.831 | G Loss : 9.969\n",
            "Batch : 227, D Loss : 3.798 | G Loss : 9.346\n",
            "Batch : 228, D Loss : 3.843 | G Loss : 9.672\n",
            "Batch : 229, D Loss : 3.791 | G Loss : 9.880\n",
            "Batch : 230, D Loss : 3.854 | G Loss : 9.936\n",
            "Batch : 231, D Loss : 3.812 | G Loss : 9.730\n",
            "Batch : 232, D Loss : 3.807 | G Loss : 10.490\n",
            "Batch : 233, D Loss : 4.031 | G Loss : 11.219\n",
            "Batch : 234, D Loss : 3.918 | G Loss : 9.960\n",
            "Batch : 235, D Loss : 3.834 | G Loss : 9.316\n",
            "Batch : 236, D Loss : 4.042 | G Loss : 11.628\n",
            "Batch : 237, D Loss : 3.910 | G Loss : 9.138\n",
            "Batch : 238, D Loss : 3.792 | G Loss : 9.782\n",
            "Batch : 239, D Loss : 3.761 | G Loss : 10.517\n",
            "Batch : 240, D Loss : 3.951 | G Loss : 10.206\n",
            "Batch : 241, D Loss : 3.910 | G Loss : 10.031\n",
            "Batch : 242, D Loss : 3.811 | G Loss : 10.956\n",
            "Batch : 243, D Loss : 3.850 | G Loss : 11.052\n",
            "Batch : 244, D Loss : 3.977 | G Loss : 9.304\n",
            "Batch : 245, D Loss : 3.990 | G Loss : 12.401\n",
            "Batch : 246, D Loss : 3.828 | G Loss : 10.440\n",
            "Batch : 247, D Loss : 3.864 | G Loss : 10.254\n",
            "Batch : 248, D Loss : 3.906 | G Loss : 10.007\n",
            "Batch : 249, D Loss : 3.833 | G Loss : 8.899\n",
            "Batch : 250, D Loss : 3.815 | G Loss : 9.344\n",
            "Batch : 251, D Loss : 3.847 | G Loss : 11.107\n",
            "Batch : 252, D Loss : 3.889 | G Loss : 10.815\n",
            "Batch : 253, D Loss : 3.928 | G Loss : 10.779\n",
            "Batch : 254, D Loss : 3.869 | G Loss : 10.131\n",
            "Batch : 255, D Loss : 3.777 | G Loss : 9.650\n",
            "Batch : 256, D Loss : 3.877 | G Loss : 9.047\n",
            "Batch : 257, D Loss : 3.789 | G Loss : 9.812\n",
            "Batch : 258, D Loss : 3.821 | G Loss : 9.672\n",
            "Batch : 259, D Loss : 3.777 | G Loss : 10.438\n",
            "Batch : 260, D Loss : 4.002 | G Loss : 10.074\n",
            "Batch : 261, D Loss : 3.958 | G Loss : 9.519\n",
            "Batch : 262, D Loss : 3.845 | G Loss : 9.846\n",
            "Batch : 263, D Loss : 4.007 | G Loss : 10.261\n",
            "Batch : 264, D Loss : 3.859 | G Loss : 11.053\n",
            "Batch : 265, D Loss : 3.887 | G Loss : 9.052\n",
            "Batch : 266, D Loss : 3.844 | G Loss : 10.898\n",
            "Batch : 267, D Loss : 3.720 | G Loss : 10.330\n",
            "Batch : 268, D Loss : 3.821 | G Loss : 10.735\n",
            "Batch : 269, D Loss : 3.924 | G Loss : 10.813\n",
            "Batch : 270, D Loss : 3.949 | G Loss : 9.889\n",
            "Batch : 271, D Loss : 3.814 | G Loss : 10.499\n",
            "Batch : 272, D Loss : 3.769 | G Loss : 9.616\n",
            "Batch : 273, D Loss : 3.793 | G Loss : 9.196\n",
            "Batch : 274, D Loss : 3.838 | G Loss : 10.418\n",
            "Batch : 275, D Loss : 3.908 | G Loss : 10.202\n",
            "Batch : 276, D Loss : 3.919 | G Loss : 10.347\n",
            "Batch : 277, D Loss : 3.951 | G Loss : 9.801\n",
            "Batch : 278, D Loss : 3.842 | G Loss : 8.955\n",
            "Batch : 279, D Loss : 3.789 | G Loss : 10.192\n",
            "Batch : 280, D Loss : 3.704 | G Loss : 11.926\n",
            "Batch : 281, D Loss : 3.881 | G Loss : 10.650\n",
            "Batch : 282, D Loss : 3.946 | G Loss : 10.800\n",
            "Batch : 283, D Loss : 3.847 | G Loss : 10.761\n",
            "Batch : 284, D Loss : 3.864 | G Loss : 9.619\n",
            "Batch : 285, D Loss : 3.864 | G Loss : 9.769\n",
            "Batch : 286, D Loss : 3.804 | G Loss : 8.930\n",
            "Batch : 287, D Loss : 3.754 | G Loss : 11.330\n",
            "Batch : 288, D Loss : 3.821 | G Loss : 10.028\n",
            "Batch : 289, D Loss : 3.854 | G Loss : 10.746\n",
            "Batch : 290, D Loss : 3.761 | G Loss : 11.696\n",
            "Batch : 291, D Loss : 3.967 | G Loss : 10.737\n",
            "Batch : 292, D Loss : 3.995 | G Loss : 9.393\n",
            "Batch : 293, D Loss : 3.818 | G Loss : 11.456\n",
            "Batch : 294, D Loss : 3.861 | G Loss : 10.011\n",
            "Batch : 295, D Loss : 3.847 | G Loss : 10.509\n",
            "Batch : 296, D Loss : 3.845 | G Loss : 9.742\n",
            "Batch : 297, D Loss : 3.858 | G Loss : 10.706\n",
            "Batch : 298, D Loss : 3.792 | G Loss : 8.910\n",
            "Batch : 299, D Loss : 3.899 | G Loss : 10.262\n",
            "Batch : 300, D Loss : 3.751 | G Loss : 9.125\n",
            "Batch : 301, D Loss : 3.693 | G Loss : 10.469\n",
            "Batch : 302, D Loss : 3.952 | G Loss : 10.848\n",
            "Batch : 303, D Loss : 3.784 | G Loss : 8.807\n",
            "Batch : 304, D Loss : 3.747 | G Loss : 9.401\n",
            "Batch : 305, D Loss : 3.885 | G Loss : 9.552\n",
            "Batch : 306, D Loss : 3.897 | G Loss : 10.872\n",
            "Batch : 307, D Loss : 3.834 | G Loss : 10.273\n",
            "Batch : 308, D Loss : 3.858 | G Loss : 12.832\n",
            "Batch : 309, D Loss : 4.051 | G Loss : 9.322\n",
            "Batch : 310, D Loss : 4.095 | G Loss : 10.824\n",
            "Batch : 311, D Loss : 3.779 | G Loss : 9.598\n",
            "Batch : 312, D Loss : 3.815 | G Loss : 9.368\n",
            "Batch : 313, D Loss : 4.488 | G Loss : 10.416\n",
            "Batch : 314, D Loss : 4.340 | G Loss : 9.923\n",
            "Batch : 315, D Loss : 4.069 | G Loss : 9.859\n",
            "Batch : 316, D Loss : 4.111 | G Loss : 10.249\n",
            "Batch : 317, D Loss : 6.135 | G Loss : 12.462\n",
            "Batch : 318, D Loss : 5.946 | G Loss : 11.312\n",
            "Batch : 319, D Loss : 5.137 | G Loss : 10.718\n",
            "Batch : 320, D Loss : 4.183 | G Loss : 12.897\n",
            "Batch : 321, D Loss : 4.296 | G Loss : 10.234\n",
            "Batch : 322, D Loss : 4.203 | G Loss : 11.203\n",
            "Batch : 323, D Loss : 4.074 | G Loss : 11.328\n",
            "Batch : 324, D Loss : 4.048 | G Loss : 10.018\n",
            "Batch : 325, D Loss : 3.981 | G Loss : 10.087\n",
            "Batch : 326, D Loss : 3.825 | G Loss : 9.591\n",
            "Batch : 327, D Loss : 3.909 | G Loss : 9.625\n",
            "Batch : 328, D Loss : 3.950 | G Loss : 10.629\n",
            "Batch : 329, D Loss : 3.936 | G Loss : 10.295\n",
            "Batch : 330, D Loss : 3.839 | G Loss : 10.863\n",
            "Batch : 331, D Loss : 3.917 | G Loss : 10.161\n",
            "Batch : 332, D Loss : 3.848 | G Loss : 9.526\n",
            "Batch : 333, D Loss : 3.895 | G Loss : 9.773\n",
            "Batch : 334, D Loss : 3.844 | G Loss : 10.573\n",
            "Batch : 335, D Loss : 3.872 | G Loss : 10.084\n",
            "Batch : 336, D Loss : 3.910 | G Loss : 9.614\n",
            "Batch : 337, D Loss : 4.011 | G Loss : 11.535\n",
            "Batch : 338, D Loss : 3.947 | G Loss : 10.165\n",
            "Batch : 339, D Loss : 3.775 | G Loss : 8.420\n",
            "Batch : 340, D Loss : 4.030 | G Loss : 11.192\n",
            "Batch : 341, D Loss : 3.895 | G Loss : 10.267\n",
            "Batch : 342, D Loss : 3.923 | G Loss : 10.305\n",
            "Batch : 343, D Loss : 3.948 | G Loss : 10.721\n",
            "Batch : 344, D Loss : 3.832 | G Loss : 10.301\n",
            "Batch : 345, D Loss : 3.871 | G Loss : 8.567\n",
            "Batch : 346, D Loss : 3.921 | G Loss : 10.253\n",
            "Batch : 347, D Loss : 3.904 | G Loss : 10.161\n",
            "Batch : 348, D Loss : 3.884 | G Loss : 9.911\n",
            "Batch : 349, D Loss : 3.916 | G Loss : 10.004\n",
            "Batch : 350, D Loss : 3.853 | G Loss : 9.796\n",
            "Batch : 351, D Loss : 3.789 | G Loss : 10.785\n",
            "Batch : 352, D Loss : 3.910 | G Loss : 9.470\n",
            "Batch : 353, D Loss : 3.833 | G Loss : 9.771\n",
            "Batch : 354, D Loss : 3.902 | G Loss : 9.348\n",
            "Batch : 355, D Loss : 3.939 | G Loss : 9.678\n",
            "Batch : 356, D Loss : 3.907 | G Loss : 12.219\n",
            "Batch : 357, D Loss : 3.935 | G Loss : 11.054\n",
            "Batch : 358, D Loss : 3.900 | G Loss : 9.852\n",
            "Batch : 359, D Loss : 3.830 | G Loss : 10.197\n",
            "Batch : 360, D Loss : 4.011 | G Loss : 9.141\n",
            "Batch : 361, D Loss : 4.043 | G Loss : 9.671\n",
            "Batch : 362, D Loss : 3.980 | G Loss : 11.105\n",
            "Batch : 363, D Loss : 3.824 | G Loss : 9.246\n",
            "Batch : 364, D Loss : 3.812 | G Loss : 9.286\n",
            "Batch : 365, D Loss : 3.890 | G Loss : 10.374\n",
            "Batch : 366, D Loss : 4.057 | G Loss : 10.259\n",
            "Batch : 367, D Loss : 3.833 | G Loss : 9.710\n",
            "Batch : 368, D Loss : 3.742 | G Loss : 10.531\n",
            "Batch : 369, D Loss : 3.981 | G Loss : 9.749\n",
            "Batch : 370, D Loss : 4.082 | G Loss : 10.014\n",
            "Batch : 371, D Loss : 3.868 | G Loss : 10.402\n",
            "Batch : 372, D Loss : 3.863 | G Loss : 9.659\n",
            "Batch : 373, D Loss : 3.739 | G Loss : 10.573\n",
            "Batch : 374, D Loss : 3.862 | G Loss : 11.307\n",
            "Batch : 375, D Loss : 3.879 | G Loss : 10.444\n",
            "Batch : 376, D Loss : 3.917 | G Loss : 8.936\n",
            "Batch : 377, D Loss : 3.823 | G Loss : 10.289\n",
            "Batch : 378, D Loss : 3.883 | G Loss : 10.686\n",
            "Batch : 379, D Loss : 3.861 | G Loss : 9.107\n",
            "Batch : 380, D Loss : 3.838 | G Loss : 9.736\n",
            "Batch : 381, D Loss : 3.867 | G Loss : 10.110\n",
            "Batch : 382, D Loss : 3.945 | G Loss : 10.638\n",
            "Batch : 383, D Loss : 3.919 | G Loss : 10.282\n",
            "Batch : 384, D Loss : 4.019 | G Loss : 11.127\n",
            "Batch : 385, D Loss : 3.778 | G Loss : 9.996\n",
            "Batch : 386, D Loss : 3.846 | G Loss : 10.417\n",
            "Batch : 387, D Loss : 3.879 | G Loss : 8.975\n",
            "Batch : 388, D Loss : 3.838 | G Loss : 9.908\n",
            "Batch : 389, D Loss : 3.904 | G Loss : 10.098\n",
            "Batch : 390, D Loss : 3.860 | G Loss : 10.204\n",
            "Batch : 391, D Loss : 3.856 | G Loss : 9.339\n",
            "Batch : 392, D Loss : 3.826 | G Loss : 10.179\n",
            "Batch : 393, D Loss : 3.852 | G Loss : 10.137\n",
            "Batch : 394, D Loss : 3.939 | G Loss : 9.246\n",
            "Batch : 395, D Loss : 3.903 | G Loss : 9.403\n",
            "Batch : 396, D Loss : 3.835 | G Loss : 11.114\n",
            "Batch : 397, D Loss : 3.858 | G Loss : 11.753\n",
            "Batch : 398, D Loss : 3.832 | G Loss : 10.215\n",
            "Batch : 399, D Loss : 3.780 | G Loss : 11.575\n",
            "Batch : 400, D Loss : 3.955 | G Loss : 11.159\n",
            "Batch : 401, D Loss : 3.983 | G Loss : 9.978\n",
            "Batch : 402, D Loss : 4.091 | G Loss : 11.151\n",
            "Batch : 403, D Loss : 3.954 | G Loss : 9.377\n",
            "Batch : 404, D Loss : 3.970 | G Loss : 9.586\n",
            "Batch : 405, D Loss : 3.879 | G Loss : 10.039\n",
            "Batch : 406, D Loss : 3.720 | G Loss : 9.472\n",
            "Batch : 407, D Loss : 3.839 | G Loss : 9.523\n",
            "Batch : 408, D Loss : 3.908 | G Loss : 10.252\n",
            "Batch : 409, D Loss : 3.786 | G Loss : 9.233\n",
            "Batch : 410, D Loss : 3.867 | G Loss : 9.414\n",
            "Batch : 411, D Loss : 3.938 | G Loss : 10.793\n",
            "Batch : 412, D Loss : 3.868 | G Loss : 8.951\n",
            "Batch : 413, D Loss : 3.763 | G Loss : 9.871\n",
            "Batch : 414, D Loss : 3.769 | G Loss : 10.917\n",
            "Batch : 415, D Loss : 3.819 | G Loss : 9.138\n",
            "Batch : 416, D Loss : 3.765 | G Loss : 9.355\n",
            "Batch : 417, D Loss : 3.758 | G Loss : 10.334\n",
            "Batch : 418, D Loss : 3.863 | G Loss : 10.165\n",
            "Batch : 419, D Loss : 3.895 | G Loss : 10.680\n",
            "Batch : 420, D Loss : 3.787 | G Loss : 11.451\n",
            "Batch : 421, D Loss : 3.833 | G Loss : 9.497\n",
            "Batch : 422, D Loss : 3.864 | G Loss : 10.469\n",
            "Batch : 423, D Loss : 3.798 | G Loss : 9.570\n",
            "Batch : 424, D Loss : 3.828 | G Loss : 9.564\n",
            "Batch : 425, D Loss : 3.732 | G Loss : 9.316\n",
            "Batch : 426, D Loss : 3.938 | G Loss : 10.292\n",
            "Batch : 427, D Loss : 3.898 | G Loss : 10.656\n",
            "Batch : 428, D Loss : 3.835 | G Loss : 9.424\n",
            "Batch : 429, D Loss : 3.865 | G Loss : 9.481\n",
            "Batch : 430, D Loss : 3.840 | G Loss : 9.249\n",
            "Batch : 431, D Loss : 3.804 | G Loss : 9.084\n",
            "Batch : 432, D Loss : 3.872 | G Loss : 10.047\n",
            "Batch : 433, D Loss : 4.354 | G Loss : 9.064\n",
            "Batch : 434, D Loss : 4.061 | G Loss : 9.946\n",
            "Batch : 435, D Loss : 3.944 | G Loss : 10.580\n",
            "Batch : 436, D Loss : 3.963 | G Loss : 10.206\n",
            "Batch : 437, D Loss : 3.912 | G Loss : 9.543\n",
            "Batch : 438, D Loss : 3.919 | G Loss : 9.447\n",
            "Batch : 439, D Loss : 3.939 | G Loss : 11.051\n",
            "Batch : 440, D Loss : 3.990 | G Loss : 10.694\n",
            "Batch : 441, D Loss : 3.906 | G Loss : 9.362\n",
            "Batch : 442, D Loss : 3.910 | G Loss : 10.696\n",
            "Batch : 443, D Loss : 3.878 | G Loss : 10.066\n",
            "Batch : 444, D Loss : 3.813 | G Loss : 11.537\n",
            "Batch : 445, D Loss : 4.058 | G Loss : 9.933\n",
            "Batch : 446, D Loss : 3.973 | G Loss : 10.778\n",
            "Batch : 447, D Loss : 4.025 | G Loss : 11.938\n",
            "Batch : 448, D Loss : 3.952 | G Loss : 10.660\n",
            "Batch : 449, D Loss : 3.835 | G Loss : 10.369\n",
            "Batch : 450, D Loss : 3.865 | G Loss : 9.720\n",
            "Batch : 451, D Loss : 3.786 | G Loss : 11.268\n",
            "Batch : 452, D Loss : 4.052 | G Loss : 8.962\n",
            "Batch : 453, D Loss : 4.121 | G Loss : 10.855\n",
            "Batch : 454, D Loss : 4.051 | G Loss : 11.316\n",
            "Batch : 455, D Loss : 3.930 | G Loss : 9.036\n",
            "Batch : 456, D Loss : 3.771 | G Loss : 11.361\n",
            "Batch : 457, D Loss : 3.842 | G Loss : 9.472\n",
            "Batch : 458, D Loss : 3.756 | G Loss : 9.437\n",
            "Batch : 459, D Loss : 3.880 | G Loss : 9.126\n",
            "Batch : 460, D Loss : 3.984 | G Loss : 9.001\n",
            "Batch : 461, D Loss : 3.789 | G Loss : 9.918\n",
            "Batch : 462, D Loss : 3.791 | G Loss : 9.282\n",
            "Batch : 463, D Loss : 3.928 | G Loss : 9.787\n",
            "Batch : 464, D Loss : 3.899 | G Loss : 10.172\n",
            "Batch : 465, D Loss : 3.908 | G Loss : 10.271\n",
            "Batch : 466, D Loss : 3.836 | G Loss : 9.482\n",
            "Batch : 467, D Loss : 3.832 | G Loss : 9.685\n",
            "Batch : 468, D Loss : 3.775 | G Loss : 8.535\n",
            "Batch : 469, D Loss : 3.854 | G Loss : 10.493\n",
            "Batch : 470, D Loss : 3.833 | G Loss : 12.128\n",
            "Batch : 471, D Loss : 3.825 | G Loss : 9.581\n",
            "Batch : 472, D Loss : 3.906 | G Loss : 8.656\n",
            "Batch : 473, D Loss : 3.804 | G Loss : 11.460\n",
            "Batch : 474, D Loss : 3.805 | G Loss : 9.352\n",
            "Batch : 475, D Loss : 3.854 | G Loss : 9.629\n",
            "Batch : 476, D Loss : 3.862 | G Loss : 10.119\n",
            "Batch : 477, D Loss : 3.765 | G Loss : 9.327\n",
            "Batch : 478, D Loss : 3.776 | G Loss : 9.254\n",
            "Batch : 479, D Loss : 3.789 | G Loss : 10.552\n",
            "Batch : 480, D Loss : 3.925 | G Loss : 9.119\n",
            "Batch : 481, D Loss : 3.987 | G Loss : 8.597\n",
            "Batch : 482, D Loss : 4.093 | G Loss : 14.184\n",
            "Batch : 483, D Loss : 3.862 | G Loss : 12.078\n",
            "Batch : 484, D Loss : 3.814 | G Loss : 10.660\n",
            "Batch : 485, D Loss : 3.956 | G Loss : 9.472\n",
            "Batch : 486, D Loss : 3.891 | G Loss : 9.150\n",
            "Batch : 487, D Loss : 3.825 | G Loss : 9.557\n",
            "Batch : 488, D Loss : 3.912 | G Loss : 9.707\n",
            "Batch : 489, D Loss : 3.866 | G Loss : 8.937\n",
            "Batch : 490, D Loss : 3.837 | G Loss : 8.602\n",
            "Batch : 491, D Loss : 3.931 | G Loss : 11.013\n",
            "Batch : 492, D Loss : 3.856 | G Loss : 9.088\n",
            "Batch : 493, D Loss : 4.165 | G Loss : 8.774\n",
            "Batch : 494, D Loss : 4.062 | G Loss : 9.827\n",
            "Batch : 495, D Loss : 3.750 | G Loss : 10.629\n",
            "Batch : 496, D Loss : 3.795 | G Loss : 11.465\n",
            "Batch : 497, D Loss : 4.064 | G Loss : 10.440\n",
            "Batch : 498, D Loss : 4.083 | G Loss : 9.678\n",
            "Batch : 499, D Loss : 3.939 | G Loss : 10.348\n",
            "Batch : 500, D Loss : 3.801 | G Loss : 9.787\n",
            "Batch : 501, D Loss : 3.815 | G Loss : 9.818\n",
            "Batch : 502, D Loss : 3.860 | G Loss : 9.048\n",
            "Batch : 503, D Loss : 3.768 | G Loss : 8.954\n",
            "Batch : 504, D Loss : 3.877 | G Loss : 9.895\n",
            "Batch : 505, D Loss : 3.824 | G Loss : 10.010\n",
            "Batch : 506, D Loss : 4.086 | G Loss : 9.275\n",
            "Batch : 507, D Loss : 3.896 | G Loss : 11.064\n",
            "Batch : 508, D Loss : 3.814 | G Loss : 10.401\n",
            "Batch : 509, D Loss : 3.811 | G Loss : 11.042\n",
            "Batch : 510, D Loss : 3.799 | G Loss : 9.529\n",
            "Batch : 511, D Loss : 3.841 | G Loss : 9.333\n",
            "Batch : 512, D Loss : 3.842 | G Loss : 9.686\n",
            "Batch : 513, D Loss : 3.929 | G Loss : 10.126\n",
            "Batch : 514, D Loss : 3.849 | G Loss : 9.702\n",
            "Batch : 515, D Loss : 3.860 | G Loss : 10.106\n",
            "Batch : 516, D Loss : 3.841 | G Loss : 10.997\n",
            "Batch : 517, D Loss : 3.834 | G Loss : 10.648\n",
            "Batch : 518, D Loss : 3.974 | G Loss : 12.173\n",
            "Batch : 519, D Loss : 3.995 | G Loss : 9.358\n",
            "Batch : 520, D Loss : 3.966 | G Loss : 9.877\n",
            "Batch : 521, D Loss : 3.858 | G Loss : 9.408\n",
            "Batch : 522, D Loss : 3.844 | G Loss : 11.555\n",
            "Batch : 523, D Loss : 3.972 | G Loss : 9.975\n",
            "Batch : 524, D Loss : 3.821 | G Loss : 9.697\n",
            "Batch : 525, D Loss : 3.822 | G Loss : 9.528\n",
            "Batch : 526, D Loss : 4.127 | G Loss : 9.579\n",
            "Batch : 527, D Loss : 4.437 | G Loss : 8.624\n",
            "Batch : 528, D Loss : 4.123 | G Loss : 10.630\n",
            "Batch : 529, D Loss : 3.906 | G Loss : 10.193\n",
            "Batch : 530, D Loss : 3.876 | G Loss : 10.917\n",
            "Batch : 531, D Loss : 3.859 | G Loss : 9.112\n",
            "Batch : 532, D Loss : 3.872 | G Loss : 9.596\n",
            "Batch : 533, D Loss : 3.921 | G Loss : 11.363\n",
            "Batch : 534, D Loss : 3.847 | G Loss : 10.027\n",
            "Batch : 535, D Loss : 3.848 | G Loss : 9.403\n",
            "Batch : 536, D Loss : 3.930 | G Loss : 9.128\n",
            "Batch : 537, D Loss : 3.808 | G Loss : 10.197\n",
            "Batch : 538, D Loss : 3.862 | G Loss : 9.391\n",
            "Batch : 539, D Loss : 3.993 | G Loss : 10.293\n",
            "Batch : 540, D Loss : 4.430 | G Loss : 14.640\n",
            "Batch : 541, D Loss : 4.319 | G Loss : 13.538\n",
            "Batch : 542, D Loss : 4.443 | G Loss : 11.738\n",
            "Batch : 543, D Loss : 4.127 | G Loss : 13.327\n",
            "Batch : 544, D Loss : 4.298 | G Loss : 11.125\n",
            "Batch : 545, D Loss : 5.121 | G Loss : 12.202\n",
            "Batch : 546, D Loss : 4.936 | G Loss : 12.079\n",
            "Batch : 547, D Loss : 3.980 | G Loss : 11.198\n",
            "Batch : 548, D Loss : 4.116 | G Loss : 11.328\n",
            "Batch : 549, D Loss : 4.185 | G Loss : 11.568\n",
            "Batch : 550, D Loss : 4.237 | G Loss : 10.877\n",
            "Batch : 551, D Loss : 4.027 | G Loss : 10.660\n",
            "Batch : 552, D Loss : 3.986 | G Loss : 9.149\n",
            "Batch : 553, D Loss : 4.113 | G Loss : 10.514\n",
            "Batch : 554, D Loss : 4.073 | G Loss : 9.200\n",
            "Batch : 555, D Loss : 3.938 | G Loss : 10.541\n",
            "Batch : 556, D Loss : 3.890 | G Loss : 9.330\n",
            "Batch : 557, D Loss : 3.999 | G Loss : 10.303\n",
            "Batch : 558, D Loss : 3.890 | G Loss : 9.146\n",
            "Batch : 559, D Loss : 3.983 | G Loss : 9.573\n",
            "Batch : 560, D Loss : 3.970 | G Loss : 9.368\n",
            "Batch : 561, D Loss : 3.847 | G Loss : 9.658\n",
            "Batch : 562, D Loss : 3.854 | G Loss : 8.707\n",
            "Batch : 563, D Loss : 3.939 | G Loss : 10.058\n",
            "Batch : 564, D Loss : 3.895 | G Loss : 10.559\n",
            "Batch : 565, D Loss : 3.882 | G Loss : 9.900\n",
            "Batch : 566, D Loss : 3.949 | G Loss : 8.823\n",
            "Batch : 567, D Loss : 3.894 | G Loss : 11.394\n",
            "Batch : 568, D Loss : 3.820 | G Loss : 9.068\n",
            "Batch : 569, D Loss : 3.984 | G Loss : 11.298\n",
            "Batch : 570, D Loss : 3.822 | G Loss : 9.203\n",
            "Batch : 571, D Loss : 3.712 | G Loss : 10.955\n",
            "Batch : 572, D Loss : 3.880 | G Loss : 10.212\n",
            "Batch : 573, D Loss : 3.961 | G Loss : 10.311\n",
            "Batch : 574, D Loss : 4.193 | G Loss : 8.968\n",
            "Batch : 575, D Loss : 4.037 | G Loss : 9.249\n",
            "Batch : 576, D Loss : 4.125 | G Loss : 13.406\n",
            "Batch : 577, D Loss : 5.328 | G Loss : 13.893\n",
            "Batch : 578, D Loss : 5.036 | G Loss : 10.730\n",
            "Batch : 579, D Loss : 4.595 | G Loss : 10.764\n",
            "Batch : 580, D Loss : 4.204 | G Loss : 13.602\n",
            "Batch : 581, D Loss : 4.878 | G Loss : 11.257\n",
            "Batch : 582, D Loss : 4.875 | G Loss : 11.606\n",
            "Batch : 583, D Loss : 4.369 | G Loss : 11.069\n",
            "Batch : 584, D Loss : 4.248 | G Loss : 10.919\n",
            "Batch : 585, D Loss : 6.045 | G Loss : 14.928\n",
            "Batch : 586, D Loss : 5.885 | G Loss : 16.621\n",
            "Batch : 587, D Loss : 5.203 | G Loss : 12.981\n",
            "Batch : 588, D Loss : 3.914 | G Loss : 15.538\n",
            "Batch : 589, D Loss : 6.477 | G Loss : 17.155\n",
            "Batch : 590, D Loss : 6.439 | G Loss : 17.003\n",
            "Batch : 591, D Loss : 6.334 | G Loss : 17.007\n",
            "Batch : 592, D Loss : 5.976 | G Loss : 15.359\n",
            "Batch : 593, D Loss : 5.706 | G Loss : 14.696\n",
            "Batch : 594, D Loss : 5.105 | G Loss : 13.443\n",
            "Batch : 595, D Loss : 4.423 | G Loss : 12.856\n",
            "Batch : 596, D Loss : 4.525 | G Loss : 12.001\n",
            "Batch : 597, D Loss : 4.480 | G Loss : 11.003\n",
            "Batch : 598, D Loss : 5.528 | G Loss : 13.142\n",
            "Batch : 599, D Loss : 4.883 | G Loss : 12.624\n",
            "Batch : 600, D Loss : 4.461 | G Loss : 11.359\n",
            "Batch : 601, D Loss : 4.326 | G Loss : 11.693\n",
            "Batch : 602, D Loss : 4.433 | G Loss : 12.463\n",
            "Batch : 603, D Loss : 4.546 | G Loss : 12.345\n",
            "Batch : 604, D Loss : 4.227 | G Loss : 13.072\n",
            "Batch : 605, D Loss : 4.147 | G Loss : 11.723\n",
            "Batch : 606, D Loss : 4.704 | G Loss : 12.961\n",
            "Batch : 607, D Loss : 4.236 | G Loss : 11.215\n",
            "Batch : 608, D Loss : 3.974 | G Loss : 11.826\n",
            "Batch : 609, D Loss : 4.411 | G Loss : 11.961\n",
            "Batch : 610, D Loss : 4.151 | G Loss : 10.794\n",
            "Batch : 611, D Loss : 3.970 | G Loss : 13.001\n",
            "Batch : 612, D Loss : 6.292 | G Loss : 13.103\n",
            "Batch : 613, D Loss : 6.123 | G Loss : 14.306\n",
            "Batch : 614, D Loss : 6.035 | G Loss : 12.576\n",
            "Batch : 615, D Loss : 5.706 | G Loss : 12.778\n",
            "Batch : 616, D Loss : 5.137 | G Loss : 11.202\n",
            "Batch : 617, D Loss : 4.604 | G Loss : 12.679\n",
            "Batch : 618, D Loss : 5.883 | G Loss : 12.655\n",
            "Batch : 619, D Loss : 5.986 | G Loss : 14.360\n",
            "Batch : 620, D Loss : 5.830 | G Loss : 12.293\n",
            "Batch : 621, D Loss : 5.336 | G Loss : 12.345\n",
            "Batch : 622, D Loss : 5.681 | G Loss : 13.591\n",
            "Batch : 623, D Loss : 5.757 | G Loss : 14.720\n",
            "Batch : 624, D Loss : 5.569 | G Loss : 13.470\n",
            "Batch : 625, D Loss : 5.044 | G Loss : 11.443\n",
            "Batch : 626, D Loss : 4.924 | G Loss : 11.977\n",
            "Batch : 627, D Loss : 4.851 | G Loss : 12.378\n",
            "Batch : 628, D Loss : 4.862 | G Loss : 11.669\n",
            "Batch : 629, D Loss : 4.734 | G Loss : 10.802\n",
            "Batch : 630, D Loss : 4.442 | G Loss : 11.350\n",
            "Batch : 631, D Loss : 4.924 | G Loss : 10.389\n",
            "Batch : 632, D Loss : 4.817 | G Loss : 11.821\n",
            "Batch : 633, D Loss : 4.239 | G Loss : 10.137\n",
            "Batch : 634, D Loss : 4.360 | G Loss : 11.980\n",
            "Batch : 635, D Loss : 4.573 | G Loss : 10.419\n",
            "Batch : 636, D Loss : 4.448 | G Loss : 10.827\n",
            "Batch : 637, D Loss : 4.174 | G Loss : 10.919\n",
            "Batch : 638, D Loss : 4.159 | G Loss : 10.610\n",
            "Batch : 639, D Loss : 4.499 | G Loss : 10.485\n",
            "Batch : 640, D Loss : 4.565 | G Loss : 10.683\n",
            "Batch : 641, D Loss : 4.284 | G Loss : 10.777\n",
            "Batch : 642, D Loss : 4.074 | G Loss : 10.639\n",
            "Batch : 643, D Loss : 4.115 | G Loss : 11.204\n",
            "Batch : 644, D Loss : 3.989 | G Loss : 11.137\n",
            "Batch : 645, D Loss : 3.995 | G Loss : 11.527\n",
            "Batch : 646, D Loss : 4.048 | G Loss : 10.112\n",
            "Batch : 647, D Loss : 3.917 | G Loss : 10.409\n",
            "Batch : 648, D Loss : 3.877 | G Loss : 10.647\n",
            "Batch : 649, D Loss : 3.839 | G Loss : 9.677\n",
            "Batch : 650, D Loss : 4.156 | G Loss : 9.776\n",
            "Batch : 651, D Loss : 4.140 | G Loss : 9.982\n",
            "Batch : 652, D Loss : 3.869 | G Loss : 11.264\n",
            "Batch : 653, D Loss : 4.092 | G Loss : 9.938\n",
            "Batch : 654, D Loss : 3.983 | G Loss : 10.005\n",
            "Batch : 655, D Loss : 3.990 | G Loss : 9.048\n",
            "Batch : 656, D Loss : 4.074 | G Loss : 9.255\n",
            "Batch : 657, D Loss : 4.006 | G Loss : 9.148\n",
            "Batch : 658, D Loss : 3.830 | G Loss : 10.817\n",
            "Batch : 659, D Loss : 3.881 | G Loss : 8.970\n",
            "Batch : 660, D Loss : 3.914 | G Loss : 11.307\n",
            "Batch : 661, D Loss : 3.870 | G Loss : 10.090\n",
            "Batch : 662, D Loss : 3.946 | G Loss : 11.838\n",
            "Batch : 663, D Loss : 4.157 | G Loss : 12.681\n",
            "Batch : 664, D Loss : 4.089 | G Loss : 10.127\n",
            "Batch : 665, D Loss : 4.005 | G Loss : 10.522\n",
            "Batch : 666, D Loss : 3.957 | G Loss : 10.108\n",
            "Batch : 667, D Loss : 4.001 | G Loss : 10.545\n",
            "Batch : 668, D Loss : 4.001 | G Loss : 9.785\n",
            "Batch : 669, D Loss : 4.904 | G Loss : 19.772\n",
            "Batch : 670, D Loss : 6.150 | G Loss : 16.695\n",
            "Batch : 671, D Loss : 5.215 | G Loss : 18.020\n",
            "Batch : 672, D Loss : 5.152 | G Loss : 17.063\n",
            "Batch : 673, D Loss : 4.594 | G Loss : 15.554\n",
            "Batch : 674, D Loss : 4.666 | G Loss : 16.820\n",
            "Batch : 675, D Loss : 4.509 | G Loss : 14.113\n",
            "Batch : 676, D Loss : 4.644 | G Loss : 12.952\n",
            "Batch : 677, D Loss : 4.519 | G Loss : 11.998\n",
            "Batch : 678, D Loss : 4.142 | G Loss : 13.047\n",
            "Batch : 679, D Loss : 4.592 | G Loss : 13.403\n",
            "Batch : 680, D Loss : 4.495 | G Loss : 13.834\n",
            "Batch : 681, D Loss : 4.209 | G Loss : 11.923\n",
            "Batch : 682, D Loss : 3.915 | G Loss : 14.408\n",
            "Batch : 683, D Loss : 4.301 | G Loss : 10.949\n",
            "Batch : 684, D Loss : 4.307 | G Loss : 12.421\n",
            "Batch : 685, D Loss : 4.098 | G Loss : 13.187\n",
            "Batch : 686, D Loss : 4.150 | G Loss : 10.780\n",
            "Batch : 687, D Loss : 3.966 | G Loss : 12.828\n",
            "Batch : 688, D Loss : 4.987 | G Loss : 13.930\n",
            "Batch : 689, D Loss : 5.135 | G Loss : 11.965\n",
            "Batch : 690, D Loss : 4.628 | G Loss : 12.530\n",
            "Batch : 691, D Loss : 4.321 | G Loss : 11.021\n",
            "Batch : 692, D Loss : 3.994 | G Loss : 11.626\n",
            "Batch : 693, D Loss : 4.299 | G Loss : 12.236\n",
            "Batch : 694, D Loss : 4.085 | G Loss : 13.885\n",
            "Batch : 695, D Loss : 4.209 | G Loss : 10.495\n",
            "Batch : 696, D Loss : 4.104 | G Loss : 12.350\n",
            "Batch : 697, D Loss : 3.958 | G Loss : 12.010\n",
            "Batch : 698, D Loss : 4.110 | G Loss : 10.656\n",
            "Batch : 699, D Loss : 4.227 | G Loss : 10.994\n",
            "Batch : 700, D Loss : 4.215 | G Loss : 11.265\n",
            "Batch : 701, D Loss : 4.167 | G Loss : 11.583\n",
            "Batch : 702, D Loss : 3.935 | G Loss : 10.129\n",
            "Batch : 703, D Loss : 3.885 | G Loss : 10.252\n",
            "Batch : 704, D Loss : 3.932 | G Loss : 11.002\n",
            "Batch : 705, D Loss : 4.223 | G Loss : 9.473\n",
            "Batch : 706, D Loss : 4.138 | G Loss : 12.003\n",
            "Batch : 707, D Loss : 4.084 | G Loss : 11.862\n",
            "Batch : 708, D Loss : 3.820 | G Loss : 10.859\n",
            "Batch : 709, D Loss : 4.030 | G Loss : 10.749\n",
            "Batch : 710, D Loss : 4.091 | G Loss : 11.960\n",
            "Batch : 711, D Loss : 4.753 | G Loss : 11.937\n",
            "Batch : 712, D Loss : 4.435 | G Loss : 11.673\n",
            "Batch : 713, D Loss : 4.408 | G Loss : 12.405\n",
            "Batch : 714, D Loss : 4.605 | G Loss : 11.840\n",
            "Batch : 715, D Loss : 4.455 | G Loss : 13.316\n",
            "Batch : 716, D Loss : 4.188 | G Loss : 10.983\n",
            "Batch : 717, D Loss : 4.082 | G Loss : 12.592\n",
            "Batch : 718, D Loss : 4.150 | G Loss : 12.465\n",
            "Batch : 719, D Loss : 4.195 | G Loss : 13.127\n",
            "Batch : 720, D Loss : 4.069 | G Loss : 11.177\n",
            "Batch : 721, D Loss : 3.976 | G Loss : 10.994\n",
            "Batch : 722, D Loss : 3.947 | G Loss : 12.223\n",
            "Batch : 723, D Loss : 4.198 | G Loss : 11.104\n",
            "Batch : 724, D Loss : 3.992 | G Loss : 11.484\n",
            "Batch : 725, D Loss : 4.020 | G Loss : 11.472\n",
            "Batch : 726, D Loss : 3.996 | G Loss : 10.331\n",
            "Batch : 727, D Loss : 3.917 | G Loss : 12.486\n",
            "Batch : 728, D Loss : 3.926 | G Loss : 10.305\n",
            "Batch : 729, D Loss : 4.378 | G Loss : 11.278\n",
            "Batch : 730, D Loss : 4.605 | G Loss : 11.572\n",
            "Batch : 731, D Loss : 4.108 | G Loss : 11.175\n",
            "Batch : 732, D Loss : 4.104 | G Loss : 12.068\n",
            "Batch : 733, D Loss : 4.040 | G Loss : 10.141\n",
            "Batch : 734, D Loss : 4.005 | G Loss : 9.706\n",
            "Batch : 735, D Loss : 4.053 | G Loss : 10.603\n",
            "Batch : 736, D Loss : 4.008 | G Loss : 11.345\n",
            "Batch : 737, D Loss : 3.843 | G Loss : 11.814\n",
            "Batch : 738, D Loss : 4.011 | G Loss : 11.815\n",
            "Batch : 739, D Loss : 4.269 | G Loss : 11.279\n",
            "Batch : 740, D Loss : 3.976 | G Loss : 9.247\n",
            "Batch : 741, D Loss : 4.053 | G Loss : 10.500\n",
            "Batch : 742, D Loss : 4.089 | G Loss : 10.192\n",
            "Batch : 743, D Loss : 4.327 | G Loss : 11.453\n",
            "Batch : 744, D Loss : 3.970 | G Loss : 10.644\n",
            "Batch : 745, D Loss : 3.901 | G Loss : 11.174\n",
            "Batch : 746, D Loss : 3.841 | G Loss : 9.650\n",
            "Batch : 747, D Loss : 4.035 | G Loss : 11.925\n",
            "Batch : 748, D Loss : 4.035 | G Loss : 10.139\n",
            "Batch : 749, D Loss : 4.125 | G Loss : 10.570\n",
            "Batch : 750, D Loss : 3.886 | G Loss : 12.121\n",
            "Batch : 751, D Loss : 4.516 | G Loss : 9.891\n",
            "Batch : 752, D Loss : 4.331 | G Loss : 10.787\n",
            "Batch : 753, D Loss : 3.881 | G Loss : 10.335\n",
            "Batch : 754, D Loss : 5.378 | G Loss : 11.031\n",
            "Batch : 755, D Loss : 5.594 | G Loss : 11.649\n",
            "Batch : 756, D Loss : 5.252 | G Loss : 12.105\n",
            "Batch : 757, D Loss : 4.429 | G Loss : 11.196\n",
            "Batch : 758, D Loss : 4.038 | G Loss : 11.381\n",
            "Batch : 759, D Loss : 4.380 | G Loss : 10.919\n",
            "Batch : 760, D Loss : 4.331 | G Loss : 10.507\n",
            "Batch : 761, D Loss : 4.104 | G Loss : 9.965\n",
            "Batch : 762, D Loss : 4.079 | G Loss : 11.733\n",
            "Batch : 763, D Loss : 3.948 | G Loss : 9.844\n",
            "Batch : 764, D Loss : 3.986 | G Loss : 9.905\n",
            "Batch : 765, D Loss : 4.097 | G Loss : 12.212\n",
            "Batch : 766, D Loss : 4.068 | G Loss : 11.027\n",
            "Batch : 767, D Loss : 4.112 | G Loss : 11.270\n",
            "Batch : 768, D Loss : 4.142 | G Loss : 12.505\n",
            "Batch : 769, D Loss : 4.086 | G Loss : 9.653\n",
            "Batch : 770, D Loss : 4.009 | G Loss : 10.253\n",
            "Batch : 771, D Loss : 3.906 | G Loss : 9.578\n",
            "Batch : 772, D Loss : 4.048 | G Loss : 9.760\n",
            "Batch : 773, D Loss : 3.975 | G Loss : 9.213\n",
            "Batch : 774, D Loss : 3.879 | G Loss : 10.684\n",
            "Batch : 775, D Loss : 4.010 | G Loss : 10.531\n",
            "Batch : 776, D Loss : 4.155 | G Loss : 10.014\n",
            "Batch : 777, D Loss : 4.341 | G Loss : 9.378\n",
            "Batch : 778, D Loss : 3.972 | G Loss : 10.030\n",
            "Batch : 779, D Loss : 4.011 | G Loss : 9.330\n",
            "Batch : 780, D Loss : 4.081 | G Loss : 12.050\n",
            "Batch : 781, D Loss : 4.108 | G Loss : 12.147\n",
            "Batch : 782, D Loss : 4.153 | G Loss : 10.767\n",
            "Batch : 783, D Loss : 4.036 | G Loss : 9.791\n",
            "Batch : 784, D Loss : 4.040 | G Loss : 10.129\n",
            "Batch : 785, D Loss : 4.035 | G Loss : 9.418\n",
            "Batch : 786, D Loss : 4.183 | G Loss : 10.552\n",
            "Batch : 787, D Loss : 3.899 | G Loss : 10.523\n",
            "Batch : 788, D Loss : 3.913 | G Loss : 10.927\n",
            "Batch : 789, D Loss : 3.911 | G Loss : 11.225\n",
            "Batch : 790, D Loss : 3.786 | G Loss : 10.728\n",
            "Batch : 791, D Loss : 4.178 | G Loss : 11.558\n",
            "Batch : 792, D Loss : 4.156 | G Loss : 9.402\n",
            "Batch : 793, D Loss : 4.030 | G Loss : 10.313\n",
            "Batch : 794, D Loss : 3.890 | G Loss : 9.865\n",
            "Batch : 795, D Loss : 4.022 | G Loss : 12.607\n",
            "Batch : 796, D Loss : 3.985 | G Loss : 9.354\n",
            "Batch : 797, D Loss : 3.942 | G Loss : 10.524\n",
            "Batch : 798, D Loss : 4.000 | G Loss : 10.624\n",
            "Batch : 799, D Loss : 3.885 | G Loss : 9.710\n",
            "Batch : 800, D Loss : 3.818 | G Loss : 8.950\n",
            "Batch : 801, D Loss : 3.893 | G Loss : 10.467\n",
            "Batch : 802, D Loss : 3.882 | G Loss : 10.985\n",
            "Batch : 803, D Loss : 3.842 | G Loss : 9.633\n",
            "Batch : 804, D Loss : 3.991 | G Loss : 11.127\n",
            "Batch : 805, D Loss : 3.886 | G Loss : 9.150\n",
            "Batch : 806, D Loss : 3.908 | G Loss : 8.993\n",
            "Batch : 807, D Loss : 3.773 | G Loss : 9.720\n",
            "Batch : 808, D Loss : 3.823 | G Loss : 9.307\n",
            "Batch : 809, D Loss : 3.855 | G Loss : 10.010\n",
            "Batch : 810, D Loss : 4.043 | G Loss : 9.969\n",
            "Batch : 811, D Loss : 3.811 | G Loss : 12.608\n",
            "Batch : 812, D Loss : 3.902 | G Loss : 10.616\n",
            "Batch : 813, D Loss : 3.789 | G Loss : 10.094\n",
            "Batch : 814, D Loss : 3.813 | G Loss : 9.474\n",
            "Batch : 815, D Loss : 3.936 | G Loss : 10.686\n",
            "Batch : 816, D Loss : 3.929 | G Loss : 9.340\n",
            "Batch : 817, D Loss : 3.903 | G Loss : 11.475\n",
            "Batch : 818, D Loss : 4.019 | G Loss : 10.293\n",
            "Batch : 819, D Loss : 3.975 | G Loss : 13.508\n",
            "Batch : 820, D Loss : 3.827 | G Loss : 9.127\n",
            "Batch : 821, D Loss : 4.447 | G Loss : 13.102\n",
            "Batch : 822, D Loss : 4.515 | G Loss : 11.916\n",
            "Batch : 823, D Loss : 4.391 | G Loss : 12.194\n",
            "Batch : 824, D Loss : 4.487 | G Loss : 12.316\n",
            "Batch : 825, D Loss : 4.261 | G Loss : 10.548\n",
            "Batch : 826, D Loss : 4.052 | G Loss : 11.881\n",
            "Batch : 827, D Loss : 4.102 | G Loss : 10.068\n",
            "Batch : 828, D Loss : 4.048 | G Loss : 11.859\n",
            "Batch : 829, D Loss : 4.049 | G Loss : 13.409\n",
            "Batch : 830, D Loss : 4.369 | G Loss : 12.977\n",
            "Batch : 831, D Loss : 4.144 | G Loss : 12.955\n",
            "Batch : 832, D Loss : 4.008 | G Loss : 11.716\n",
            "Batch : 833, D Loss : 3.909 | G Loss : 10.557\n",
            "Batch : 834, D Loss : 4.119 | G Loss : 12.633\n",
            "Batch : 835, D Loss : 3.977 | G Loss : 9.699\n",
            "Batch : 836, D Loss : 3.886 | G Loss : 10.038\n",
            "Batch : 837, D Loss : 3.890 | G Loss : 9.590\n",
            "Batch : 838, D Loss : 4.012 | G Loss : 10.688\n",
            "Batch : 839, D Loss : 4.029 | G Loss : 10.249\n",
            "Batch : 840, D Loss : 3.907 | G Loss : 9.989\n",
            "Batch : 841, D Loss : 3.876 | G Loss : 10.754\n",
            "Batch : 842, D Loss : 3.888 | G Loss : 9.704\n",
            "Batch : 843, D Loss : 4.026 | G Loss : 10.350\n",
            "Batch : 844, D Loss : 3.978 | G Loss : 10.479\n",
            "Batch : 845, D Loss : 4.021 | G Loss : 10.244\n",
            "Batch : 846, D Loss : 3.900 | G Loss : 11.807\n",
            "Batch : 847, D Loss : 3.838 | G Loss : 8.839\n",
            "Batch : 848, D Loss : 3.895 | G Loss : 11.487\n",
            "Batch : 849, D Loss : 3.797 | G Loss : 11.102\n",
            "Batch : 850, D Loss : 3.956 | G Loss : 10.411\n",
            "Batch : 851, D Loss : 3.910 | G Loss : 9.782\n",
            "Batch : 852, D Loss : 4.082 | G Loss : 9.612\n",
            "Batch : 853, D Loss : 4.019 | G Loss : 11.621\n",
            "Batch : 854, D Loss : 3.946 | G Loss : 9.759\n",
            "Batch : 855, D Loss : 3.949 | G Loss : 9.870\n",
            "Batch : 856, D Loss : 3.924 | G Loss : 9.776\n",
            "Batch : 857, D Loss : 3.961 | G Loss : 9.570\n",
            "Batch : 858, D Loss : 3.793 | G Loss : 9.318\n",
            "Batch : 859, D Loss : 5.702 | G Loss : 11.952\n",
            "Batch : 860, D Loss : 5.304 | G Loss : 11.109\n",
            "Batch : 861, D Loss : 4.511 | G Loss : 9.911\n",
            "Batch : 862, D Loss : 3.833 | G Loss : 10.365\n",
            "Batch : 863, D Loss : 4.581 | G Loss : 9.894\n",
            "Batch : 864, D Loss : 4.534 | G Loss : 10.791\n",
            "Batch : 865, D Loss : 4.317 | G Loss : 11.176\n",
            "Batch : 866, D Loss : 3.914 | G Loss : 10.011\n",
            "Batch : 867, D Loss : 3.895 | G Loss : 11.641\n",
            "Batch : 868, D Loss : 3.948 | G Loss : 10.059\n",
            "Batch : 869, D Loss : 4.005 | G Loss : 9.802\n",
            "Batch : 870, D Loss : 4.006 | G Loss : 10.559\n",
            "Batch : 871, D Loss : 3.958 | G Loss : 9.611\n",
            "Batch : 872, D Loss : 3.787 | G Loss : 11.215\n",
            "Batch : 873, D Loss : 3.928 | G Loss : 11.862\n",
            "Batch : 874, D Loss : 3.869 | G Loss : 9.583\n",
            "Batch : 875, D Loss : 4.013 | G Loss : 9.259\n",
            "Batch : 876, D Loss : 3.874 | G Loss : 10.376\n",
            "Batch : 877, D Loss : 3.881 | G Loss : 10.690\n",
            "Batch : 878, D Loss : 3.840 | G Loss : 12.463\n",
            "Batch : 879, D Loss : 3.883 | G Loss : 9.638\n",
            "Batch : 880, D Loss : 3.866 | G Loss : 9.817\n",
            "Batch : 881, D Loss : 3.906 | G Loss : 11.381\n",
            "Batch : 882, D Loss : 3.927 | G Loss : 12.626\n",
            "Batch : 883, D Loss : 3.894 | G Loss : 9.431\n",
            "Batch : 884, D Loss : 3.856 | G Loss : 9.778\n",
            "Batch : 885, D Loss : 3.811 | G Loss : 8.594\n",
            "Batch : 886, D Loss : 3.773 | G Loss : 10.159\n",
            "Batch : 887, D Loss : 3.803 | G Loss : 10.452\n",
            "Batch : 888, D Loss : 3.845 | G Loss : 9.151\n",
            "Batch : 889, D Loss : 3.792 | G Loss : 9.335\n",
            "Batch : 890, D Loss : 3.939 | G Loss : 9.897\n",
            "Batch : 891, D Loss : 3.828 | G Loss : 10.936\n",
            "Batch : 892, D Loss : 3.854 | G Loss : 8.825\n",
            "Batch : 893, D Loss : 3.831 | G Loss : 10.224\n",
            "Batch : 894, D Loss : 3.924 | G Loss : 10.649\n",
            "Batch : 895, D Loss : 3.799 | G Loss : 9.015\n",
            "Batch : 896, D Loss : 3.953 | G Loss : 10.229\n",
            "Batch : 897, D Loss : 3.918 | G Loss : 10.379\n",
            "Batch : 898, D Loss : 3.857 | G Loss : 11.345\n",
            "Batch : 899, D Loss : 3.859 | G Loss : 9.573\n",
            "Batch : 900, D Loss : 3.730 | G Loss : 9.506\n",
            "Batch : 901, D Loss : 3.820 | G Loss : 8.987\n",
            "Batch : 902, D Loss : 3.846 | G Loss : 10.109\n",
            "Batch : 903, D Loss : 3.881 | G Loss : 10.707\n",
            "Batch : 904, D Loss : 3.811 | G Loss : 10.440\n",
            "Batch : 905, D Loss : 3.882 | G Loss : 9.832\n",
            "Batch : 906, D Loss : 3.854 | G Loss : 10.358\n",
            "Batch : 907, D Loss : 3.832 | G Loss : 11.531\n",
            "Batch : 908, D Loss : 3.894 | G Loss : 10.977\n",
            "Batch : 909, D Loss : 3.848 | G Loss : 8.657\n",
            "Batch : 910, D Loss : 3.831 | G Loss : 9.884\n",
            "Batch : 911, D Loss : 3.855 | G Loss : 9.229\n",
            "Batch : 912, D Loss : 3.931 | G Loss : 10.533\n",
            "Batch : 913, D Loss : 4.030 | G Loss : 11.098\n",
            "Batch : 914, D Loss : 3.882 | G Loss : 9.193\n",
            "Batch : 915, D Loss : 3.935 | G Loss : 10.305\n",
            "Batch : 916, D Loss : 3.893 | G Loss : 9.236\n",
            "Batch : 917, D Loss : 3.813 | G Loss : 9.785\n",
            "Batch : 918, D Loss : 3.921 | G Loss : 9.214\n",
            "Batch : 919, D Loss : 3.854 | G Loss : 9.323\n",
            "Batch : 920, D Loss : 3.732 | G Loss : 9.673\n",
            "Batch : 921, D Loss : 3.838 | G Loss : 10.694\n",
            "Batch : 922, D Loss : 3.891 | G Loss : 11.076\n",
            "Batch : 923, D Loss : 3.850 | G Loss : 9.622\n",
            "Batch : 924, D Loss : 3.922 | G Loss : 9.864\n",
            "Batch : 925, D Loss : 3.909 | G Loss : 10.195\n",
            "Batch : 926, D Loss : 3.823 | G Loss : 9.169\n",
            "Batch : 927, D Loss : 3.825 | G Loss : 8.780\n",
            "Batch : 928, D Loss : 3.878 | G Loss : 10.022\n",
            "Batch : 929, D Loss : 3.872 | G Loss : 10.350\n",
            "Batch : 930, D Loss : 3.826 | G Loss : 9.357\n",
            "Batch : 931, D Loss : 3.921 | G Loss : 9.674\n",
            "Batch : 932, D Loss : 3.802 | G Loss : 10.312\n",
            "Batch : 933, D Loss : 3.840 | G Loss : 9.324\n",
            "Batch : 934, D Loss : 3.841 | G Loss : 9.112\n",
            "Batch : 935, D Loss : 3.778 | G Loss : 9.757\n",
            "Batch : 936, D Loss : 3.839 | G Loss : 8.133\n",
            "Batch : 937, D Loss : 3.932 | G Loss : 10.976\n",
            "Batch : 938, D Loss : 3.926 | G Loss : 9.403\n",
            "Batch : 939, D Loss : 3.802 | G Loss : 10.523\n",
            "Batch : 940, D Loss : 4.148 | G Loss : 10.560\n",
            "Batch : 941, D Loss : 4.192 | G Loss : 9.540\n",
            "Batch : 942, D Loss : 3.984 | G Loss : 8.808\n",
            "Batch : 943, D Loss : 3.984 | G Loss : 9.596\n",
            "Batch : 944, D Loss : 3.768 | G Loss : 10.414\n",
            "Batch : 945, D Loss : 3.827 | G Loss : 10.744\n",
            "Batch : 946, D Loss : 3.745 | G Loss : 9.203\n",
            "Batch : 947, D Loss : 3.812 | G Loss : 8.880\n",
            "Batch : 948, D Loss : 3.870 | G Loss : 9.130\n",
            "Batch : 949, D Loss : 3.780 | G Loss : 8.533\n",
            "Batch : 950, D Loss : 3.933 | G Loss : 8.764\n",
            "Batch : 951, D Loss : 3.953 | G Loss : 9.350\n",
            "Batch : 952, D Loss : 3.784 | G Loss : 10.206\n",
            "Batch : 953, D Loss : 3.860 | G Loss : 10.616\n",
            "Batch : 954, D Loss : 3.843 | G Loss : 9.618\n",
            "Batch : 955, D Loss : 3.835 | G Loss : 9.028\n",
            "Batch : 956, D Loss : 3.822 | G Loss : 9.765\n",
            "Batch : 957, D Loss : 3.773 | G Loss : 10.895\n",
            "Batch : 958, D Loss : 3.911 | G Loss : 9.183\n",
            "Batch : 959, D Loss : 3.839 | G Loss : 9.442\n",
            "Batch : 960, D Loss : 3.831 | G Loss : 8.773\n",
            "Batch : 961, D Loss : 3.772 | G Loss : 10.980\n",
            "Batch : 962, D Loss : 3.847 | G Loss : 9.172\n",
            "Batch : 963, D Loss : 3.952 | G Loss : 8.615\n",
            "Batch : 964, D Loss : 3.897 | G Loss : 9.571\n",
            "Batch : 965, D Loss : 3.865 | G Loss : 10.836\n",
            "Batch : 966, D Loss : 3.880 | G Loss : 9.526\n",
            "Batch : 967, D Loss : 3.824 | G Loss : 9.324\n",
            "Batch : 968, D Loss : 3.770 | G Loss : 8.661\n",
            "Batch : 969, D Loss : 3.933 | G Loss : 11.717\n",
            "Batch : 970, D Loss : 3.882 | G Loss : 10.400\n",
            "Batch : 971, D Loss : 3.942 | G Loss : 9.472\n",
            "Batch : 972, D Loss : 3.840 | G Loss : 10.624\n",
            "Batch : 973, D Loss : 3.857 | G Loss : 10.037\n",
            "Batch : 974, D Loss : 3.910 | G Loss : 9.116\n",
            "Batch : 975, D Loss : 3.980 | G Loss : 9.959\n",
            "Batch : 976, D Loss : 3.938 | G Loss : 9.905\n",
            "Batch : 977, D Loss : 3.854 | G Loss : 11.000\n",
            "Batch : 978, D Loss : 3.749 | G Loss : 9.946\n",
            "Batch : 979, D Loss : 4.641 | G Loss : 9.927\n",
            "Batch : 980, D Loss : 4.652 | G Loss : 10.549\n",
            "Batch : 981, D Loss : 4.365 | G Loss : 9.075\n",
            "Batch : 982, D Loss : 4.233 | G Loss : 11.304\n",
            "Batch : 983, D Loss : 4.645 | G Loss : 11.506\n",
            "Batch : 984, D Loss : 4.558 | G Loss : 9.941\n",
            "Batch : 985, D Loss : 4.101 | G Loss : 10.623\n",
            "Batch : 986, D Loss : 4.012 | G Loss : 8.869\n",
            "Batch : 987, D Loss : 4.166 | G Loss : 10.059\n",
            "Batch : 988, D Loss : 4.202 | G Loss : 10.576\n",
            "Batch : 989, D Loss : 4.285 | G Loss : 9.923\n",
            "Batch : 990, D Loss : 4.172 | G Loss : 9.719\n",
            "Batch : 991, D Loss : 4.024 | G Loss : 9.025\n",
            "Batch : 992, D Loss : 4.106 | G Loss : 10.124\n",
            "Batch : 993, D Loss : 4.045 | G Loss : 9.235\n",
            "Batch : 994, D Loss : 4.088 | G Loss : 9.377\n",
            "Batch : 995, D Loss : 4.131 | G Loss : 9.348\n",
            "Batch : 996, D Loss : 4.000 | G Loss : 9.864\n",
            "Batch : 997, D Loss : 4.006 | G Loss : 9.100\n",
            "Batch : 998, D Loss : 3.915 | G Loss : 10.074\n",
            "Batch : 999, D Loss : 4.170 | G Loss : 9.813\n",
            "Batch : 1000, D Loss : 4.081 | G Loss : 10.908\n",
            "Batch : 1001, D Loss : 4.007 | G Loss : 9.695\n",
            "Batch : 1002, D Loss : 4.089 | G Loss : 10.507\n",
            "Batch : 1003, D Loss : 4.125 | G Loss : 9.261\n",
            "Batch : 1004, D Loss : 4.060 | G Loss : 9.604\n",
            "Batch : 1005, D Loss : 3.958 | G Loss : 11.383\n",
            "Batch : 1006, D Loss : 3.916 | G Loss : 9.684\n",
            "Batch : 1007, D Loss : 4.107 | G Loss : 9.681\n",
            "Batch : 1008, D Loss : 3.986 | G Loss : 10.742\n",
            "Batch : 1009, D Loss : 4.424 | G Loss : 10.946\n",
            "Batch : 1010, D Loss : 4.491 | G Loss : 10.896\n",
            "Batch : 1011, D Loss : 4.109 | G Loss : 10.795\n",
            "Batch : 1012, D Loss : 4.082 | G Loss : 11.160\n",
            "Batch : 1013, D Loss : 4.162 | G Loss : 10.595\n",
            "Batch : 1014, D Loss : 4.252 | G Loss : 10.492\n",
            "Batch : 1015, D Loss : 4.129 | G Loss : 10.759\n",
            "Batch : 1016, D Loss : 4.124 | G Loss : 10.037\n",
            "Batch : 1017, D Loss : 4.138 | G Loss : 10.088\n",
            "Batch : 1018, D Loss : 4.198 | G Loss : 10.695\n",
            "Batch : 1019, D Loss : 3.950 | G Loss : 10.741\n",
            "Batch : 1020, D Loss : 4.077 | G Loss : 10.531\n",
            "Batch : 1021, D Loss : 3.982 | G Loss : 9.648\n",
            "Batch : 1022, D Loss : 4.033 | G Loss : 10.215\n",
            "Batch : 1023, D Loss : 4.126 | G Loss : 10.988\n",
            "Batch : 1024, D Loss : 3.968 | G Loss : 10.950\n",
            "Batch : 1025, D Loss : 4.173 | G Loss : 10.153\n",
            "Batch : 1026, D Loss : 4.080 | G Loss : 10.596\n",
            "Batch : 1027, D Loss : 3.972 | G Loss : 10.475\n",
            "Batch : 1028, D Loss : 3.902 | G Loss : 11.146\n",
            "Batch : 1029, D Loss : 4.106 | G Loss : 10.936\n",
            "Batch : 1030, D Loss : 3.938 | G Loss : 11.334\n",
            "Batch : 1031, D Loss : 3.881 | G Loss : 9.661\n",
            "Batch : 1032, D Loss : 3.842 | G Loss : 11.820\n",
            "Batch : 1033, D Loss : 4.033 | G Loss : 9.140\n",
            "Batch : 1034, D Loss : 4.104 | G Loss : 9.796\n",
            "Batch : 1035, D Loss : 4.183 | G Loss : 9.293\n",
            "Batch : 1036, D Loss : 3.931 | G Loss : 10.317\n",
            "Batch : 1037, D Loss : 3.776 | G Loss : 9.623\n",
            "Batch : 1038, D Loss : 3.945 | G Loss : 8.938\n",
            "Batch : 1039, D Loss : 3.966 | G Loss : 10.375\n",
            "Batch : 1040, D Loss : 3.809 | G Loss : 8.871\n",
            "Batch : 1041, D Loss : 3.892 | G Loss : 9.951\n",
            "Batch : 1042, D Loss : 3.943 | G Loss : 10.420\n",
            "Batch : 1043, D Loss : 3.876 | G Loss : 8.826\n",
            "Batch : 1044, D Loss : 3.817 | G Loss : 10.714\n",
            "Batch : 1045, D Loss : 3.981 | G Loss : 9.911\n",
            "Batch : 1046, D Loss : 3.986 | G Loss : 9.873\n",
            "Batch : 1047, D Loss : 3.853 | G Loss : 11.723\n",
            "Batch : 1048, D Loss : 3.908 | G Loss : 11.531\n",
            "Batch : 1049, D Loss : 4.067 | G Loss : 11.277\n",
            "Batch : 1050, D Loss : 3.961 | G Loss : 11.684\n",
            "Batch : 1051, D Loss : 3.876 | G Loss : 10.308\n",
            "Batch : 1052, D Loss : 3.871 | G Loss : 9.623\n",
            "Batch : 1053, D Loss : 3.967 | G Loss : 10.127\n",
            "Batch : 1054, D Loss : 3.852 | G Loss : 10.421\n",
            "Batch : 1055, D Loss : 3.853 | G Loss : 10.090\n",
            "Batch : 1056, D Loss : 3.922 | G Loss : 9.067\n",
            "Batch : 1057, D Loss : 3.793 | G Loss : 9.627\n",
            "Batch : 1058, D Loss : 3.904 | G Loss : 9.164\n",
            "Batch : 1059, D Loss : 3.792 | G Loss : 10.251\n",
            "Batch : 1060, D Loss : 3.997 | G Loss : 9.736\n",
            "Batch : 1061, D Loss : 3.892 | G Loss : 9.224\n",
            "Batch : 1062, D Loss : 3.900 | G Loss : 9.888\n",
            "Batch : 1063, D Loss : 3.895 | G Loss : 11.000\n",
            "Batch : 1064, D Loss : 3.866 | G Loss : 8.950\n",
            "Batch : 1065, D Loss : 3.840 | G Loss : 9.720\n",
            "Batch : 1066, D Loss : 3.874 | G Loss : 10.334\n",
            "Batch : 1067, D Loss : 3.867 | G Loss : 8.332\n",
            "Batch : 1068, D Loss : 3.866 | G Loss : 10.420\n",
            "Batch : 1069, D Loss : 3.909 | G Loss : 10.738\n",
            "Batch : 1070, D Loss : 4.035 | G Loss : 9.414\n",
            "Batch : 1071, D Loss : 3.890 | G Loss : 8.928\n",
            "Batch : 1072, D Loss : 3.832 | G Loss : 9.628\n",
            "Batch : 1073, D Loss : 3.898 | G Loss : 10.926\n",
            "Batch : 1074, D Loss : 3.805 | G Loss : 11.667\n",
            "Batch : 1075, D Loss : 3.922 | G Loss : 9.049\n",
            "Batch : 1076, D Loss : 3.970 | G Loss : 8.933\n",
            "Batch : 1077, D Loss : 3.827 | G Loss : 9.486\n",
            "Batch : 1078, D Loss : 3.906 | G Loss : 9.418\n",
            "Batch : 1079, D Loss : 3.881 | G Loss : 10.176\n",
            "Batch : 1080, D Loss : 3.913 | G Loss : 12.083\n",
            "Batch : 1081, D Loss : 3.878 | G Loss : 9.859\n",
            "Batch : 1082, D Loss : 3.869 | G Loss : 10.607\n",
            "Batch : 1083, D Loss : 3.856 | G Loss : 10.600\n",
            "Batch : 1084, D Loss : 3.981 | G Loss : 9.443\n",
            "Batch : 1085, D Loss : 4.277 | G Loss : 10.766\n",
            "Batch : 1086, D Loss : 4.011 | G Loss : 9.666\n",
            "Batch : 1087, D Loss : 3.866 | G Loss : 11.530\n",
            "Batch : 1088, D Loss : 3.820 | G Loss : 10.190\n",
            "Batch : 1089, D Loss : 3.901 | G Loss : 10.246\n",
            "Batch : 1090, D Loss : 3.856 | G Loss : 9.085\n",
            "Batch : 1091, D Loss : 3.911 | G Loss : 9.759\n",
            "Batch : 1092, D Loss : 3.763 | G Loss : 10.048\n",
            "Batch : 1093, D Loss : 3.957 | G Loss : 9.397\n",
            "Batch : 1094, D Loss : 3.805 | G Loss : 10.739\n",
            "Batch : 1095, D Loss : 3.913 | G Loss : 9.519\n",
            "Batch : 1096, D Loss : 3.893 | G Loss : 10.176\n",
            "Batch : 1097, D Loss : 3.820 | G Loss : 10.915\n",
            "Batch : 1098, D Loss : 3.771 | G Loss : 9.175\n",
            "Batch : 1099, D Loss : 3.948 | G Loss : 8.935\n",
            "Batch : 1100, D Loss : 3.839 | G Loss : 9.504\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            ">Saved: plot_000003.png and g_model & d_model\n",
            " ========== Epoch 4 ========== \n",
            "Batch : 1, D Loss : 3.963 | G Loss : 11.118\n",
            "Batch : 2, D Loss : 3.831 | G Loss : 10.147\n",
            "Batch : 3, D Loss : 3.896 | G Loss : 10.292\n",
            "Batch : 4, D Loss : 3.954 | G Loss : 10.041\n",
            "Batch : 5, D Loss : 3.933 | G Loss : 10.535\n",
            "Batch : 6, D Loss : 3.936 | G Loss : 9.095\n",
            "Batch : 7, D Loss : 3.773 | G Loss : 9.765\n",
            "Batch : 8, D Loss : 3.889 | G Loss : 10.303\n",
            "Batch : 9, D Loss : 3.962 | G Loss : 11.237\n",
            "Batch : 10, D Loss : 4.049 | G Loss : 9.897\n",
            "Batch : 11, D Loss : 3.922 | G Loss : 10.071\n",
            "Batch : 12, D Loss : 3.859 | G Loss : 8.940\n",
            "Batch : 13, D Loss : 3.818 | G Loss : 9.060\n",
            "Batch : 14, D Loss : 3.822 | G Loss : 9.574\n",
            "Batch : 15, D Loss : 3.874 | G Loss : 10.023\n",
            "Batch : 16, D Loss : 3.828 | G Loss : 9.010\n",
            "Batch : 17, D Loss : 3.834 | G Loss : 9.016\n",
            "Batch : 18, D Loss : 3.880 | G Loss : 9.243\n",
            "Batch : 19, D Loss : 3.936 | G Loss : 9.037\n",
            "Batch : 20, D Loss : 3.897 | G Loss : 9.287\n",
            "Batch : 21, D Loss : 3.878 | G Loss : 10.299\n",
            "Batch : 22, D Loss : 3.888 | G Loss : 9.705\n",
            "Batch : 23, D Loss : 4.004 | G Loss : 10.007\n",
            "Batch : 24, D Loss : 4.053 | G Loss : 9.200\n",
            "Batch : 25, D Loss : 3.882 | G Loss : 8.697\n",
            "Batch : 26, D Loss : 3.886 | G Loss : 8.873\n",
            "Batch : 27, D Loss : 3.858 | G Loss : 9.137\n",
            "Batch : 28, D Loss : 3.988 | G Loss : 10.166\n",
            "Batch : 29, D Loss : 3.920 | G Loss : 9.384\n",
            "Batch : 30, D Loss : 3.870 | G Loss : 9.475\n",
            "Batch : 31, D Loss : 3.886 | G Loss : 9.573\n",
            "Batch : 32, D Loss : 3.942 | G Loss : 9.790\n",
            "Batch : 33, D Loss : 3.913 | G Loss : 10.972\n",
            "Batch : 34, D Loss : 3.890 | G Loss : 9.102\n",
            "Batch : 35, D Loss : 3.864 | G Loss : 8.100\n",
            "Batch : 36, D Loss : 5.267 | G Loss : 11.611\n",
            "Batch : 37, D Loss : 5.393 | G Loss : 10.288\n",
            "Batch : 38, D Loss : 4.337 | G Loss : 10.548\n",
            "Batch : 39, D Loss : 3.819 | G Loss : 10.965\n",
            "Batch : 40, D Loss : 4.241 | G Loss : 12.454\n",
            "Batch : 41, D Loss : 4.393 | G Loss : 10.575\n",
            "Batch : 42, D Loss : 4.049 | G Loss : 9.523\n",
            "Batch : 43, D Loss : 4.010 | G Loss : 10.029\n",
            "Batch : 44, D Loss : 4.004 | G Loss : 10.130\n",
            "Batch : 45, D Loss : 3.986 | G Loss : 9.598\n",
            "Batch : 46, D Loss : 3.915 | G Loss : 10.856\n",
            "Batch : 47, D Loss : 4.083 | G Loss : 9.373\n",
            "Batch : 48, D Loss : 4.065 | G Loss : 10.163\n",
            "Batch : 49, D Loss : 3.840 | G Loss : 8.743\n",
            "Batch : 50, D Loss : 3.876 | G Loss : 8.263\n",
            "Batch : 51, D Loss : 3.845 | G Loss : 9.768\n",
            "Batch : 52, D Loss : 3.856 | G Loss : 10.340\n",
            "Batch : 53, D Loss : 3.827 | G Loss : 9.934\n",
            "Batch : 54, D Loss : 3.855 | G Loss : 11.076\n",
            "Batch : 55, D Loss : 4.377 | G Loss : 11.910\n",
            "Batch : 56, D Loss : 4.408 | G Loss : 10.812\n",
            "Batch : 57, D Loss : 4.357 | G Loss : 10.418\n",
            "Batch : 58, D Loss : 4.165 | G Loss : 9.153\n",
            "Batch : 59, D Loss : 4.243 | G Loss : 9.733\n",
            "Batch : 60, D Loss : 3.758 | G Loss : 11.765\n",
            "Batch : 61, D Loss : 4.203 | G Loss : 9.122\n",
            "Batch : 62, D Loss : 4.315 | G Loss : 9.589\n",
            "Batch : 63, D Loss : 4.272 | G Loss : 10.105\n",
            "Batch : 64, D Loss : 4.309 | G Loss : 9.491\n",
            "Batch : 65, D Loss : 3.959 | G Loss : 10.260\n",
            "Batch : 66, D Loss : 4.004 | G Loss : 10.390\n",
            "Batch : 67, D Loss : 4.049 | G Loss : 10.574\n",
            "Batch : 68, D Loss : 3.867 | G Loss : 9.157\n",
            "Batch : 69, D Loss : 3.792 | G Loss : 10.320\n",
            "Batch : 70, D Loss : 3.891 | G Loss : 8.511\n",
            "Batch : 71, D Loss : 3.835 | G Loss : 10.188\n",
            "Batch : 72, D Loss : 3.811 | G Loss : 9.524\n",
            "Batch : 73, D Loss : 3.802 | G Loss : 11.396\n",
            "Batch : 74, D Loss : 3.889 | G Loss : 8.408\n",
            "Batch : 75, D Loss : 3.897 | G Loss : 10.798\n",
            "Batch : 76, D Loss : 3.902 | G Loss : 10.202\n",
            "Batch : 77, D Loss : 3.825 | G Loss : 9.777\n",
            "Batch : 78, D Loss : 3.986 | G Loss : 8.969\n",
            "Batch : 79, D Loss : 3.894 | G Loss : 9.877\n",
            "Batch : 80, D Loss : 3.784 | G Loss : 10.091\n",
            "Batch : 81, D Loss : 3.939 | G Loss : 9.845\n",
            "Batch : 82, D Loss : 3.974 | G Loss : 11.270\n",
            "Batch : 83, D Loss : 3.830 | G Loss : 9.933\n",
            "Batch : 84, D Loss : 3.964 | G Loss : 10.763\n",
            "Batch : 85, D Loss : 3.895 | G Loss : 8.688\n",
            "Batch : 86, D Loss : 3.874 | G Loss : 9.352\n",
            "Batch : 87, D Loss : 3.870 | G Loss : 9.356\n",
            "Batch : 88, D Loss : 3.818 | G Loss : 8.855\n",
            "Batch : 89, D Loss : 4.042 | G Loss : 9.442\n",
            "Batch : 90, D Loss : 3.923 | G Loss : 11.057\n",
            "Batch : 91, D Loss : 3.793 | G Loss : 8.745\n",
            "Batch : 92, D Loss : 3.741 | G Loss : 10.165\n",
            "Batch : 93, D Loss : 3.992 | G Loss : 9.828\n",
            "Batch : 94, D Loss : 3.840 | G Loss : 8.837\n",
            "Batch : 95, D Loss : 3.834 | G Loss : 10.445\n",
            "Batch : 96, D Loss : 3.757 | G Loss : 9.739\n",
            "Batch : 97, D Loss : 3.855 | G Loss : 9.195\n",
            "Batch : 98, D Loss : 4.128 | G Loss : 10.329\n",
            "Batch : 99, D Loss : 4.013 | G Loss : 8.926\n",
            "Batch : 100, D Loss : 3.836 | G Loss : 9.458\n",
            "Batch : 101, D Loss : 4.046 | G Loss : 9.970\n",
            "Batch : 102, D Loss : 3.923 | G Loss : 10.055\n",
            "Batch : 103, D Loss : 3.775 | G Loss : 9.943\n",
            "Batch : 104, D Loss : 3.887 | G Loss : 8.949\n",
            "Batch : 105, D Loss : 3.915 | G Loss : 11.235\n",
            "Batch : 106, D Loss : 3.877 | G Loss : 9.399\n",
            "Batch : 107, D Loss : 3.841 | G Loss : 10.953\n",
            "Batch : 108, D Loss : 3.845 | G Loss : 10.232\n",
            "Batch : 109, D Loss : 3.796 | G Loss : 8.757\n",
            "Batch : 110, D Loss : 3.948 | G Loss : 9.278\n",
            "Batch : 111, D Loss : 3.758 | G Loss : 9.737\n",
            "Batch : 112, D Loss : 3.783 | G Loss : 9.557\n",
            "Batch : 113, D Loss : 3.871 | G Loss : 9.921\n",
            "Batch : 114, D Loss : 3.860 | G Loss : 9.613\n",
            "Batch : 115, D Loss : 3.912 | G Loss : 9.538\n",
            "Batch : 116, D Loss : 3.740 | G Loss : 8.840\n",
            "Batch : 117, D Loss : 3.810 | G Loss : 9.434\n",
            "Batch : 118, D Loss : 3.917 | G Loss : 9.509\n",
            "Batch : 119, D Loss : 3.844 | G Loss : 9.253\n",
            "Batch : 120, D Loss : 3.737 | G Loss : 10.865\n",
            "Batch : 121, D Loss : 3.945 | G Loss : 9.153\n",
            "Batch : 122, D Loss : 3.928 | G Loss : 9.010\n",
            "Batch : 123, D Loss : 3.878 | G Loss : 9.858\n",
            "Batch : 124, D Loss : 3.952 | G Loss : 10.041\n",
            "Batch : 125, D Loss : 3.832 | G Loss : 9.503\n",
            "Batch : 126, D Loss : 3.827 | G Loss : 8.804\n",
            "Batch : 127, D Loss : 3.754 | G Loss : 8.903\n",
            "Batch : 128, D Loss : 4.053 | G Loss : 8.665\n",
            "Batch : 129, D Loss : 3.942 | G Loss : 9.684\n",
            "Batch : 130, D Loss : 3.791 | G Loss : 9.019\n",
            "Batch : 131, D Loss : 3.843 | G Loss : 9.372\n",
            "Batch : 132, D Loss : 3.812 | G Loss : 9.386\n",
            "Batch : 133, D Loss : 3.895 | G Loss : 9.167\n",
            "Batch : 134, D Loss : 3.970 | G Loss : 9.858\n",
            "Batch : 135, D Loss : 3.911 | G Loss : 8.420\n",
            "Batch : 136, D Loss : 3.880 | G Loss : 9.700\n",
            "Batch : 137, D Loss : 3.862 | G Loss : 9.866\n",
            "Batch : 138, D Loss : 3.920 | G Loss : 8.863\n",
            "Batch : 139, D Loss : 4.185 | G Loss : 10.893\n",
            "Batch : 140, D Loss : 4.299 | G Loss : 11.970\n",
            "Batch : 141, D Loss : 4.008 | G Loss : 9.945\n",
            "Batch : 142, D Loss : 6.338 | G Loss : 14.162\n",
            "Batch : 143, D Loss : 6.475 | G Loss : 15.950\n",
            "Batch : 144, D Loss : 5.718 | G Loss : 13.358\n",
            "Batch : 145, D Loss : 5.393 | G Loss : 13.431\n",
            "Batch : 146, D Loss : 6.183 | G Loss : 13.247\n",
            "Batch : 147, D Loss : 6.258 | G Loss : 14.684\n",
            "Batch : 148, D Loss : 5.838 | G Loss : 12.153\n",
            "Batch : 149, D Loss : 5.131 | G Loss : 11.688\n",
            "Batch : 150, D Loss : 6.334 | G Loss : 14.476\n",
            "Batch : 151, D Loss : 6.375 | G Loss : 14.405\n",
            "Batch : 152, D Loss : 6.204 | G Loss : 13.167\n",
            "Batch : 153, D Loss : 6.046 | G Loss : 13.825\n",
            "Batch : 154, D Loss : 6.205 | G Loss : 13.157\n",
            "Batch : 155, D Loss : 5.920 | G Loss : 13.889\n",
            "Batch : 156, D Loss : 5.654 | G Loss : 10.988\n",
            "Batch : 157, D Loss : 5.033 | G Loss : 11.343\n",
            "Batch : 158, D Loss : 4.711 | G Loss : 11.239\n",
            "Batch : 159, D Loss : 4.607 | G Loss : 12.529\n",
            "Batch : 160, D Loss : 4.854 | G Loss : 11.010\n",
            "Batch : 161, D Loss : 4.756 | G Loss : 10.490\n",
            "Batch : 162, D Loss : 4.358 | G Loss : 10.238\n",
            "Batch : 163, D Loss : 4.203 | G Loss : 11.720\n",
            "Batch : 164, D Loss : 4.462 | G Loss : 9.534\n",
            "Batch : 165, D Loss : 4.271 | G Loss : 10.482\n",
            "Batch : 166, D Loss : 4.284 | G Loss : 10.879\n",
            "Batch : 167, D Loss : 4.392 | G Loss : 9.973\n",
            "Batch : 168, D Loss : 4.260 | G Loss : 9.852\n",
            "Batch : 169, D Loss : 4.102 | G Loss : 10.394\n",
            "Batch : 170, D Loss : 4.035 | G Loss : 10.312\n",
            "Batch : 171, D Loss : 4.192 | G Loss : 11.070\n",
            "Batch : 172, D Loss : 4.295 | G Loss : 10.580\n",
            "Batch : 173, D Loss : 4.158 | G Loss : 10.133\n",
            "Batch : 174, D Loss : 3.964 | G Loss : 9.173\n",
            "Batch : 175, D Loss : 4.192 | G Loss : 10.465\n",
            "Batch : 176, D Loss : 4.036 | G Loss : 10.154\n",
            "Batch : 177, D Loss : 4.074 | G Loss : 10.979\n",
            "Batch : 178, D Loss : 3.945 | G Loss : 10.728\n",
            "Batch : 179, D Loss : 4.376 | G Loss : 8.694\n",
            "Batch : 180, D Loss : 4.220 | G Loss : 10.908\n",
            "Batch : 181, D Loss : 4.050 | G Loss : 11.411\n",
            "Batch : 182, D Loss : 6.183 | G Loss : 18.261\n",
            "Batch : 183, D Loss : 6.430 | G Loss : 17.384\n",
            "Batch : 184, D Loss : 6.190 | G Loss : 15.913\n",
            "Batch : 185, D Loss : 6.058 | G Loss : 15.946\n",
            "Batch : 186, D Loss : 5.313 | G Loss : 17.512\n",
            "Batch : 187, D Loss : 5.388 | G Loss : 14.491\n",
            "Batch : 188, D Loss : 5.115 | G Loss : 14.727\n",
            "Batch : 189, D Loss : 4.684 | G Loss : 14.900\n",
            "Batch : 190, D Loss : 4.828 | G Loss : 14.329\n",
            "Batch : 191, D Loss : 5.155 | G Loss : 14.371\n",
            "Batch : 192, D Loss : 4.008 | G Loss : 17.703\n",
            "Batch : 193, D Loss : 6.426 | G Loss : 14.763\n",
            "Batch : 194, D Loss : 6.505 | G Loss : 15.173\n",
            "Batch : 195, D Loss : 6.200 | G Loss : 16.745\n",
            "Batch : 196, D Loss : 6.201 | G Loss : 15.463\n",
            "Batch : 197, D Loss : 5.690 | G Loss : 13.543\n",
            "Batch : 198, D Loss : 4.648 | G Loss : 14.157\n",
            "Batch : 199, D Loss : 5.840 | G Loss : 12.942\n",
            "Batch : 200, D Loss : 4.993 | G Loss : 13.206\n",
            "Batch : 201, D Loss : 3.947 | G Loss : 14.110\n",
            "Batch : 202, D Loss : 6.387 | G Loss : 16.396\n",
            "Batch : 203, D Loss : 6.600 | G Loss : 17.089\n",
            "Batch : 204, D Loss : 6.491 | G Loss : 19.134\n",
            "Batch : 205, D Loss : 6.549 | G Loss : 15.595\n",
            "Batch : 206, D Loss : 6.289 | G Loss : 15.727\n",
            "Batch : 207, D Loss : 6.499 | G Loss : 15.034\n",
            "Batch : 208, D Loss : 5.798 | G Loss : 13.012\n",
            "Batch : 209, D Loss : 5.524 | G Loss : 13.181\n",
            "Batch : 210, D Loss : 4.838 | G Loss : 13.438\n",
            "Batch : 211, D Loss : 4.637 | G Loss : 14.598\n",
            "Batch : 212, D Loss : 6.350 | G Loss : 15.568\n",
            "Batch : 213, D Loss : 6.409 | G Loss : 15.818\n",
            "Batch : 214, D Loss : 6.376 | G Loss : 16.896\n",
            "Batch : 215, D Loss : 6.253 | G Loss : 15.908\n",
            "Batch : 216, D Loss : 6.270 | G Loss : 15.158\n",
            "Batch : 217, D Loss : 6.479 | G Loss : 15.059\n",
            "Batch : 218, D Loss : 6.339 | G Loss : 16.949\n",
            "Batch : 219, D Loss : 5.798 | G Loss : 14.574\n",
            "Batch : 220, D Loss : 5.717 | G Loss : 13.741\n",
            "Batch : 221, D Loss : 5.120 | G Loss : 14.259\n",
            "Batch : 222, D Loss : 4.866 | G Loss : 13.238\n",
            "Batch : 223, D Loss : 5.864 | G Loss : 13.918\n",
            "Batch : 224, D Loss : 5.031 | G Loss : 13.353\n",
            "Batch : 225, D Loss : 4.195 | G Loss : 12.350\n",
            "Batch : 226, D Loss : 4.734 | G Loss : 12.675\n",
            "Batch : 227, D Loss : 4.211 | G Loss : 13.654\n",
            "Batch : 228, D Loss : 5.880 | G Loss : 11.986\n",
            "Batch : 229, D Loss : 5.485 | G Loss : 11.623\n",
            "Batch : 230, D Loss : 4.565 | G Loss : 11.593\n",
            "Batch : 231, D Loss : 4.241 | G Loss : 12.399\n",
            "Batch : 232, D Loss : 4.543 | G Loss : 10.505\n",
            "Batch : 233, D Loss : 4.053 | G Loss : 11.020\n",
            "Batch : 234, D Loss : 4.477 | G Loss : 12.359\n",
            "Batch : 235, D Loss : 4.215 | G Loss : 12.666\n",
            "Batch : 236, D Loss : 3.991 | G Loss : 9.842\n",
            "Batch : 237, D Loss : 4.587 | G Loss : 11.347\n",
            "Batch : 238, D Loss : 4.206 | G Loss : 11.358\n",
            "Batch : 239, D Loss : 3.827 | G Loss : 11.736\n",
            "Batch : 240, D Loss : 6.271 | G Loss : 13.474\n",
            "Batch : 241, D Loss : 6.183 | G Loss : 15.312\n",
            "Batch : 242, D Loss : 6.136 | G Loss : 12.887\n",
            "Batch : 243, D Loss : 5.597 | G Loss : 12.522\n",
            "Batch : 244, D Loss : 5.075 | G Loss : 13.918\n",
            "Batch : 245, D Loss : 4.720 | G Loss : 10.766\n",
            "Batch : 246, D Loss : 4.641 | G Loss : 10.760\n",
            "Batch : 247, D Loss : 4.492 | G Loss : 12.318\n",
            "Batch : 248, D Loss : 4.344 | G Loss : 11.193\n",
            "Batch : 249, D Loss : 4.141 | G Loss : 11.677\n",
            "Batch : 250, D Loss : 5.042 | G Loss : 12.042\n",
            "Batch : 251, D Loss : 5.025 | G Loss : 10.370\n",
            "Batch : 252, D Loss : 4.428 | G Loss : 12.206\n",
            "Batch : 253, D Loss : 3.942 | G Loss : 11.521\n",
            "Batch : 254, D Loss : 3.930 | G Loss : 13.605\n",
            "Batch : 255, D Loss : 4.220 | G Loss : 10.201\n",
            "Batch : 256, D Loss : 4.177 | G Loss : 10.805\n",
            "Batch : 257, D Loss : 4.090 | G Loss : 11.857\n",
            "Batch : 258, D Loss : 3.850 | G Loss : 10.971\n",
            "Batch : 259, D Loss : 3.979 | G Loss : 11.540\n",
            "Batch : 260, D Loss : 3.985 | G Loss : 10.591\n",
            "Batch : 261, D Loss : 4.014 | G Loss : 10.834\n",
            "Batch : 262, D Loss : 4.063 | G Loss : 10.955\n",
            "Batch : 263, D Loss : 4.001 | G Loss : 10.651\n",
            "Batch : 264, D Loss : 3.959 | G Loss : 10.252\n",
            "Batch : 265, D Loss : 3.966 | G Loss : 12.214\n",
            "Batch : 266, D Loss : 4.012 | G Loss : 12.069\n",
            "Batch : 267, D Loss : 4.010 | G Loss : 10.480\n",
            "Batch : 268, D Loss : 3.991 | G Loss : 10.442\n",
            "Batch : 269, D Loss : 3.978 | G Loss : 11.043\n",
            "Batch : 270, D Loss : 3.869 | G Loss : 9.105\n",
            "Batch : 271, D Loss : 4.055 | G Loss : 10.675\n",
            "Batch : 272, D Loss : 3.984 | G Loss : 12.275\n",
            "Batch : 273, D Loss : 4.009 | G Loss : 11.683\n",
            "Batch : 274, D Loss : 3.921 | G Loss : 10.193\n",
            "Batch : 275, D Loss : 3.855 | G Loss : 11.043\n",
            "Batch : 276, D Loss : 4.171 | G Loss : 11.530\n",
            "Batch : 277, D Loss : 3.998 | G Loss : 16.763\n",
            "Batch : 278, D Loss : 5.691 | G Loss : 14.257\n",
            "Batch : 279, D Loss : 5.233 | G Loss : 13.924\n",
            "Batch : 280, D Loss : 4.627 | G Loss : 13.713\n",
            "Batch : 281, D Loss : 4.385 | G Loss : 14.779\n",
            "Batch : 282, D Loss : 4.396 | G Loss : 13.215\n",
            "Batch : 283, D Loss : 4.151 | G Loss : 12.351\n",
            "Batch : 284, D Loss : 4.212 | G Loss : 11.226\n",
            "Batch : 285, D Loss : 3.865 | G Loss : 11.102\n",
            "Batch : 286, D Loss : 5.326 | G Loss : 13.038\n",
            "Batch : 287, D Loss : 5.061 | G Loss : 11.011\n",
            "Batch : 288, D Loss : 4.231 | G Loss : 12.165\n",
            "Batch : 289, D Loss : 3.974 | G Loss : 12.612\n",
            "Batch : 290, D Loss : 5.229 | G Loss : 11.778\n",
            "Batch : 291, D Loss : 5.040 | G Loss : 12.528\n",
            "Batch : 292, D Loss : 4.111 | G Loss : 10.980\n",
            "Batch : 293, D Loss : 3.792 | G Loss : 10.415\n",
            "Batch : 294, D Loss : 4.410 | G Loss : 12.237\n",
            "Batch : 295, D Loss : 4.462 | G Loss : 10.961\n",
            "Batch : 296, D Loss : 4.212 | G Loss : 11.162\n",
            "Batch : 297, D Loss : 3.925 | G Loss : 11.619\n",
            "Batch : 298, D Loss : 4.924 | G Loss : 12.176\n",
            "Batch : 299, D Loss : 4.855 | G Loss : 12.154\n",
            "Batch : 300, D Loss : 4.196 | G Loss : 11.070\n",
            "Batch : 301, D Loss : 3.958 | G Loss : 12.892\n",
            "Batch : 302, D Loss : 4.255 | G Loss : 10.109\n",
            "Batch : 303, D Loss : 4.251 | G Loss : 11.345\n",
            "Batch : 304, D Loss : 4.058 | G Loss : 10.403\n",
            "Batch : 305, D Loss : 3.874 | G Loss : 10.401\n",
            "Batch : 306, D Loss : 5.170 | G Loss : 12.175\n",
            "Batch : 307, D Loss : 4.820 | G Loss : 12.055\n",
            "Batch : 308, D Loss : 4.245 | G Loss : 11.507\n",
            "Batch : 309, D Loss : 4.031 | G Loss : 10.876\n",
            "Batch : 310, D Loss : 4.122 | G Loss : 11.008\n",
            "Batch : 311, D Loss : 3.965 | G Loss : 9.890\n",
            "Batch : 312, D Loss : 3.896 | G Loss : 11.419\n",
            "Batch : 313, D Loss : 4.001 | G Loss : 9.998\n",
            "Batch : 314, D Loss : 4.033 | G Loss : 12.087\n",
            "Batch : 315, D Loss : 3.904 | G Loss : 10.814\n",
            "Batch : 316, D Loss : 3.892 | G Loss : 10.597\n",
            "Batch : 317, D Loss : 4.226 | G Loss : 9.655\n",
            "Batch : 318, D Loss : 4.100 | G Loss : 10.590\n",
            "Batch : 319, D Loss : 3.930 | G Loss : 11.106\n",
            "Batch : 320, D Loss : 3.761 | G Loss : 10.480\n",
            "Batch : 321, D Loss : 4.495 | G Loss : 11.069\n",
            "Batch : 322, D Loss : 4.468 | G Loss : 9.947\n",
            "Batch : 323, D Loss : 4.086 | G Loss : 10.926\n",
            "Batch : 324, D Loss : 3.839 | G Loss : 10.976\n",
            "Batch : 325, D Loss : 4.071 | G Loss : 10.607\n",
            "Batch : 326, D Loss : 4.032 | G Loss : 10.661\n",
            "Batch : 327, D Loss : 3.911 | G Loss : 9.162\n",
            "Batch : 328, D Loss : 3.832 | G Loss : 10.425\n",
            "Batch : 329, D Loss : 3.973 | G Loss : 11.126\n",
            "Batch : 330, D Loss : 4.012 | G Loss : 9.677\n",
            "Batch : 331, D Loss : 3.830 | G Loss : 9.361\n",
            "Batch : 332, D Loss : 4.149 | G Loss : 9.955\n",
            "Batch : 333, D Loss : 4.043 | G Loss : 11.123\n",
            "Batch : 334, D Loss : 3.925 | G Loss : 10.661\n",
            "Batch : 335, D Loss : 4.103 | G Loss : 10.249\n",
            "Batch : 336, D Loss : 4.074 | G Loss : 12.220\n",
            "Batch : 337, D Loss : 3.951 | G Loss : 9.468\n",
            "Batch : 338, D Loss : 3.897 | G Loss : 10.879\n",
            "Batch : 339, D Loss : 4.203 | G Loss : 10.328\n",
            "Batch : 340, D Loss : 4.148 | G Loss : 10.569\n",
            "Batch : 341, D Loss : 3.988 | G Loss : 11.174\n",
            "Batch : 342, D Loss : 4.112 | G Loss : 10.342\n",
            "Batch : 343, D Loss : 4.039 | G Loss : 9.738\n",
            "Batch : 344, D Loss : 3.849 | G Loss : 11.080\n",
            "Batch : 345, D Loss : 4.349 | G Loss : 10.729\n",
            "Batch : 346, D Loss : 4.021 | G Loss : 9.770\n",
            "Batch : 347, D Loss : 3.970 | G Loss : 11.288\n",
            "Batch : 348, D Loss : 3.912 | G Loss : 11.040\n",
            "Batch : 349, D Loss : 3.990 | G Loss : 12.035\n",
            "Batch : 350, D Loss : 4.138 | G Loss : 9.542\n",
            "Batch : 351, D Loss : 3.966 | G Loss : 10.511\n",
            "Batch : 352, D Loss : 4.171 | G Loss : 12.566\n",
            "Batch : 353, D Loss : 3.985 | G Loss : 10.204\n",
            "Batch : 354, D Loss : 3.911 | G Loss : 9.440\n",
            "Batch : 355, D Loss : 3.989 | G Loss : 9.617\n",
            "Batch : 356, D Loss : 3.934 | G Loss : 10.881\n",
            "Batch : 357, D Loss : 4.041 | G Loss : 10.957\n",
            "Batch : 358, D Loss : 3.951 | G Loss : 8.855\n",
            "Batch : 359, D Loss : 4.002 | G Loss : 10.245\n",
            "Batch : 360, D Loss : 3.893 | G Loss : 10.159\n",
            "Batch : 361, D Loss : 3.945 | G Loss : 10.842\n",
            "Batch : 362, D Loss : 3.818 | G Loss : 9.807\n",
            "Batch : 363, D Loss : 3.941 | G Loss : 8.818\n",
            "Batch : 364, D Loss : 3.993 | G Loss : 9.979\n",
            "Batch : 365, D Loss : 4.034 | G Loss : 10.048\n",
            "Batch : 366, D Loss : 3.846 | G Loss : 9.040\n",
            "Batch : 367, D Loss : 3.834 | G Loss : 10.539\n",
            "Batch : 368, D Loss : 3.924 | G Loss : 10.557\n",
            "Batch : 369, D Loss : 3.904 | G Loss : 9.180\n",
            "Batch : 370, D Loss : 4.162 | G Loss : 11.744\n",
            "Batch : 371, D Loss : 3.965 | G Loss : 9.576\n",
            "Batch : 372, D Loss : 3.904 | G Loss : 9.908\n",
            "Batch : 373, D Loss : 3.803 | G Loss : 10.830\n",
            "Batch : 374, D Loss : 4.154 | G Loss : 9.947\n",
            "Batch : 375, D Loss : 4.066 | G Loss : 11.628\n",
            "Batch : 376, D Loss : 3.851 | G Loss : 10.266\n",
            "Batch : 377, D Loss : 3.933 | G Loss : 10.831\n",
            "Batch : 378, D Loss : 3.917 | G Loss : 11.190\n",
            "Batch : 379, D Loss : 4.041 | G Loss : 8.980\n",
            "Batch : 380, D Loss : 4.034 | G Loss : 11.443\n",
            "Batch : 381, D Loss : 3.921 | G Loss : 11.022\n",
            "Batch : 382, D Loss : 3.971 | G Loss : 10.688\n",
            "Batch : 383, D Loss : 3.965 | G Loss : 10.241\n",
            "Batch : 384, D Loss : 3.883 | G Loss : 11.116\n",
            "Batch : 385, D Loss : 4.354 | G Loss : 10.734\n",
            "Batch : 386, D Loss : 4.254 | G Loss : 10.196\n",
            "Batch : 387, D Loss : 4.255 | G Loss : 14.207\n",
            "Batch : 388, D Loss : 5.816 | G Loss : 12.429\n",
            "Batch : 389, D Loss : 5.595 | G Loss : 11.201\n",
            "Batch : 390, D Loss : 4.768 | G Loss : 13.078\n",
            "Batch : 391, D Loss : 4.353 | G Loss : 10.198\n",
            "Batch : 392, D Loss : 4.164 | G Loss : 13.136\n",
            "Batch : 393, D Loss : 4.269 | G Loss : 10.591\n",
            "Batch : 394, D Loss : 4.286 | G Loss : 10.542\n",
            "Batch : 395, D Loss : 4.009 | G Loss : 11.408\n",
            "Batch : 396, D Loss : 4.044 | G Loss : 11.164\n",
            "Batch : 397, D Loss : 4.174 | G Loss : 10.822\n",
            "Batch : 398, D Loss : 4.085 | G Loss : 10.758\n",
            "Batch : 399, D Loss : 3.956 | G Loss : 10.187\n",
            "Batch : 400, D Loss : 4.076 | G Loss : 10.507\n",
            "Batch : 401, D Loss : 3.973 | G Loss : 10.920\n",
            "Batch : 402, D Loss : 4.203 | G Loss : 10.931\n",
            "Batch : 403, D Loss : 4.041 | G Loss : 9.457\n",
            "Batch : 404, D Loss : 3.981 | G Loss : 12.298\n",
            "Batch : 405, D Loss : 3.878 | G Loss : 10.306\n",
            "Batch : 406, D Loss : 3.922 | G Loss : 11.814\n",
            "Batch : 407, D Loss : 4.162 | G Loss : 10.734\n",
            "Batch : 408, D Loss : 4.166 | G Loss : 10.977\n",
            "Batch : 409, D Loss : 4.187 | G Loss : 11.097\n",
            "Batch : 410, D Loss : 4.073 | G Loss : 10.564\n",
            "Batch : 411, D Loss : 4.005 | G Loss : 10.594\n",
            "Batch : 412, D Loss : 3.930 | G Loss : 10.653\n",
            "Batch : 413, D Loss : 3.994 | G Loss : 10.002\n",
            "Batch : 414, D Loss : 3.922 | G Loss : 10.330\n",
            "Batch : 415, D Loss : 3.846 | G Loss : 11.634\n",
            "Batch : 416, D Loss : 4.354 | G Loss : 10.256\n",
            "Batch : 417, D Loss : 4.224 | G Loss : 9.949\n",
            "Batch : 418, D Loss : 4.004 | G Loss : 8.937\n",
            "Batch : 419, D Loss : 3.829 | G Loss : 10.430\n",
            "Batch : 420, D Loss : 4.047 | G Loss : 11.599\n",
            "Batch : 421, D Loss : 4.076 | G Loss : 10.301\n",
            "Batch : 422, D Loss : 4.000 | G Loss : 9.198\n",
            "Batch : 423, D Loss : 3.928 | G Loss : 10.111\n",
            "Batch : 424, D Loss : 3.928 | G Loss : 10.393\n",
            "Batch : 425, D Loss : 4.081 | G Loss : 10.422\n",
            "Batch : 426, D Loss : 4.065 | G Loss : 9.259\n",
            "Batch : 427, D Loss : 4.019 | G Loss : 11.132\n",
            "Batch : 428, D Loss : 4.151 | G Loss : 11.990\n",
            "Batch : 429, D Loss : 3.973 | G Loss : 11.472\n",
            "Batch : 430, D Loss : 3.866 | G Loss : 10.279\n",
            "Batch : 431, D Loss : 4.212 | G Loss : 9.190\n",
            "Batch : 432, D Loss : 4.391 | G Loss : 10.508\n",
            "Batch : 433, D Loss : 4.198 | G Loss : 11.236\n",
            "Batch : 434, D Loss : 3.941 | G Loss : 11.689\n",
            "Batch : 435, D Loss : 4.620 | G Loss : 11.202\n",
            "Batch : 436, D Loss : 4.382 | G Loss : 10.383\n",
            "Batch : 437, D Loss : 4.049 | G Loss : 10.715\n",
            "Batch : 438, D Loss : 3.785 | G Loss : 9.562\n",
            "Batch : 439, D Loss : 4.140 | G Loss : 10.476\n",
            "Batch : 440, D Loss : 4.141 | G Loss : 10.767\n",
            "Batch : 441, D Loss : 3.935 | G Loss : 9.969\n",
            "Batch : 442, D Loss : 3.846 | G Loss : 10.938\n",
            "Batch : 443, D Loss : 4.271 | G Loss : 9.975\n",
            "Batch : 444, D Loss : 4.001 | G Loss : 9.181\n",
            "Batch : 445, D Loss : 4.038 | G Loss : 11.553\n",
            "Batch : 446, D Loss : 3.826 | G Loss : 10.051\n",
            "Batch : 447, D Loss : 4.247 | G Loss : 10.176\n",
            "Batch : 448, D Loss : 4.441 | G Loss : 9.874\n",
            "Batch : 449, D Loss : 4.116 | G Loss : 10.356\n",
            "Batch : 450, D Loss : 3.944 | G Loss : 9.983\n",
            "Batch : 451, D Loss : 3.933 | G Loss : 9.768\n",
            "Batch : 452, D Loss : 4.017 | G Loss : 10.588\n",
            "Batch : 453, D Loss : 4.390 | G Loss : 11.450\n",
            "Batch : 454, D Loss : 5.703 | G Loss : 12.645\n",
            "Batch : 455, D Loss : 5.197 | G Loss : 11.631\n",
            "Batch : 456, D Loss : 4.602 | G Loss : 11.449\n",
            "Batch : 457, D Loss : 4.333 | G Loss : 11.687\n",
            "Batch : 458, D Loss : 4.461 | G Loss : 11.331\n",
            "Batch : 459, D Loss : 4.041 | G Loss : 9.795\n",
            "Batch : 460, D Loss : 3.993 | G Loss : 9.565\n",
            "Batch : 461, D Loss : 4.052 | G Loss : 10.916\n",
            "Batch : 462, D Loss : 4.060 | G Loss : 9.916\n",
            "Batch : 463, D Loss : 4.090 | G Loss : 9.626\n",
            "Batch : 464, D Loss : 4.121 | G Loss : 10.613\n",
            "Batch : 465, D Loss : 3.856 | G Loss : 10.375\n",
            "Batch : 466, D Loss : 3.891 | G Loss : 10.807\n",
            "Batch : 467, D Loss : 3.981 | G Loss : 9.492\n",
            "Batch : 468, D Loss : 3.956 | G Loss : 9.447\n",
            "Batch : 469, D Loss : 3.957 | G Loss : 9.893\n",
            "Batch : 470, D Loss : 3.962 | G Loss : 12.386\n",
            "Batch : 471, D Loss : 4.040 | G Loss : 9.688\n",
            "Batch : 472, D Loss : 3.983 | G Loss : 9.665\n",
            "Batch : 473, D Loss : 3.906 | G Loss : 8.753\n",
            "Batch : 474, D Loss : 4.161 | G Loss : 11.909\n",
            "Batch : 475, D Loss : 4.007 | G Loss : 10.398\n",
            "Batch : 476, D Loss : 3.874 | G Loss : 10.385\n",
            "Batch : 477, D Loss : 3.884 | G Loss : 10.048\n",
            "Batch : 478, D Loss : 4.402 | G Loss : 13.231\n",
            "Batch : 479, D Loss : 4.346 | G Loss : 10.901\n",
            "Batch : 480, D Loss : 3.936 | G Loss : 9.813\n",
            "Batch : 481, D Loss : 3.921 | G Loss : 9.600\n",
            "Batch : 482, D Loss : 4.055 | G Loss : 11.277\n",
            "Batch : 483, D Loss : 4.099 | G Loss : 9.895\n",
            "Batch : 484, D Loss : 4.124 | G Loss : 9.517\n",
            "Batch : 485, D Loss : 3.957 | G Loss : 8.906\n",
            "Batch : 486, D Loss : 4.073 | G Loss : 12.222\n",
            "Batch : 487, D Loss : 4.168 | G Loss : 10.075\n",
            "Batch : 488, D Loss : 4.181 | G Loss : 10.315\n",
            "Batch : 489, D Loss : 3.915 | G Loss : 9.508\n",
            "Batch : 490, D Loss : 3.831 | G Loss : 9.912\n",
            "Batch : 491, D Loss : 4.200 | G Loss : 10.342\n",
            "Batch : 492, D Loss : 4.223 | G Loss : 9.492\n",
            "Batch : 493, D Loss : 4.066 | G Loss : 10.097\n",
            "Batch : 494, D Loss : 3.933 | G Loss : 9.360\n",
            "Batch : 495, D Loss : 4.007 | G Loss : 11.048\n",
            "Batch : 496, D Loss : 4.151 | G Loss : 11.588\n",
            "Batch : 497, D Loss : 3.953 | G Loss : 10.193\n",
            "Batch : 498, D Loss : 6.186 | G Loss : 14.408\n",
            "Batch : 499, D Loss : 6.078 | G Loss : 15.458\n",
            "Batch : 500, D Loss : 5.997 | G Loss : 14.789\n",
            "Batch : 501, D Loss : 6.097 | G Loss : 15.423\n",
            "Batch : 502, D Loss : 5.614 | G Loss : 14.304\n",
            "Batch : 503, D Loss : 5.237 | G Loss : 13.738\n",
            "Batch : 504, D Loss : 4.899 | G Loss : 13.257\n",
            "Batch : 505, D Loss : 5.082 | G Loss : 13.564\n",
            "Batch : 506, D Loss : 4.958 | G Loss : 15.165\n",
            "Batch : 507, D Loss : 6.294 | G Loss : 17.462\n",
            "Batch : 508, D Loss : 6.327 | G Loss : 16.271\n",
            "Batch : 509, D Loss : 6.273 | G Loss : 15.101\n",
            "Batch : 510, D Loss : 6.356 | G Loss : 15.259\n",
            "Batch : 511, D Loss : 6.235 | G Loss : 15.309\n",
            "Batch : 512, D Loss : 5.891 | G Loss : 15.587\n",
            "Batch : 513, D Loss : 5.958 | G Loss : 15.542\n",
            "Batch : 514, D Loss : 6.431 | G Loss : 14.827\n",
            "Batch : 515, D Loss : 6.123 | G Loss : 14.916\n",
            "Batch : 516, D Loss : 6.181 | G Loss : 15.193\n",
            "Batch : 517, D Loss : 5.657 | G Loss : 17.311\n",
            "Batch : 518, D Loss : 4.971 | G Loss : 15.061\n",
            "Batch : 519, D Loss : 4.901 | G Loss : 14.573\n",
            "Batch : 520, D Loss : 5.444 | G Loss : 12.831\n",
            "Batch : 521, D Loss : 4.752 | G Loss : 12.874\n",
            "Batch : 522, D Loss : 5.999 | G Loss : 13.565\n",
            "Batch : 523, D Loss : 5.087 | G Loss : 14.210\n",
            "Batch : 524, D Loss : 4.035 | G Loss : 16.579\n",
            "Batch : 525, D Loss : 4.593 | G Loss : 12.634\n",
            "Batch : 526, D Loss : 6.298 | G Loss : 14.409\n",
            "Batch : 527, D Loss : 6.301 | G Loss : 15.355\n",
            "Batch : 528, D Loss : 6.360 | G Loss : 13.639\n",
            "Batch : 529, D Loss : 6.090 | G Loss : 13.402\n",
            "Batch : 530, D Loss : 4.525 | G Loss : 12.404\n",
            "Batch : 531, D Loss : 4.159 | G Loss : 12.841\n",
            "Batch : 532, D Loss : 6.266 | G Loss : 13.558\n",
            "Batch : 533, D Loss : 6.291 | G Loss : 13.721\n",
            "Batch : 534, D Loss : 6.191 | G Loss : 12.629\n",
            "Batch : 535, D Loss : 5.100 | G Loss : 11.978\n",
            "Batch : 536, D Loss : 4.226 | G Loss : 15.050\n",
            "Batch : 537, D Loss : 6.308 | G Loss : 15.512\n",
            "Batch : 538, D Loss : 6.445 | G Loss : 15.680\n",
            "Batch : 539, D Loss : 6.431 | G Loss : 16.737\n",
            "Batch : 540, D Loss : 6.505 | G Loss : 15.681\n",
            "Batch : 541, D Loss : 6.346 | G Loss : 15.874\n",
            "Batch : 542, D Loss : 6.364 | G Loss : 14.870\n",
            "Batch : 543, D Loss : 6.256 | G Loss : 14.363\n",
            "Batch : 544, D Loss : 6.141 | G Loss : 13.685\n",
            "Batch : 545, D Loss : 4.423 | G Loss : 14.353\n",
            "Batch : 546, D Loss : 6.096 | G Loss : 14.465\n",
            "Batch : 547, D Loss : 5.815 | G Loss : 13.059\n",
            "Batch : 548, D Loss : 4.509 | G Loss : 12.567\n",
            "Batch : 549, D Loss : 6.353 | G Loss : 15.381\n",
            "Batch : 550, D Loss : 6.555 | G Loss : 15.604\n",
            "Batch : 551, D Loss : 6.467 | G Loss : 13.670\n",
            "Batch : 552, D Loss : 5.939 | G Loss : 14.614\n",
            "Batch : 553, D Loss : 5.766 | G Loss : 14.093\n",
            "Batch : 554, D Loss : 5.196 | G Loss : 13.649\n",
            "Batch : 555, D Loss : 4.284 | G Loss : 12.184\n",
            "Batch : 556, D Loss : 4.241 | G Loss : 11.952\n",
            "Batch : 557, D Loss : 4.521 | G Loss : 12.950\n",
            "Batch : 558, D Loss : 4.293 | G Loss : 11.282\n",
            "Batch : 559, D Loss : 4.104 | G Loss : 11.016\n",
            "Batch : 560, D Loss : 4.121 | G Loss : 12.425\n",
            "Batch : 561, D Loss : 4.625 | G Loss : 12.250\n",
            "Batch : 562, D Loss : 4.375 | G Loss : 11.466\n",
            "Batch : 563, D Loss : 3.961 | G Loss : 12.021\n",
            "Batch : 564, D Loss : 5.451 | G Loss : 12.002\n",
            "Batch : 565, D Loss : 5.376 | G Loss : 12.569\n",
            "Batch : 566, D Loss : 4.577 | G Loss : 11.903\n",
            "Batch : 567, D Loss : 4.008 | G Loss : 9.964\n",
            "Batch : 568, D Loss : 3.924 | G Loss : 11.491\n",
            "Batch : 569, D Loss : 4.978 | G Loss : 12.384\n",
            "Batch : 570, D Loss : 5.232 | G Loss : 12.494\n",
            "Batch : 571, D Loss : 4.495 | G Loss : 11.408\n",
            "Batch : 572, D Loss : 4.022 | G Loss : 11.922\n",
            "Batch : 573, D Loss : 4.936 | G Loss : 13.092\n",
            "Batch : 574, D Loss : 4.761 | G Loss : 11.754\n",
            "Batch : 575, D Loss : 4.189 | G Loss : 11.670\n",
            "Batch : 576, D Loss : 3.700 | G Loss : 10.098\n",
            "Batch : 577, D Loss : 4.272 | G Loss : 11.138\n",
            "Batch : 578, D Loss : 4.169 | G Loss : 10.165\n",
            "Batch : 579, D Loss : 4.065 | G Loss : 11.799\n",
            "Batch : 580, D Loss : 4.092 | G Loss : 11.367\n",
            "Batch : 581, D Loss : 4.109 | G Loss : 12.474\n",
            "Batch : 582, D Loss : 4.037 | G Loss : 11.062\n",
            "Batch : 583, D Loss : 3.905 | G Loss : 12.841\n",
            "Batch : 584, D Loss : 4.004 | G Loss : 11.150\n",
            "Batch : 585, D Loss : 4.035 | G Loss : 11.575\n",
            "Batch : 586, D Loss : 4.026 | G Loss : 10.423\n",
            "Batch : 587, D Loss : 3.903 | G Loss : 11.834\n",
            "Batch : 588, D Loss : 4.179 | G Loss : 9.896\n",
            "Batch : 589, D Loss : 4.094 | G Loss : 9.621\n",
            "Batch : 590, D Loss : 3.820 | G Loss : 10.928\n",
            "Batch : 591, D Loss : 4.284 | G Loss : 11.062\n",
            "Batch : 592, D Loss : 4.265 | G Loss : 11.048\n",
            "Batch : 593, D Loss : 3.940 | G Loss : 10.706\n",
            "Batch : 594, D Loss : 3.934 | G Loss : 12.162\n",
            "Batch : 595, D Loss : 4.012 | G Loss : 10.354\n",
            "Batch : 596, D Loss : 3.929 | G Loss : 10.728\n",
            "Batch : 597, D Loss : 4.277 | G Loss : 11.092\n",
            "Batch : 598, D Loss : 4.185 | G Loss : 11.899\n",
            "Batch : 599, D Loss : 3.914 | G Loss : 10.677\n",
            "Batch : 600, D Loss : 4.135 | G Loss : 10.895\n",
            "Batch : 601, D Loss : 4.057 | G Loss : 10.645\n",
            "Batch : 602, D Loss : 3.877 | G Loss : 10.150\n",
            "Batch : 603, D Loss : 3.961 | G Loss : 9.962\n",
            "Batch : 604, D Loss : 3.934 | G Loss : 11.325\n",
            "Batch : 605, D Loss : 3.948 | G Loss : 10.216\n",
            "Batch : 606, D Loss : 3.889 | G Loss : 10.073\n",
            "Batch : 607, D Loss : 4.017 | G Loss : 11.123\n",
            "Batch : 608, D Loss : 3.971 | G Loss : 11.470\n",
            "Batch : 609, D Loss : 3.948 | G Loss : 10.112\n",
            "Batch : 610, D Loss : 3.932 | G Loss : 10.414\n",
            "Batch : 611, D Loss : 3.944 | G Loss : 11.340\n",
            "Batch : 612, D Loss : 3.875 | G Loss : 10.082\n",
            "Batch : 613, D Loss : 3.892 | G Loss : 9.515\n",
            "Batch : 614, D Loss : 3.942 | G Loss : 10.562\n",
            "Batch : 615, D Loss : 3.890 | G Loss : 12.114\n",
            "Batch : 616, D Loss : 3.946 | G Loss : 9.564\n",
            "Batch : 617, D Loss : 4.069 | G Loss : 9.065\n",
            "Batch : 618, D Loss : 3.877 | G Loss : 9.975\n",
            "Batch : 619, D Loss : 4.766 | G Loss : 11.081\n",
            "Batch : 620, D Loss : 4.443 | G Loss : 9.980\n",
            "Batch : 621, D Loss : 3.849 | G Loss : 10.270\n",
            "Batch : 622, D Loss : 3.747 | G Loss : 11.338\n",
            "Batch : 623, D Loss : 4.740 | G Loss : 10.241\n",
            "Batch : 624, D Loss : 4.538 | G Loss : 12.007\n",
            "Batch : 625, D Loss : 3.994 | G Loss : 9.430\n",
            "Batch : 626, D Loss : 3.640 | G Loss : 11.053\n",
            "Batch : 627, D Loss : 4.081 | G Loss : 11.632\n",
            "Batch : 628, D Loss : 4.078 | G Loss : 11.331\n",
            "Batch : 629, D Loss : 3.965 | G Loss : 9.289\n",
            "Batch : 630, D Loss : 4.075 | G Loss : 10.257\n",
            "Batch : 631, D Loss : 3.870 | G Loss : 9.870\n",
            "Batch : 632, D Loss : 4.066 | G Loss : 13.404\n",
            "Batch : 633, D Loss : 3.872 | G Loss : 9.929\n",
            "Batch : 634, D Loss : 3.824 | G Loss : 10.516\n",
            "Batch : 635, D Loss : 3.852 | G Loss : 9.308\n",
            "Batch : 636, D Loss : 4.503 | G Loss : 10.965\n",
            "Batch : 637, D Loss : 4.315 | G Loss : 11.313\n",
            "Batch : 638, D Loss : 3.951 | G Loss : 11.123\n",
            "Batch : 639, D Loss : 4.754 | G Loss : 10.125\n",
            "Batch : 640, D Loss : 4.644 | G Loss : 10.376\n",
            "Batch : 641, D Loss : 4.055 | G Loss : 10.225\n",
            "Batch : 642, D Loss : 3.779 | G Loss : 12.475\n",
            "Batch : 643, D Loss : 5.832 | G Loss : 11.797\n",
            "Batch : 644, D Loss : 6.146 | G Loss : 11.954\n",
            "Batch : 645, D Loss : 5.421 | G Loss : 12.583\n",
            "Batch : 646, D Loss : 4.548 | G Loss : 10.352\n",
            "Batch : 647, D Loss : 3.928 | G Loss : 10.479\n",
            "Batch : 648, D Loss : 4.845 | G Loss : 10.352\n",
            "Batch : 649, D Loss : 4.869 | G Loss : 12.578\n",
            "Batch : 650, D Loss : 4.385 | G Loss : 10.151\n",
            "Batch : 651, D Loss : 3.925 | G Loss : 9.388\n",
            "Batch : 652, D Loss : 3.824 | G Loss : 10.685\n",
            "Batch : 653, D Loss : 3.936 | G Loss : 10.598\n",
            "Batch : 654, D Loss : 4.011 | G Loss : 10.498\n",
            "Batch : 655, D Loss : 3.936 | G Loss : 9.895\n",
            "Batch : 656, D Loss : 3.842 | G Loss : 9.961\n",
            "Batch : 657, D Loss : 4.734 | G Loss : 9.506\n",
            "Batch : 658, D Loss : 4.751 | G Loss : 9.891\n",
            "Batch : 659, D Loss : 4.139 | G Loss : 9.569\n",
            "Batch : 660, D Loss : 3.740 | G Loss : 8.961\n",
            "Batch : 661, D Loss : 3.784 | G Loss : 10.688\n",
            "Batch : 662, D Loss : 3.942 | G Loss : 10.246\n",
            "Batch : 663, D Loss : 4.950 | G Loss : 10.791\n",
            "Batch : 664, D Loss : 4.876 | G Loss : 11.089\n",
            "Batch : 665, D Loss : 4.606 | G Loss : 11.331\n",
            "Batch : 666, D Loss : 4.432 | G Loss : 11.829\n",
            "Batch : 667, D Loss : 5.985 | G Loss : 12.292\n",
            "Batch : 668, D Loss : 6.138 | G Loss : 13.473\n",
            "Batch : 669, D Loss : 6.196 | G Loss : 12.067\n",
            "Batch : 670, D Loss : 6.017 | G Loss : 11.286\n",
            "Batch : 671, D Loss : 5.524 | G Loss : 10.466\n",
            "Batch : 672, D Loss : 4.886 | G Loss : 11.833\n",
            "Batch : 673, D Loss : 4.197 | G Loss : 10.837\n",
            "Batch : 674, D Loss : 4.360 | G Loss : 11.904\n",
            "Batch : 675, D Loss : 4.314 | G Loss : 11.638\n",
            "Batch : 676, D Loss : 4.323 | G Loss : 10.864\n",
            "Batch : 677, D Loss : 4.162 | G Loss : 9.661\n",
            "Batch : 678, D Loss : 4.175 | G Loss : 9.588\n",
            "Batch : 679, D Loss : 4.019 | G Loss : 9.995\n",
            "Batch : 680, D Loss : 4.077 | G Loss : 9.411\n",
            "Batch : 681, D Loss : 4.038 | G Loss : 10.089\n",
            "Batch : 682, D Loss : 4.026 | G Loss : 9.484\n",
            "Batch : 683, D Loss : 4.152 | G Loss : 10.120\n",
            "Batch : 684, D Loss : 3.895 | G Loss : 8.960\n",
            "Batch : 685, D Loss : 4.027 | G Loss : 10.547\n",
            "Batch : 686, D Loss : 4.041 | G Loss : 11.531\n",
            "Batch : 687, D Loss : 3.944 | G Loss : 10.227\n",
            "Batch : 688, D Loss : 3.992 | G Loss : 10.563\n",
            "Batch : 689, D Loss : 3.956 | G Loss : 9.841\n",
            "Batch : 690, D Loss : 3.939 | G Loss : 9.253\n",
            "Batch : 691, D Loss : 3.894 | G Loss : 10.055\n",
            "Batch : 692, D Loss : 3.835 | G Loss : 10.278\n",
            "Batch : 693, D Loss : 4.014 | G Loss : 9.858\n",
            "Batch : 694, D Loss : 3.929 | G Loss : 10.140\n",
            "Batch : 695, D Loss : 3.808 | G Loss : 9.841\n",
            "Batch : 696, D Loss : 3.883 | G Loss : 9.656\n",
            "Batch : 697, D Loss : 3.840 | G Loss : 9.469\n",
            "Batch : 698, D Loss : 3.893 | G Loss : 8.963\n",
            "Batch : 699, D Loss : 3.949 | G Loss : 10.772\n",
            "Batch : 700, D Loss : 3.896 | G Loss : 10.406\n",
            "Batch : 701, D Loss : 3.915 | G Loss : 9.652\n",
            "Batch : 702, D Loss : 3.832 | G Loss : 11.262\n",
            "Batch : 703, D Loss : 3.892 | G Loss : 10.138\n",
            "Batch : 704, D Loss : 3.881 | G Loss : 10.096\n",
            "Batch : 705, D Loss : 3.879 | G Loss : 11.074\n",
            "Batch : 706, D Loss : 3.910 | G Loss : 9.439\n",
            "Batch : 707, D Loss : 3.988 | G Loss : 10.710\n",
            "Batch : 708, D Loss : 3.873 | G Loss : 9.292\n",
            "Batch : 709, D Loss : 3.770 | G Loss : 9.395\n",
            "Batch : 710, D Loss : 3.974 | G Loss : 12.120\n",
            "Batch : 711, D Loss : 3.892 | G Loss : 10.097\n",
            "Batch : 712, D Loss : 4.144 | G Loss : 9.275\n",
            "Batch : 713, D Loss : 4.200 | G Loss : 10.849\n",
            "Batch : 714, D Loss : 4.073 | G Loss : 11.454\n",
            "Batch : 715, D Loss : 3.957 | G Loss : 8.556\n",
            "Batch : 716, D Loss : 3.902 | G Loss : 9.569\n",
            "Batch : 717, D Loss : 4.033 | G Loss : 12.208\n",
            "Batch : 718, D Loss : 3.956 | G Loss : 10.004\n",
            "Batch : 719, D Loss : 3.872 | G Loss : 9.390\n",
            "Batch : 720, D Loss : 3.811 | G Loss : 10.429\n",
            "Batch : 721, D Loss : 3.864 | G Loss : 10.027\n",
            "Batch : 722, D Loss : 3.896 | G Loss : 8.939\n",
            "Batch : 723, D Loss : 3.871 | G Loss : 8.680\n",
            "Batch : 724, D Loss : 3.866 | G Loss : 11.840\n",
            "Batch : 725, D Loss : 3.895 | G Loss : 9.722\n",
            "Batch : 726, D Loss : 3.897 | G Loss : 10.260\n",
            "Batch : 727, D Loss : 3.844 | G Loss : 9.641\n",
            "Batch : 728, D Loss : 3.910 | G Loss : 9.557\n",
            "Batch : 729, D Loss : 3.911 | G Loss : 10.041\n",
            "Batch : 730, D Loss : 3.957 | G Loss : 10.900\n",
            "Batch : 731, D Loss : 3.847 | G Loss : 9.510\n",
            "Batch : 732, D Loss : 3.938 | G Loss : 9.633\n",
            "Batch : 733, D Loss : 3.933 | G Loss : 8.958\n",
            "Batch : 734, D Loss : 3.841 | G Loss : 9.540\n",
            "Batch : 735, D Loss : 3.891 | G Loss : 10.208\n",
            "Batch : 736, D Loss : 3.901 | G Loss : 9.919\n",
            "Batch : 737, D Loss : 3.816 | G Loss : 9.076\n",
            "Batch : 738, D Loss : 3.748 | G Loss : 9.370\n",
            "Batch : 739, D Loss : 3.798 | G Loss : 10.521\n",
            "Batch : 740, D Loss : 3.857 | G Loss : 8.239\n",
            "Batch : 741, D Loss : 3.892 | G Loss : 9.623\n",
            "Batch : 742, D Loss : 3.817 | G Loss : 8.411\n",
            "Batch : 743, D Loss : 3.847 | G Loss : 10.352\n",
            "Batch : 744, D Loss : 3.907 | G Loss : 9.590\n",
            "Batch : 745, D Loss : 3.831 | G Loss : 10.714\n",
            "Batch : 746, D Loss : 3.871 | G Loss : 9.833\n",
            "Batch : 747, D Loss : 3.933 | G Loss : 10.456\n",
            "Batch : 748, D Loss : 3.919 | G Loss : 8.964\n",
            "Batch : 749, D Loss : 3.860 | G Loss : 9.463\n",
            "Batch : 750, D Loss : 3.762 | G Loss : 10.986\n",
            "Batch : 751, D Loss : 3.877 | G Loss : 10.551\n",
            "Batch : 752, D Loss : 3.953 | G Loss : 10.341\n",
            "Batch : 753, D Loss : 3.860 | G Loss : 8.938\n",
            "Batch : 754, D Loss : 3.865 | G Loss : 11.130\n",
            "Batch : 755, D Loss : 3.928 | G Loss : 9.486\n",
            "Batch : 756, D Loss : 3.822 | G Loss : 10.227\n",
            "Batch : 757, D Loss : 3.784 | G Loss : 9.483\n",
            "Batch : 758, D Loss : 3.751 | G Loss : 10.600\n",
            "Batch : 759, D Loss : 3.876 | G Loss : 9.981\n",
            "Batch : 760, D Loss : 3.878 | G Loss : 10.323\n",
            "Batch : 761, D Loss : 3.883 | G Loss : 10.705\n",
            "Batch : 762, D Loss : 3.837 | G Loss : 9.935\n",
            "Batch : 763, D Loss : 3.843 | G Loss : 8.949\n",
            "Batch : 764, D Loss : 3.843 | G Loss : 10.313\n",
            "Batch : 765, D Loss : 3.865 | G Loss : 10.393\n",
            "Batch : 766, D Loss : 3.843 | G Loss : 11.291\n",
            "Batch : 767, D Loss : 4.011 | G Loss : 10.324\n",
            "Batch : 768, D Loss : 3.908 | G Loss : 10.935\n",
            "Batch : 769, D Loss : 3.888 | G Loss : 10.685\n",
            "Batch : 770, D Loss : 3.822 | G Loss : 9.282\n",
            "Batch : 771, D Loss : 3.833 | G Loss : 9.446\n",
            "Batch : 772, D Loss : 3.843 | G Loss : 9.431\n",
            "Batch : 773, D Loss : 3.875 | G Loss : 10.721\n",
            "Batch : 774, D Loss : 3.857 | G Loss : 8.495\n",
            "Batch : 775, D Loss : 3.797 | G Loss : 9.136\n",
            "Batch : 776, D Loss : 3.793 | G Loss : 10.351\n",
            "Batch : 777, D Loss : 3.877 | G Loss : 9.359\n",
            "Batch : 778, D Loss : 3.825 | G Loss : 9.742\n",
            "Batch : 779, D Loss : 3.854 | G Loss : 9.565\n",
            "Batch : 780, D Loss : 3.832 | G Loss : 8.954\n",
            "Batch : 781, D Loss : 5.485 | G Loss : 10.884\n",
            "Batch : 782, D Loss : 5.607 | G Loss : 10.177\n",
            "Batch : 783, D Loss : 5.162 | G Loss : 9.726\n",
            "Batch : 784, D Loss : 4.667 | G Loss : 11.373\n",
            "Batch : 785, D Loss : 4.896 | G Loss : 11.046\n",
            "Batch : 786, D Loss : 4.716 | G Loss : 10.251\n",
            "Batch : 787, D Loss : 4.571 | G Loss : 9.515\n",
            "Batch : 788, D Loss : 4.321 | G Loss : 12.662\n",
            "Batch : 789, D Loss : 4.246 | G Loss : 9.486\n",
            "Batch : 790, D Loss : 4.144 | G Loss : 11.621\n",
            "Batch : 791, D Loss : 4.479 | G Loss : 9.506\n",
            "Batch : 792, D Loss : 4.289 | G Loss : 10.381\n",
            "Batch : 793, D Loss : 4.177 | G Loss : 10.178\n",
            "Batch : 794, D Loss : 4.095 | G Loss : 9.786\n",
            "Batch : 795, D Loss : 4.177 | G Loss : 10.168\n",
            "Batch : 796, D Loss : 4.108 | G Loss : 9.715\n",
            "Batch : 797, D Loss : 4.116 | G Loss : 9.707\n",
            "Batch : 798, D Loss : 4.103 | G Loss : 9.609\n",
            "Batch : 799, D Loss : 4.029 | G Loss : 9.915\n",
            "Batch : 800, D Loss : 3.871 | G Loss : 9.208\n",
            "Batch : 801, D Loss : 3.741 | G Loss : 10.864\n",
            "Batch : 802, D Loss : 3.945 | G Loss : 10.930\n",
            "Batch : 803, D Loss : 3.921 | G Loss : 9.412\n",
            "Batch : 804, D Loss : 3.908 | G Loss : 9.494\n",
            "Batch : 805, D Loss : 3.827 | G Loss : 10.554\n",
            "Batch : 806, D Loss : 3.925 | G Loss : 9.986\n",
            "Batch : 807, D Loss : 3.853 | G Loss : 8.749\n",
            "Batch : 808, D Loss : 3.907 | G Loss : 9.970\n",
            "Batch : 809, D Loss : 3.999 | G Loss : 10.644\n",
            "Batch : 810, D Loss : 3.842 | G Loss : 10.534\n",
            "Batch : 811, D Loss : 3.913 | G Loss : 9.524\n",
            "Batch : 812, D Loss : 4.001 | G Loss : 10.182\n",
            "Batch : 813, D Loss : 3.865 | G Loss : 9.148\n",
            "Batch : 814, D Loss : 3.836 | G Loss : 10.522\n",
            "Batch : 815, D Loss : 3.861 | G Loss : 9.459\n",
            "Batch : 816, D Loss : 3.851 | G Loss : 9.502\n",
            "Batch : 817, D Loss : 3.842 | G Loss : 9.834\n",
            "Batch : 818, D Loss : 3.744 | G Loss : 10.137\n",
            "Batch : 819, D Loss : 3.866 | G Loss : 9.997\n",
            "Batch : 820, D Loss : 3.865 | G Loss : 10.320\n",
            "Batch : 821, D Loss : 3.869 | G Loss : 9.778\n",
            "Batch : 822, D Loss : 3.865 | G Loss : 9.566\n",
            "Batch : 823, D Loss : 3.831 | G Loss : 9.886\n",
            "Batch : 824, D Loss : 3.922 | G Loss : 11.579\n",
            "Batch : 825, D Loss : 4.006 | G Loss : 10.843\n",
            "Batch : 826, D Loss : 3.807 | G Loss : 11.080\n",
            "Batch : 827, D Loss : 3.958 | G Loss : 10.351\n",
            "Batch : 828, D Loss : 3.832 | G Loss : 9.979\n",
            "Batch : 829, D Loss : 3.916 | G Loss : 11.944\n",
            "Batch : 830, D Loss : 3.783 | G Loss : 10.030\n",
            "Batch : 831, D Loss : 3.847 | G Loss : 8.981\n",
            "Batch : 832, D Loss : 3.887 | G Loss : 10.082\n",
            "Batch : 833, D Loss : 3.871 | G Loss : 9.964\n",
            "Batch : 834, D Loss : 3.800 | G Loss : 9.122\n",
            "Batch : 835, D Loss : 3.805 | G Loss : 8.857\n",
            "Batch : 836, D Loss : 3.816 | G Loss : 9.767\n",
            "Batch : 837, D Loss : 3.896 | G Loss : 9.622\n",
            "Batch : 838, D Loss : 3.780 | G Loss : 9.472\n",
            "Batch : 839, D Loss : 3.741 | G Loss : 9.493\n",
            "Batch : 840, D Loss : 3.870 | G Loss : 11.418\n",
            "Batch : 841, D Loss : 4.043 | G Loss : 10.557\n",
            "Batch : 842, D Loss : 3.973 | G Loss : 11.143\n",
            "Batch : 843, D Loss : 3.807 | G Loss : 9.205\n",
            "Batch : 844, D Loss : 3.730 | G Loss : 9.063\n",
            "Batch : 845, D Loss : 3.952 | G Loss : 10.476\n",
            "Batch : 846, D Loss : 3.921 | G Loss : 9.592\n",
            "Batch : 847, D Loss : 3.831 | G Loss : 10.931\n",
            "Batch : 848, D Loss : 3.857 | G Loss : 10.826\n",
            "Batch : 849, D Loss : 3.920 | G Loss : 10.329\n",
            "Batch : 850, D Loss : 4.013 | G Loss : 12.356\n",
            "Batch : 851, D Loss : 3.977 | G Loss : 9.919\n",
            "Batch : 852, D Loss : 4.006 | G Loss : 8.936\n",
            "Batch : 853, D Loss : 4.274 | G Loss : 9.669\n",
            "Batch : 854, D Loss : 4.188 | G Loss : 10.195\n",
            "Batch : 855, D Loss : 3.971 | G Loss : 9.756\n",
            "Batch : 856, D Loss : 3.826 | G Loss : 12.332\n",
            "Batch : 857, D Loss : 3.861 | G Loss : 10.500\n",
            "Batch : 858, D Loss : 3.897 | G Loss : 9.871\n",
            "Batch : 859, D Loss : 3.973 | G Loss : 9.276\n",
            "Batch : 860, D Loss : 3.838 | G Loss : 10.505\n",
            "Batch : 861, D Loss : 3.784 | G Loss : 10.037\n",
            "Batch : 862, D Loss : 3.792 | G Loss : 10.610\n",
            "Batch : 863, D Loss : 3.832 | G Loss : 8.942\n",
            "Batch : 864, D Loss : 3.843 | G Loss : 9.801\n",
            "Batch : 865, D Loss : 3.817 | G Loss : 8.815\n",
            "Batch : 866, D Loss : 3.865 | G Loss : 9.351\n",
            "Batch : 867, D Loss : 3.867 | G Loss : 10.346\n",
            "Batch : 868, D Loss : 3.815 | G Loss : 9.644\n",
            "Batch : 869, D Loss : 3.865 | G Loss : 10.507\n",
            "Batch : 870, D Loss : 3.876 | G Loss : 9.261\n",
            "Batch : 871, D Loss : 3.809 | G Loss : 10.240\n",
            "Batch : 872, D Loss : 3.836 | G Loss : 10.100\n",
            "Batch : 873, D Loss : 3.836 | G Loss : 9.752\n",
            "Batch : 874, D Loss : 3.774 | G Loss : 9.188\n",
            "Batch : 875, D Loss : 3.879 | G Loss : 10.393\n",
            "Batch : 876, D Loss : 3.870 | G Loss : 9.271\n",
            "Batch : 877, D Loss : 3.797 | G Loss : 10.290\n",
            "Batch : 878, D Loss : 3.895 | G Loss : 9.988\n",
            "Batch : 879, D Loss : 3.885 | G Loss : 10.275\n",
            "Batch : 880, D Loss : 3.867 | G Loss : 10.978\n",
            "Batch : 881, D Loss : 3.827 | G Loss : 8.379\n",
            "Batch : 882, D Loss : 3.775 | G Loss : 10.418\n",
            "Batch : 883, D Loss : 3.806 | G Loss : 9.370\n",
            "Batch : 884, D Loss : 3.881 | G Loss : 9.310\n",
            "Batch : 885, D Loss : 3.837 | G Loss : 11.035\n",
            "Batch : 886, D Loss : 3.834 | G Loss : 10.156\n",
            "Batch : 887, D Loss : 3.900 | G Loss : 10.470\n",
            "Batch : 888, D Loss : 3.831 | G Loss : 9.554\n",
            "Batch : 889, D Loss : 3.841 | G Loss : 8.882\n",
            "Batch : 890, D Loss : 3.845 | G Loss : 8.995\n",
            "Batch : 891, D Loss : 3.828 | G Loss : 10.092\n",
            "Batch : 892, D Loss : 3.899 | G Loss : 9.255\n",
            "Batch : 893, D Loss : 3.872 | G Loss : 9.118\n",
            "Batch : 894, D Loss : 3.875 | G Loss : 9.016\n",
            "Batch : 895, D Loss : 3.759 | G Loss : 9.294\n",
            "Batch : 896, D Loss : 3.830 | G Loss : 8.177\n",
            "Batch : 897, D Loss : 3.766 | G Loss : 9.694\n",
            "Batch : 898, D Loss : 3.860 | G Loss : 10.955\n",
            "Batch : 899, D Loss : 3.921 | G Loss : 9.967\n",
            "Batch : 900, D Loss : 3.880 | G Loss : 9.048\n",
            "Batch : 901, D Loss : 3.807 | G Loss : 9.754\n",
            "Batch : 902, D Loss : 3.876 | G Loss : 9.991\n",
            "Batch : 903, D Loss : 3.922 | G Loss : 9.997\n",
            "Batch : 904, D Loss : 3.909 | G Loss : 9.945\n",
            "Batch : 905, D Loss : 4.000 | G Loss : 11.280\n",
            "Batch : 906, D Loss : 3.973 | G Loss : 11.806\n",
            "Batch : 907, D Loss : 3.898 | G Loss : 10.298\n",
            "Batch : 908, D Loss : 3.808 | G Loss : 9.382\n",
            "Batch : 909, D Loss : 3.769 | G Loss : 9.460\n",
            "Batch : 910, D Loss : 3.991 | G Loss : 8.723\n",
            "Batch : 911, D Loss : 3.880 | G Loss : 10.673\n",
            "Batch : 912, D Loss : 3.839 | G Loss : 10.051\n",
            "Batch : 913, D Loss : 3.836 | G Loss : 10.039\n",
            "Batch : 914, D Loss : 3.834 | G Loss : 8.221\n",
            "Batch : 915, D Loss : 3.827 | G Loss : 10.252\n",
            "Batch : 916, D Loss : 3.955 | G Loss : 9.396\n",
            "Batch : 917, D Loss : 3.864 | G Loss : 10.641\n",
            "Batch : 918, D Loss : 3.837 | G Loss : 9.862\n",
            "Batch : 919, D Loss : 3.774 | G Loss : 8.766\n",
            "Batch : 920, D Loss : 3.865 | G Loss : 10.637\n",
            "Batch : 921, D Loss : 3.837 | G Loss : 9.655\n",
            "Batch : 922, D Loss : 3.730 | G Loss : 9.125\n",
            "Batch : 923, D Loss : 3.799 | G Loss : 9.375\n",
            "Batch : 924, D Loss : 3.783 | G Loss : 10.746\n",
            "Batch : 925, D Loss : 3.883 | G Loss : 10.344\n",
            "Batch : 926, D Loss : 3.847 | G Loss : 10.703\n",
            "Batch : 927, D Loss : 3.818 | G Loss : 9.370\n",
            "Batch : 928, D Loss : 3.839 | G Loss : 10.875\n",
            "Batch : 929, D Loss : 3.811 | G Loss : 9.106\n",
            "Batch : 930, D Loss : 3.806 | G Loss : 8.475\n",
            "Batch : 931, D Loss : 3.839 | G Loss : 8.556\n",
            "Batch : 932, D Loss : 3.746 | G Loss : 10.535\n",
            "Batch : 933, D Loss : 3.798 | G Loss : 9.142\n",
            "Batch : 934, D Loss : 3.864 | G Loss : 11.049\n",
            "Batch : 935, D Loss : 3.837 | G Loss : 9.076\n",
            "Batch : 936, D Loss : 3.793 | G Loss : 9.685\n",
            "Batch : 937, D Loss : 3.841 | G Loss : 9.295\n",
            "Batch : 938, D Loss : 3.929 | G Loss : 9.441\n",
            "Batch : 939, D Loss : 3.884 | G Loss : 9.708\n",
            "Batch : 940, D Loss : 3.831 | G Loss : 9.793\n",
            "Batch : 941, D Loss : 3.916 | G Loss : 10.261\n",
            "Batch : 942, D Loss : 3.853 | G Loss : 10.725\n",
            "Batch : 943, D Loss : 4.075 | G Loss : 10.018\n",
            "Batch : 944, D Loss : 3.991 | G Loss : 9.727\n",
            "Batch : 945, D Loss : 3.980 | G Loss : 8.858\n",
            "Batch : 946, D Loss : 3.926 | G Loss : 10.259\n",
            "Batch : 947, D Loss : 3.882 | G Loss : 9.181\n",
            "Batch : 948, D Loss : 3.873 | G Loss : 9.641\n",
            "Batch : 949, D Loss : 3.775 | G Loss : 10.500\n",
            "Batch : 950, D Loss : 3.868 | G Loss : 8.429\n",
            "Batch : 951, D Loss : 3.930 | G Loss : 10.709\n",
            "Batch : 952, D Loss : 3.945 | G Loss : 9.523\n",
            "Batch : 953, D Loss : 3.940 | G Loss : 8.571\n",
            "Batch : 954, D Loss : 3.872 | G Loss : 10.229\n",
            "Batch : 955, D Loss : 3.856 | G Loss : 9.290\n",
            "Batch : 956, D Loss : 3.848 | G Loss : 10.084\n",
            "Batch : 957, D Loss : 3.864 | G Loss : 9.366\n",
            "Batch : 958, D Loss : 3.780 | G Loss : 9.007\n",
            "Batch : 959, D Loss : 3.855 | G Loss : 7.945\n",
            "Batch : 960, D Loss : 3.856 | G Loss : 9.438\n",
            "Batch : 961, D Loss : 3.815 | G Loss : 8.787\n",
            "Batch : 962, D Loss : 3.893 | G Loss : 8.875\n",
            "Batch : 963, D Loss : 3.741 | G Loss : 9.533\n",
            "Batch : 964, D Loss : 4.107 | G Loss : 8.848\n",
            "Batch : 965, D Loss : 4.025 | G Loss : 11.019\n",
            "Batch : 966, D Loss : 3.754 | G Loss : 8.754\n",
            "Batch : 967, D Loss : 3.967 | G Loss : 9.112\n",
            "Batch : 968, D Loss : 3.868 | G Loss : 8.385\n",
            "Batch : 969, D Loss : 3.817 | G Loss : 9.820\n",
            "Batch : 970, D Loss : 3.849 | G Loss : 8.425\n",
            "Batch : 971, D Loss : 3.922 | G Loss : 10.046\n",
            "Batch : 972, D Loss : 3.908 | G Loss : 8.899\n",
            "Batch : 973, D Loss : 3.762 | G Loss : 9.184\n",
            "Batch : 974, D Loss : 3.841 | G Loss : 9.380\n",
            "Batch : 975, D Loss : 3.858 | G Loss : 9.861\n",
            "Batch : 976, D Loss : 3.830 | G Loss : 9.877\n",
            "Batch : 977, D Loss : 3.837 | G Loss : 8.611\n",
            "Batch : 978, D Loss : 3.838 | G Loss : 11.414\n",
            "Batch : 979, D Loss : 3.827 | G Loss : 8.909\n",
            "Batch : 980, D Loss : 3.841 | G Loss : 9.967\n",
            "Batch : 981, D Loss : 3.901 | G Loss : 9.668\n",
            "Batch : 982, D Loss : 3.833 | G Loss : 8.705\n",
            "Batch : 983, D Loss : 3.700 | G Loss : 9.188\n",
            "Batch : 984, D Loss : 3.840 | G Loss : 11.398\n",
            "Batch : 985, D Loss : 4.018 | G Loss : 9.573\n",
            "Batch : 986, D Loss : 3.892 | G Loss : 8.932\n",
            "Batch : 987, D Loss : 3.734 | G Loss : 9.041\n",
            "Batch : 988, D Loss : 3.798 | G Loss : 10.764\n",
            "Batch : 989, D Loss : 3.840 | G Loss : 9.482\n",
            "Batch : 990, D Loss : 3.848 | G Loss : 8.973\n",
            "Batch : 991, D Loss : 3.835 | G Loss : 9.987\n",
            "Batch : 992, D Loss : 3.770 | G Loss : 10.325\n",
            "Batch : 993, D Loss : 3.813 | G Loss : 9.597\n",
            "Batch : 994, D Loss : 4.057 | G Loss : 8.796\n",
            "Batch : 995, D Loss : 4.019 | G Loss : 10.005\n",
            "Batch : 996, D Loss : 3.898 | G Loss : 8.168\n",
            "Batch : 997, D Loss : 3.790 | G Loss : 9.961\n",
            "Batch : 998, D Loss : 3.886 | G Loss : 10.326\n",
            "Batch : 999, D Loss : 3.944 | G Loss : 8.663\n",
            "Batch : 1000, D Loss : 3.961 | G Loss : 9.331\n",
            "Batch : 1001, D Loss : 3.796 | G Loss : 9.693\n",
            "Batch : 1002, D Loss : 3.793 | G Loss : 8.768\n",
            "Batch : 1003, D Loss : 3.832 | G Loss : 8.620\n",
            "Batch : 1004, D Loss : 3.728 | G Loss : 9.884\n",
            "Batch : 1005, D Loss : 3.712 | G Loss : 9.652\n",
            "Batch : 1006, D Loss : 3.843 | G Loss : 9.784\n",
            "Batch : 1007, D Loss : 3.802 | G Loss : 9.031\n",
            "Batch : 1008, D Loss : 3.821 | G Loss : 10.652\n",
            "Batch : 1009, D Loss : 3.812 | G Loss : 10.972\n",
            "Batch : 1010, D Loss : 3.888 | G Loss : 9.332\n",
            "Batch : 1011, D Loss : 3.834 | G Loss : 10.346\n",
            "Batch : 1012, D Loss : 3.867 | G Loss : 9.408\n",
            "Batch : 1013, D Loss : 3.839 | G Loss : 9.590\n",
            "Batch : 1014, D Loss : 4.382 | G Loss : 9.229\n",
            "Batch : 1015, D Loss : 4.388 | G Loss : 9.251\n",
            "Batch : 1016, D Loss : 4.061 | G Loss : 8.704\n",
            "Batch : 1017, D Loss : 3.804 | G Loss : 9.497\n",
            "Batch : 1018, D Loss : 3.980 | G Loss : 8.809\n",
            "Batch : 1019, D Loss : 4.018 | G Loss : 10.138\n",
            "Batch : 1020, D Loss : 3.844 | G Loss : 10.189\n",
            "Batch : 1021, D Loss : 3.700 | G Loss : 10.925\n",
            "Batch : 1022, D Loss : 3.865 | G Loss : 9.508\n",
            "Batch : 1023, D Loss : 3.870 | G Loss : 9.332\n",
            "Batch : 1024, D Loss : 3.813 | G Loss : 9.594\n",
            "Batch : 1025, D Loss : 3.928 | G Loss : 9.971\n",
            "Batch : 1026, D Loss : 3.872 | G Loss : 10.657\n",
            "Batch : 1027, D Loss : 3.861 | G Loss : 8.642\n",
            "Batch : 1028, D Loss : 3.745 | G Loss : 8.738\n",
            "Batch : 1029, D Loss : 3.833 | G Loss : 9.665\n",
            "Batch : 1030, D Loss : 3.827 | G Loss : 8.790\n",
            "Batch : 1031, D Loss : 3.805 | G Loss : 8.822\n",
            "Batch : 1032, D Loss : 3.742 | G Loss : 8.538\n",
            "Batch : 1033, D Loss : 3.831 | G Loss : 9.948\n",
            "Batch : 1034, D Loss : 3.839 | G Loss : 11.317\n",
            "Batch : 1035, D Loss : 3.868 | G Loss : 8.387\n",
            "Batch : 1036, D Loss : 3.806 | G Loss : 9.156\n",
            "Batch : 1037, D Loss : 3.873 | G Loss : 10.673\n",
            "Batch : 1038, D Loss : 3.904 | G Loss : 10.300\n",
            "Batch : 1039, D Loss : 3.827 | G Loss : 11.444\n",
            "Batch : 1040, D Loss : 3.825 | G Loss : 9.425\n",
            "Batch : 1041, D Loss : 3.811 | G Loss : 9.007\n",
            "Batch : 1042, D Loss : 3.775 | G Loss : 9.755\n",
            "Batch : 1043, D Loss : 3.896 | G Loss : 9.152\n",
            "Batch : 1044, D Loss : 3.830 | G Loss : 9.400\n",
            "Batch : 1045, D Loss : 3.725 | G Loss : 9.158\n",
            "Batch : 1046, D Loss : 3.818 | G Loss : 10.002\n",
            "Batch : 1047, D Loss : 3.823 | G Loss : 8.402\n",
            "Batch : 1048, D Loss : 3.768 | G Loss : 9.207\n",
            "Batch : 1049, D Loss : 3.828 | G Loss : 9.058\n",
            "Batch : 1050, D Loss : 3.876 | G Loss : 9.956\n",
            "Batch : 1051, D Loss : 3.820 | G Loss : 10.450\n",
            "Batch : 1052, D Loss : 3.887 | G Loss : 9.620\n",
            "Batch : 1053, D Loss : 3.916 | G Loss : 9.701\n",
            "Batch : 1054, D Loss : 3.834 | G Loss : 7.854\n",
            "Batch : 1055, D Loss : 3.794 | G Loss : 9.331\n",
            "Batch : 1056, D Loss : 3.852 | G Loss : 10.531\n",
            "Batch : 1057, D Loss : 3.904 | G Loss : 9.373\n",
            "Batch : 1058, D Loss : 3.827 | G Loss : 10.198\n",
            "Batch : 1059, D Loss : 3.850 | G Loss : 8.804\n",
            "Batch : 1060, D Loss : 3.800 | G Loss : 9.073\n",
            "Batch : 1061, D Loss : 3.879 | G Loss : 9.063\n",
            "Batch : 1062, D Loss : 3.763 | G Loss : 9.661\n",
            "Batch : 1063, D Loss : 3.858 | G Loss : 9.500\n",
            "Batch : 1064, D Loss : 3.883 | G Loss : 8.781\n",
            "Batch : 1065, D Loss : 3.749 | G Loss : 8.920\n",
            "Batch : 1066, D Loss : 3.782 | G Loss : 8.900\n",
            "Batch : 1067, D Loss : 3.799 | G Loss : 8.435\n",
            "Batch : 1068, D Loss : 3.823 | G Loss : 9.067\n",
            "Batch : 1069, D Loss : 3.847 | G Loss : 10.065\n",
            "Batch : 1070, D Loss : 3.790 | G Loss : 8.107\n",
            "Batch : 1071, D Loss : 3.768 | G Loss : 8.175\n",
            "Batch : 1072, D Loss : 3.778 | G Loss : 9.231\n",
            "Batch : 1073, D Loss : 3.779 | G Loss : 9.423\n",
            "Batch : 1074, D Loss : 3.832 | G Loss : 10.010\n",
            "Batch : 1075, D Loss : 3.853 | G Loss : 10.680\n",
            "Batch : 1076, D Loss : 3.793 | G Loss : 9.395\n",
            "Batch : 1077, D Loss : 3.855 | G Loss : 9.349\n",
            "Batch : 1078, D Loss : 3.864 | G Loss : 9.181\n",
            "Batch : 1079, D Loss : 3.892 | G Loss : 8.625\n",
            "Batch : 1080, D Loss : 3.798 | G Loss : 8.853\n",
            "Batch : 1081, D Loss : 3.870 | G Loss : 9.355\n",
            "Batch : 1082, D Loss : 3.814 | G Loss : 8.250\n",
            "Batch : 1083, D Loss : 3.786 | G Loss : 9.119\n",
            "Batch : 1084, D Loss : 3.774 | G Loss : 9.166\n",
            "Batch : 1085, D Loss : 3.766 | G Loss : 9.116\n",
            "Batch : 1086, D Loss : 3.875 | G Loss : 9.386\n",
            "Batch : 1087, D Loss : 3.798 | G Loss : 9.208\n",
            "Batch : 1088, D Loss : 3.929 | G Loss : 9.130\n",
            "Batch : 1089, D Loss : 3.775 | G Loss : 9.775\n",
            "Batch : 1090, D Loss : 3.812 | G Loss : 10.949\n",
            "Batch : 1091, D Loss : 3.852 | G Loss : 9.130\n",
            "Batch : 1092, D Loss : 3.945 | G Loss : 10.372\n",
            "Batch : 1093, D Loss : 3.892 | G Loss : 9.379\n",
            "Batch : 1094, D Loss : 3.853 | G Loss : 10.860\n",
            "Batch : 1095, D Loss : 3.854 | G Loss : 9.818\n",
            "Batch : 1096, D Loss : 3.865 | G Loss : 9.359\n",
            "Batch : 1097, D Loss : 3.817 | G Loss : 10.003\n",
            "Batch : 1098, D Loss : 3.877 | G Loss : 8.521\n",
            "Batch : 1099, D Loss : 3.893 | G Loss : 9.330\n",
            "Batch : 1100, D Loss : 3.816 | G Loss : 8.540\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            ">Saved: plot_000004.png and g_model & d_model\n",
            " ========== Epoch 5 ========== \n",
            "Batch : 1, D Loss : 3.863 | G Loss : 9.326\n",
            "Batch : 2, D Loss : 3.818 | G Loss : 9.472\n",
            "Batch : 3, D Loss : 3.739 | G Loss : 9.443\n",
            "Batch : 4, D Loss : 3.792 | G Loss : 9.246\n",
            "Batch : 5, D Loss : 3.947 | G Loss : 8.313\n",
            "Batch : 6, D Loss : 3.886 | G Loss : 9.232\n",
            "Batch : 7, D Loss : 3.861 | G Loss : 8.309\n",
            "Batch : 8, D Loss : 3.905 | G Loss : 10.722\n",
            "Batch : 9, D Loss : 3.902 | G Loss : 10.330\n",
            "Batch : 10, D Loss : 3.852 | G Loss : 10.050\n",
            "Batch : 11, D Loss : 3.821 | G Loss : 10.035\n",
            "Batch : 12, D Loss : 3.753 | G Loss : 9.640\n",
            "Batch : 13, D Loss : 3.920 | G Loss : 8.016\n",
            "Batch : 14, D Loss : 3.986 | G Loss : 8.883\n",
            "Batch : 15, D Loss : 3.912 | G Loss : 10.393\n",
            "Batch : 16, D Loss : 3.808 | G Loss : 9.312\n",
            "Batch : 17, D Loss : 3.809 | G Loss : 8.803\n",
            "Batch : 18, D Loss : 3.855 | G Loss : 8.838\n",
            "Batch : 19, D Loss : 3.800 | G Loss : 9.099\n",
            "Batch : 20, D Loss : 3.878 | G Loss : 9.760\n",
            "Batch : 21, D Loss : 3.793 | G Loss : 8.912\n",
            "Batch : 22, D Loss : 3.789 | G Loss : 9.930\n",
            "Batch : 23, D Loss : 3.882 | G Loss : 9.371\n",
            "Batch : 24, D Loss : 4.616 | G Loss : 10.478\n",
            "Batch : 25, D Loss : 4.494 | G Loss : 11.209\n",
            "Batch : 26, D Loss : 4.173 | G Loss : 10.022\n",
            "Batch : 27, D Loss : 4.066 | G Loss : 9.109\n",
            "Batch : 28, D Loss : 3.964 | G Loss : 9.176\n",
            "Batch : 29, D Loss : 3.809 | G Loss : 10.854\n",
            "Batch : 30, D Loss : 3.927 | G Loss : 11.466\n",
            "Batch : 31, D Loss : 4.351 | G Loss : 11.961\n",
            "Batch : 32, D Loss : 4.343 | G Loss : 10.139\n",
            "Batch : 33, D Loss : 4.339 | G Loss : 11.263\n",
            "Batch : 34, D Loss : 4.174 | G Loss : 9.324\n",
            "Batch : 35, D Loss : 4.074 | G Loss : 10.014\n",
            "Batch : 36, D Loss : 3.984 | G Loss : 10.000\n",
            "Batch : 37, D Loss : 3.986 | G Loss : 10.851\n",
            "Batch : 38, D Loss : 4.045 | G Loss : 10.340\n",
            "Batch : 39, D Loss : 3.870 | G Loss : 8.188\n",
            "Batch : 40, D Loss : 3.929 | G Loss : 10.076\n",
            "Batch : 41, D Loss : 3.786 | G Loss : 9.516\n",
            "Batch : 42, D Loss : 3.902 | G Loss : 10.799\n",
            "Batch : 43, D Loss : 3.949 | G Loss : 9.635\n",
            "Batch : 44, D Loss : 3.909 | G Loss : 9.019\n",
            "Batch : 45, D Loss : 3.867 | G Loss : 8.126\n",
            "Batch : 46, D Loss : 3.805 | G Loss : 11.555\n",
            "Batch : 47, D Loss : 3.922 | G Loss : 8.278\n",
            "Batch : 48, D Loss : 4.438 | G Loss : 11.171\n",
            "Batch : 49, D Loss : 4.313 | G Loss : 11.064\n",
            "Batch : 50, D Loss : 3.963 | G Loss : 10.291\n",
            "Batch : 51, D Loss : 3.848 | G Loss : 8.866\n",
            "Batch : 52, D Loss : 4.078 | G Loss : 10.475\n",
            "Batch : 53, D Loss : 3.911 | G Loss : 9.983\n",
            "Batch : 54, D Loss : 3.908 | G Loss : 9.524\n",
            "Batch : 55, D Loss : 3.882 | G Loss : 8.529\n",
            "Batch : 56, D Loss : 3.884 | G Loss : 9.337\n",
            "Batch : 57, D Loss : 3.822 | G Loss : 9.184\n",
            "Batch : 58, D Loss : 3.893 | G Loss : 8.275\n",
            "Batch : 59, D Loss : 3.830 | G Loss : 10.091\n",
            "Batch : 60, D Loss : 3.906 | G Loss : 11.053\n",
            "Batch : 61, D Loss : 3.878 | G Loss : 9.813\n",
            "Batch : 62, D Loss : 3.871 | G Loss : 9.000\n",
            "Batch : 63, D Loss : 3.901 | G Loss : 9.406\n",
            "Batch : 64, D Loss : 3.855 | G Loss : 8.744\n",
            "Batch : 65, D Loss : 3.919 | G Loss : 9.210\n",
            "Batch : 66, D Loss : 3.856 | G Loss : 10.828\n",
            "Batch : 67, D Loss : 3.925 | G Loss : 10.647\n",
            "Batch : 68, D Loss : 3.868 | G Loss : 9.086\n",
            "Batch : 69, D Loss : 3.875 | G Loss : 8.727\n",
            "Batch : 70, D Loss : 3.784 | G Loss : 11.645\n",
            "Batch : 71, D Loss : 3.939 | G Loss : 9.582\n",
            "Batch : 72, D Loss : 3.861 | G Loss : 8.945\n",
            "Batch : 73, D Loss : 3.781 | G Loss : 9.010\n",
            "Batch : 74, D Loss : 3.828 | G Loss : 8.602\n",
            "Batch : 75, D Loss : 3.836 | G Loss : 9.304\n",
            "Batch : 76, D Loss : 3.838 | G Loss : 9.036\n",
            "Batch : 77, D Loss : 3.881 | G Loss : 9.869\n",
            "Batch : 78, D Loss : 3.782 | G Loss : 8.442\n",
            "Batch : 79, D Loss : 3.733 | G Loss : 8.864\n",
            "Batch : 80, D Loss : 3.857 | G Loss : 9.913\n",
            "Batch : 81, D Loss : 3.857 | G Loss : 9.491\n",
            "Batch : 82, D Loss : 3.862 | G Loss : 8.560\n",
            "Batch : 83, D Loss : 3.816 | G Loss : 9.449\n",
            "Batch : 84, D Loss : 3.791 | G Loss : 9.325\n",
            "Batch : 85, D Loss : 3.859 | G Loss : 9.116\n",
            "Batch : 86, D Loss : 3.816 | G Loss : 8.783\n",
            "Batch : 87, D Loss : 3.806 | G Loss : 10.853\n",
            "Batch : 88, D Loss : 3.942 | G Loss : 8.987\n",
            "Batch : 89, D Loss : 3.869 | G Loss : 9.752\n",
            "Batch : 90, D Loss : 3.810 | G Loss : 8.945\n",
            "Batch : 91, D Loss : 3.757 | G Loss : 8.733\n",
            "Batch : 92, D Loss : 3.856 | G Loss : 8.851\n",
            "Batch : 93, D Loss : 3.991 | G Loss : 9.802\n",
            "Batch : 94, D Loss : 3.955 | G Loss : 8.245\n",
            "Batch : 95, D Loss : 3.808 | G Loss : 8.608\n",
            "Batch : 96, D Loss : 3.725 | G Loss : 8.776\n",
            "Batch : 97, D Loss : 3.936 | G Loss : 11.398\n",
            "Batch : 98, D Loss : 3.959 | G Loss : 9.760\n",
            "Batch : 99, D Loss : 3.921 | G Loss : 9.005\n",
            "Batch : 100, D Loss : 3.817 | G Loss : 9.234\n",
            "Batch : 101, D Loss : 3.865 | G Loss : 8.810\n",
            "Batch : 102, D Loss : 3.969 | G Loss : 9.436\n",
            "Batch : 103, D Loss : 3.817 | G Loss : 8.526\n",
            "Batch : 104, D Loss : 3.945 | G Loss : 9.206\n",
            "Batch : 105, D Loss : 3.851 | G Loss : 9.032\n",
            "Batch : 106, D Loss : 3.846 | G Loss : 10.501\n",
            "Batch : 107, D Loss : 3.817 | G Loss : 9.378\n",
            "Batch : 108, D Loss : 3.841 | G Loss : 10.266\n",
            "Batch : 109, D Loss : 3.851 | G Loss : 10.529\n",
            "Batch : 110, D Loss : 3.807 | G Loss : 9.957\n",
            "Batch : 111, D Loss : 3.905 | G Loss : 8.102\n",
            "Batch : 112, D Loss : 3.808 | G Loss : 8.825\n",
            "Batch : 113, D Loss : 3.862 | G Loss : 9.260\n",
            "Batch : 114, D Loss : 3.847 | G Loss : 9.182\n",
            "Batch : 115, D Loss : 3.902 | G Loss : 9.702\n",
            "Batch : 116, D Loss : 3.893 | G Loss : 8.517\n",
            "Batch : 117, D Loss : 3.874 | G Loss : 10.202\n",
            "Batch : 118, D Loss : 3.935 | G Loss : 9.121\n",
            "Batch : 119, D Loss : 3.900 | G Loss : 8.309\n",
            "Batch : 120, D Loss : 3.856 | G Loss : 8.251\n",
            "Batch : 121, D Loss : 3.807 | G Loss : 9.353\n",
            "Batch : 122, D Loss : 3.919 | G Loss : 10.580\n",
            "Batch : 123, D Loss : 3.964 | G Loss : 9.008\n",
            "Batch : 124, D Loss : 3.807 | G Loss : 9.181\n",
            "Batch : 125, D Loss : 3.828 | G Loss : 8.369\n",
            "Batch : 126, D Loss : 3.834 | G Loss : 9.053\n",
            "Batch : 127, D Loss : 3.873 | G Loss : 9.736\n",
            "Batch : 128, D Loss : 3.901 | G Loss : 11.563\n",
            "Batch : 129, D Loss : 3.984 | G Loss : 8.443\n",
            "Batch : 130, D Loss : 3.870 | G Loss : 8.396\n",
            "Batch : 131, D Loss : 3.864 | G Loss : 10.447\n",
            "Batch : 132, D Loss : 3.923 | G Loss : 9.249\n",
            "Batch : 133, D Loss : 3.830 | G Loss : 9.921\n",
            "Batch : 134, D Loss : 3.830 | G Loss : 9.854\n",
            "Batch : 135, D Loss : 3.857 | G Loss : 9.831\n",
            "Batch : 136, D Loss : 3.896 | G Loss : 8.923\n",
            "Batch : 137, D Loss : 3.757 | G Loss : 9.680\n",
            "Batch : 138, D Loss : 3.863 | G Loss : 8.997\n",
            "Batch : 139, D Loss : 3.817 | G Loss : 9.645\n",
            "Batch : 140, D Loss : 3.819 | G Loss : 9.460\n",
            "Batch : 141, D Loss : 3.868 | G Loss : 9.213\n",
            "Batch : 142, D Loss : 3.827 | G Loss : 9.346\n",
            "Batch : 143, D Loss : 3.727 | G Loss : 8.638\n",
            "Batch : 144, D Loss : 3.881 | G Loss : 10.738\n",
            "Batch : 145, D Loss : 3.851 | G Loss : 11.328\n",
            "Batch : 146, D Loss : 3.793 | G Loss : 8.827\n",
            "Batch : 147, D Loss : 3.798 | G Loss : 10.060\n",
            "Batch : 148, D Loss : 3.809 | G Loss : 9.584\n",
            "Batch : 149, D Loss : 3.854 | G Loss : 9.099\n",
            "Batch : 150, D Loss : 3.897 | G Loss : 8.759\n",
            "Batch : 151, D Loss : 3.857 | G Loss : 8.801\n",
            "Batch : 152, D Loss : 3.778 | G Loss : 10.471\n",
            "Batch : 153, D Loss : 4.040 | G Loss : 9.767\n",
            "Batch : 154, D Loss : 3.915 | G Loss : 10.198\n",
            "Batch : 155, D Loss : 3.838 | G Loss : 9.589\n",
            "Batch : 156, D Loss : 3.852 | G Loss : 8.519\n",
            "Batch : 157, D Loss : 3.825 | G Loss : 9.797\n",
            "Batch : 158, D Loss : 3.795 | G Loss : 8.537\n",
            "Batch : 159, D Loss : 3.884 | G Loss : 8.566\n",
            "Batch : 160, D Loss : 3.840 | G Loss : 8.973\n",
            "Batch : 161, D Loss : 3.851 | G Loss : 10.216\n",
            "Batch : 162, D Loss : 3.741 | G Loss : 9.560\n",
            "Batch : 163, D Loss : 3.812 | G Loss : 8.505\n",
            "Batch : 164, D Loss : 3.880 | G Loss : 10.328\n",
            "Batch : 165, D Loss : 3.812 | G Loss : 9.860\n",
            "Batch : 166, D Loss : 3.764 | G Loss : 10.698\n",
            "Batch : 167, D Loss : 3.842 | G Loss : 9.291\n",
            "Batch : 168, D Loss : 3.787 | G Loss : 8.564\n",
            "Batch : 169, D Loss : 3.803 | G Loss : 8.578\n",
            "Batch : 170, D Loss : 3.872 | G Loss : 10.366\n",
            "Batch : 171, D Loss : 4.005 | G Loss : 9.734\n",
            "Batch : 172, D Loss : 3.892 | G Loss : 9.095\n",
            "Batch : 173, D Loss : 3.789 | G Loss : 9.666\n",
            "Batch : 174, D Loss : 3.819 | G Loss : 9.320\n",
            "Batch : 175, D Loss : 3.810 | G Loss : 11.195\n",
            "Batch : 176, D Loss : 3.970 | G Loss : 9.218\n",
            "Batch : 177, D Loss : 3.841 | G Loss : 9.417\n",
            "Batch : 178, D Loss : 3.883 | G Loss : 9.228\n",
            "Batch : 179, D Loss : 3.803 | G Loss : 10.104\n",
            "Batch : 180, D Loss : 3.944 | G Loss : 8.576\n",
            "Batch : 181, D Loss : 3.829 | G Loss : 9.287\n",
            "Batch : 182, D Loss : 3.887 | G Loss : 10.148\n",
            "Batch : 183, D Loss : 4.084 | G Loss : 11.527\n",
            "Batch : 184, D Loss : 4.061 | G Loss : 9.833\n",
            "Batch : 185, D Loss : 4.624 | G Loss : 9.163\n",
            "Batch : 186, D Loss : 4.294 | G Loss : 9.725\n",
            "Batch : 187, D Loss : 4.004 | G Loss : 8.600\n",
            "Batch : 188, D Loss : 3.977 | G Loss : 8.469\n",
            "Batch : 189, D Loss : 3.900 | G Loss : 8.029\n",
            "Batch : 190, D Loss : 3.989 | G Loss : 9.021\n",
            "Batch : 191, D Loss : 3.822 | G Loss : 9.348\n",
            "Batch : 192, D Loss : 3.874 | G Loss : 9.163\n",
            "Batch : 193, D Loss : 3.977 | G Loss : 9.587\n",
            "Batch : 194, D Loss : 3.903 | G Loss : 8.219\n",
            "Batch : 195, D Loss : 3.768 | G Loss : 8.563\n",
            "Batch : 196, D Loss : 3.855 | G Loss : 9.496\n",
            "Batch : 197, D Loss : 3.848 | G Loss : 9.297\n",
            "Batch : 198, D Loss : 3.888 | G Loss : 9.907\n",
            "Batch : 199, D Loss : 3.775 | G Loss : 9.830\n",
            "Batch : 200, D Loss : 3.834 | G Loss : 8.365\n",
            "Batch : 201, D Loss : 3.792 | G Loss : 9.453\n",
            "Batch : 202, D Loss : 3.811 | G Loss : 10.015\n",
            "Batch : 203, D Loss : 3.933 | G Loss : 10.218\n",
            "Batch : 204, D Loss : 3.957 | G Loss : 9.964\n",
            "Batch : 205, D Loss : 3.899 | G Loss : 8.679\n",
            "Batch : 206, D Loss : 3.862 | G Loss : 8.994\n",
            "Batch : 207, D Loss : 3.890 | G Loss : 9.119\n",
            "Batch : 208, D Loss : 3.944 | G Loss : 9.153\n",
            "Batch : 209, D Loss : 3.820 | G Loss : 7.961\n",
            "Batch : 210, D Loss : 3.843 | G Loss : 8.182\n",
            "Batch : 211, D Loss : 3.862 | G Loss : 9.403\n",
            "Batch : 212, D Loss : 3.821 | G Loss : 10.908\n",
            "Batch : 213, D Loss : 3.928 | G Loss : 8.645\n",
            "Batch : 214, D Loss : 3.882 | G Loss : 8.692\n",
            "Batch : 215, D Loss : 3.822 | G Loss : 9.611\n",
            "Batch : 216, D Loss : 3.956 | G Loss : 9.432\n",
            "Batch : 217, D Loss : 3.747 | G Loss : 9.892\n",
            "Batch : 218, D Loss : 3.919 | G Loss : 9.760\n",
            "Batch : 219, D Loss : 3.825 | G Loss : 10.663\n",
            "Batch : 220, D Loss : 3.827 | G Loss : 8.862\n",
            "Batch : 221, D Loss : 3.901 | G Loss : 9.188\n",
            "Batch : 222, D Loss : 3.835 | G Loss : 8.917\n",
            "Batch : 223, D Loss : 3.789 | G Loss : 8.591\n",
            "Batch : 224, D Loss : 3.820 | G Loss : 7.814\n",
            "Batch : 225, D Loss : 3.825 | G Loss : 8.857\n",
            "Batch : 226, D Loss : 3.743 | G Loss : 10.511\n",
            "Batch : 227, D Loss : 3.837 | G Loss : 8.954\n",
            "Batch : 228, D Loss : 3.885 | G Loss : 9.060\n",
            "Batch : 229, D Loss : 3.772 | G Loss : 9.421\n",
            "Batch : 230, D Loss : 3.790 | G Loss : 9.820\n",
            "Batch : 231, D Loss : 3.728 | G Loss : 8.947\n",
            "Batch : 232, D Loss : 3.821 | G Loss : 8.925\n",
            "Batch : 233, D Loss : 3.869 | G Loss : 8.999\n",
            "Batch : 234, D Loss : 3.902 | G Loss : 9.846\n",
            "Batch : 235, D Loss : 3.839 | G Loss : 8.306\n",
            "Batch : 236, D Loss : 3.818 | G Loss : 8.720\n",
            "Batch : 237, D Loss : 3.816 | G Loss : 10.613\n",
            "Batch : 238, D Loss : 3.842 | G Loss : 9.302\n",
            "Batch : 239, D Loss : 3.857 | G Loss : 9.419\n",
            "Batch : 240, D Loss : 3.834 | G Loss : 8.822\n",
            "Batch : 241, D Loss : 3.808 | G Loss : 12.182\n",
            "Batch : 242, D Loss : 3.885 | G Loss : 9.818\n",
            "Batch : 243, D Loss : 3.822 | G Loss : 9.181\n",
            "Batch : 244, D Loss : 3.838 | G Loss : 9.751\n",
            "Batch : 245, D Loss : 3.817 | G Loss : 8.041\n",
            "Batch : 246, D Loss : 3.792 | G Loss : 9.822\n",
            "Batch : 247, D Loss : 3.814 | G Loss : 8.325\n",
            "Batch : 248, D Loss : 3.859 | G Loss : 8.884\n",
            "Batch : 249, D Loss : 3.975 | G Loss : 10.442\n",
            "Batch : 250, D Loss : 3.930 | G Loss : 11.118\n",
            "Batch : 251, D Loss : 3.860 | G Loss : 10.580\n",
            "Batch : 252, D Loss : 3.828 | G Loss : 10.608\n",
            "Batch : 253, D Loss : 3.869 | G Loss : 8.418\n",
            "Batch : 254, D Loss : 3.820 | G Loss : 9.919\n",
            "Batch : 255, D Loss : 3.842 | G Loss : 9.077\n",
            "Batch : 256, D Loss : 3.860 | G Loss : 8.237\n",
            "Batch : 257, D Loss : 3.790 | G Loss : 10.287\n",
            "Batch : 258, D Loss : 3.897 | G Loss : 8.615\n",
            "Batch : 259, D Loss : 3.839 | G Loss : 8.804\n",
            "Batch : 260, D Loss : 3.882 | G Loss : 10.941\n",
            "Batch : 261, D Loss : 3.845 | G Loss : 9.696\n",
            "Batch : 262, D Loss : 3.842 | G Loss : 11.056\n",
            "Batch : 263, D Loss : 3.863 | G Loss : 9.401\n",
            "Batch : 264, D Loss : 3.807 | G Loss : 9.127\n",
            "Batch : 265, D Loss : 3.897 | G Loss : 11.539\n",
            "Batch : 266, D Loss : 3.923 | G Loss : 9.171\n",
            "Batch : 267, D Loss : 3.797 | G Loss : 8.277\n",
            "Batch : 268, D Loss : 3.731 | G Loss : 8.769\n",
            "Batch : 269, D Loss : 3.872 | G Loss : 9.058\n",
            "Batch : 270, D Loss : 3.830 | G Loss : 8.906\n",
            "Batch : 271, D Loss : 3.863 | G Loss : 11.655\n",
            "Batch : 272, D Loss : 3.901 | G Loss : 9.411\n",
            "Batch : 273, D Loss : 3.856 | G Loss : 9.095\n",
            "Batch : 274, D Loss : 3.785 | G Loss : 8.977\n",
            "Batch : 275, D Loss : 3.949 | G Loss : 11.110\n",
            "Batch : 276, D Loss : 3.976 | G Loss : 9.563\n",
            "Batch : 277, D Loss : 3.872 | G Loss : 8.494\n",
            "Batch : 278, D Loss : 3.775 | G Loss : 8.083\n",
            "Batch : 279, D Loss : 3.775 | G Loss : 10.292\n",
            "Batch : 280, D Loss : 3.943 | G Loss : 9.872\n",
            "Batch : 281, D Loss : 3.886 | G Loss : 8.576\n",
            "Batch : 282, D Loss : 3.836 | G Loss : 9.109\n",
            "Batch : 283, D Loss : 3.783 | G Loss : 8.438\n",
            "Batch : 284, D Loss : 3.882 | G Loss : 8.314\n",
            "Batch : 285, D Loss : 3.847 | G Loss : 9.820\n",
            "Batch : 286, D Loss : 3.858 | G Loss : 9.326\n",
            "Batch : 287, D Loss : 3.816 | G Loss : 8.977\n",
            "Batch : 288, D Loss : 3.733 | G Loss : 8.627\n",
            "Batch : 289, D Loss : 3.847 | G Loss : 8.146\n",
            "Batch : 290, D Loss : 3.822 | G Loss : 8.066\n",
            "Batch : 291, D Loss : 3.859 | G Loss : 9.938\n",
            "Batch : 292, D Loss : 3.807 | G Loss : 8.700\n",
            "Batch : 293, D Loss : 3.822 | G Loss : 10.098\n",
            "Batch : 294, D Loss : 3.966 | G Loss : 10.563\n",
            "Batch : 295, D Loss : 3.884 | G Loss : 9.656\n",
            "Batch : 296, D Loss : 3.865 | G Loss : 10.138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6BV87tT_LKv1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}